{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXQd3XuVBELXEqVYtLzY1h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishekkm8088/Pyspark-Abhi/blob/main/ScalaAdvProgramming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing the scala environment"
      ],
      "metadata": {
        "id": "nmg1aLpoAZE3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj4KTddSwcsa",
        "outputId": "f80327e1-7c3f-43dc-a69e-099d7dccae09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 130340 files and directories currently installed.)\n",
            "Preparing to unpack scala-2.12.18.deb ...\n",
            "Unpacking scala (2.12.18-400) over (2.12.18-400) ...\n",
            "Setting up scala (2.12.18-400) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 42577  100 42577    0     0  78905      0 --:--:-- --:--:-- --:--:-- 78905\n",
            "Error: /root/.local/share/jupyter/kernels/scala already exists, pass --force to force erasing it\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!wget -q https://github.com/scala/scala/releases/download/v2.12.18/scala-2.12.18.deb\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "!dpkg -i scala-2.12.18.deb\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "\n",
        "!curl -Lo coursier https://git.io/coursier-cli && chmod +x coursier\n",
        "!./coursier launch --fork almond --scala 2.12.10 -- --install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /usr/lib/jvm/java-11-openjdk-amd64\n",
        "!ls /content/spark-3.4.1-bin-hadoop3\n",
        "!ls /content/scala-2.12.18.deb\n",
        "!echo $JAVA_HOME\n",
        "!echo $SPARK_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoohG5w77qyv",
        "outputId": "c3004750-0355-4ea0-d008-ff89bb29a4c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin  conf  docs  include  jmods  legal\tlib  man  release\n",
            "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\n",
            "conf  examples\tkubernetes  licenses  python  README.md  sbin\n",
            "/content/scala-2.12.18.deb\n",
            "/usr/lib/jvm/java-11-openjdk-amd64\n",
            "/content/spark-3.4.1-bin-hadoop3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Colab Scala Spark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "mwTEYSRB89Za",
        "outputId": "3665fe9c-1643-420d-9f2c-698ebe98ae20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7faf12701650>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e9396b4389d3:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab Scala Spark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile HelloWorld.scala\n",
        "import org.apache.spark.sql.SparkSession\n",
        "\n",
        "object HelloWorld {\n",
        "  def main(args: Array[String]): Unit = {\n",
        "    val spark = SparkSession.builder()\n",
        "      .appName(\"ScalaAdvancedTransformations\")\n",
        "      .master(\"local[*]\")\n",
        "      .config(\"spark.driver.memory\", \"1g\")  // Increase memory to 1 GB\n",
        "      .getOrCreate()\n",
        "\n",
        "    val sc = spark.sparkContext\n",
        "\n",
        "    // Sample data\n",
        "    val rdd = sc.parallelize(Seq(\n",
        "      (\"apple\", 3), (\"apple\", 4),\n",
        "      (\"apple\", 2), (\"banana\", 3),\n",
        "      (\"banana\", 1), (\"orange\", 5),\n",
        "      (\"apple\", 6)\n",
        "    ))\n",
        "\n",
        "    // Using combineByKey (Advanced Transformation)\n",
        "    val combined = rdd.combineByKey(\n",
        "      (value: Int) => (value, 1),                                 // CreateCombiner\n",
        "      (acc1: (Int, Int), value: Int) => (acc1._1 + value, acc1._2 + 1), // MergeValue\n",
        "      (acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)  // MergeCombiners\n",
        "    )\n",
        "\n",
        "    // Compute average per key\n",
        "    val averages = combined.mapValues { case (sum, count) => sum.toDouble / count }\n",
        "\n",
        "    averages.collect().foreach(println)\n",
        "\n",
        "    spark.stop()\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mVl9OLB9IFD",
        "outputId": "8844b8ec-8126-45a5-a920-e43e4dacffc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting HelloWorld.scala\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scalac -classpath \"$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" HelloWorld.scala"
      ],
      "metadata": {
        "id": "n0c1EwU_-Brz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!scala -J-Xmx1g -classpath \".:$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" HelloWorld"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7ya6xMK-Ywp",
        "outputId": "aa788b52-a4dc-4d5b-a619-101c4578926a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/01 04:53:06 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/01 04:53:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/01 04:53:07 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 04:53:07 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/01 04:53:07 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 04:53:07 INFO SparkContext: Submitted application: ScalaAdvancedTransformations\n",
            "25/08/01 04:53:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/01 04:53:07 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/01 04:53:07 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/01 04:53:07 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/01 04:53:07 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/01 04:53:07 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/01 04:53:07 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/01 04:53:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/01 04:53:07 INFO Utils: Successfully started service 'sparkDriver' on port 35079.\n",
            "25/08/01 04:53:08 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/01 04:53:08 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/01 04:53:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/01 04:53:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/01 04:53:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/01 04:53:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-595f9280-77df-45e6-9ddf-bcb2ab99508f\n",
            "25/08/01 04:53:08 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "25/08/01 04:53:08 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/01 04:53:08 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/01 04:53:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/01 04:53:08 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/01 04:53:08 INFO Executor: Starting executor ID driver on host e9396b4389d3\n",
            "25/08/01 04:53:08 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/01 04:53:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44981.\n",
            "25/08/01 04:53:09 INFO NettyBlockTransferService: Server created on e9396b4389d3:44981\n",
            "25/08/01 04:53:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/01 04:53:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e9396b4389d3, 44981, None)\n",
            "25/08/01 04:53:09 INFO BlockManagerMasterEndpoint: Registering block manager e9396b4389d3:44981 with 434.4 MiB RAM, BlockManagerId(driver, e9396b4389d3, 44981, None)\n",
            "25/08/01 04:53:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e9396b4389d3, 44981, None)\n",
            "25/08/01 04:53:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e9396b4389d3, 44981, None)\n",
            "25/08/01 04:53:10 INFO SparkContext: Starting job: collect at HelloWorld.scala:31\n",
            "25/08/01 04:53:10 INFO DAGScheduler: Registering RDD 0 (parallelize at HelloWorld.scala:14) as input to shuffle 0\n",
            "25/08/01 04:53:10 INFO DAGScheduler: Got job 0 (collect at HelloWorld.scala:31) with 2 output partitions\n",
            "25/08/01 04:53:10 INFO DAGScheduler: Final stage: ResultStage 1 (collect at HelloWorld.scala:31)\n",
            "25/08/01 04:53:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "25/08/01 04:53:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "25/08/01 04:53:10 INFO DAGScheduler: Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelize at HelloWorld.scala:14), which has no missing parents\n",
            "25/08/01 04:53:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.9 KiB, free 434.4 MiB)\n",
            "25/08/01 04:53:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 434.4 MiB)\n",
            "25/08/01 04:53:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on e9396b4389d3:44981 (size: 2.3 KiB, free: 434.4 MiB)\n",
            "25/08/01 04:53:10 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/01 04:53:10 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelize at HelloWorld.scala:14) (first 15 tasks are for partitions Vector(0, 1))\n",
            "25/08/01 04:53:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "25/08/01 04:53:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (e9396b4389d3, executor driver, partition 0, PROCESS_LOCAL, 7453 bytes) \n",
            "25/08/01 04:53:11 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (e9396b4389d3, executor driver, partition 1, PROCESS_LOCAL, 7482 bytes) \n",
            "25/08/01 04:53:11 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "25/08/01 04:53:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/01 04:53:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1260 bytes result sent to driver\n",
            "25/08/01 04:53:11 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1260 bytes result sent to driver\n",
            "25/08/01 04:53:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 457 ms on e9396b4389d3 (executor driver) (1/2)\n",
            "25/08/01 04:53:11 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 366 ms on e9396b4389d3 (executor driver) (2/2)\n",
            "25/08/01 04:53:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/01 04:53:11 INFO DAGScheduler: ShuffleMapStage 0 (parallelize at HelloWorld.scala:14) finished in 0.897 s\n",
            "25/08/01 04:53:11 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/01 04:53:11 INFO DAGScheduler: running: Set()\n",
            "25/08/01 04:53:11 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "25/08/01 04:53:11 INFO DAGScheduler: failed: Set()\n",
            "25/08/01 04:53:11 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[2] at mapValues at HelloWorld.scala:29), which has no missing parents\n",
            "25/08/01 04:53:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.6 KiB, free 434.4 MiB)\n",
            "25/08/01 04:53:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 434.4 MiB)\n",
            "25/08/01 04:53:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on e9396b4389d3:44981 (size: 3.1 KiB, free: 434.4 MiB)\n",
            "25/08/01 04:53:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/01 04:53:11 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at mapValues at HelloWorld.scala:29) (first 15 tasks are for partitions Vector(0, 1))\n",
            "25/08/01 04:53:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "25/08/01 04:53:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (e9396b4389d3, executor driver, partition 0, NODE_LOCAL, 7181 bytes) \n",
            "25/08/01 04:53:11 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (e9396b4389d3, executor driver, partition 1, NODE_LOCAL, 7181 bytes) \n",
            "25/08/01 04:53:11 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
            "25/08/01 04:53:11 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)\n",
            "25/08/01 04:53:11 INFO ShuffleBlockFetcherIterator: Getting 2 (396.0 B) non-empty blocks including 2 (396.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/01 04:53:11 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/01 04:53:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 25 ms\n",
            "25/08/01 04:53:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms\n",
            "25/08/01 04:53:11 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1887 bytes result sent to driver\n",
            "25/08/01 04:53:11 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 1902 bytes result sent to driver\n",
            "25/08/01 04:53:11 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 224 ms on e9396b4389d3 (executor driver) (1/2)\n",
            "25/08/01 04:53:11 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 222 ms on e9396b4389d3 (executor driver) (2/2)\n",
            "25/08/01 04:53:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/01 04:53:11 INFO DAGScheduler: ResultStage 1 (collect at HelloWorld.scala:31) finished in 0.255 s\n",
            "25/08/01 04:53:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/01 04:53:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/01 04:53:11 INFO DAGScheduler: Job 0 finished: collect at HelloWorld.scala:31, took 1.354425 s\n",
            "(orange,5.0)\n",
            "(apple,3.75)\n",
            "(banana,2.0)\n",
            "25/08/01 04:53:11 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/01 04:53:11 INFO SparkUI: Stopped Spark web UI at http://e9396b4389d3:4041\n",
            "25/08/01 04:53:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/01 04:53:11 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/01 04:53:11 INFO BlockManager: BlockManager stopped\n",
            "25/08/01 04:53:12 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/01 04:53:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/01 04:53:12 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/01 04:53:12 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/01 04:53:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-3cb70c49-157f-4a57-ac27-4c3c533b39b4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile evenSquares.scala\n",
        "import org.apache.spark.sql.SparkSession\n",
        "\n",
        "object evenSquares {\n",
        "  def main(args: Array[String]): Unit = {\n",
        "    val spark = SparkSession.builder()\n",
        "      .appName(\"ScalaAdvancedTransformations\")\n",
        "      .master(\"local[*]\")\n",
        "      .config(\"spark.driver.memory\", \"1g\")  // Increase memory to 1 GB\n",
        "      .getOrCreate()\n",
        "\n",
        "    val sc = spark.sparkContext\n",
        "\n",
        "    // Sample data\n",
        "     val nums = List(1, 2, 3, 4, 5)\n",
        "\n",
        "    val evenSquares = nums.filter(_ % 2 == 0).map(x => x * x)\n",
        "\n",
        "    evenSquares.foreach(println)\n",
        "\n",
        "\n",
        "\n",
        "    spark.stop()\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f2zVYFOC4h8",
        "outputId": "b64f903e-e0f0-43d6-9832-30567ff878a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting evenSquares.scala\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scalac -classpath \"$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" evenSquares.scala"
      ],
      "metadata": {
        "id": "c9EjhnpODSfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!scala -J-Xmx1g -classpath \".:$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" evenSquares"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qFV6T8VDZCO",
        "outputId": "59242ab8-f4c4-43d8-d1c4-44ed0a33b7b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/01 05:24:49 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/01 05:24:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/01 05:24:50 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 05:24:50 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/01 05:24:50 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 05:24:50 INFO SparkContext: Submitted application: ScalaAdvancedTransformations\n",
            "25/08/01 05:24:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/01 05:24:50 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/01 05:24:50 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/01 05:24:50 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/01 05:24:50 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/01 05:24:50 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/01 05:24:50 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/01 05:24:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/01 05:24:51 INFO Utils: Successfully started service 'sparkDriver' on port 39017.\n",
            "25/08/01 05:24:51 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/01 05:24:51 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/01 05:24:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/01 05:24:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/01 05:24:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/01 05:24:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a07589bc-f815-44fb-b5e1-5e05c2798d95\n",
            "25/08/01 05:24:51 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "25/08/01 05:24:51 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/01 05:24:51 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/01 05:24:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/01 05:24:51 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/01 05:24:52 INFO Executor: Starting executor ID driver on host e9396b4389d3\n",
            "25/08/01 05:24:52 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/01 05:24:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46625.\n",
            "25/08/01 05:24:52 INFO NettyBlockTransferService: Server created on e9396b4389d3:46625\n",
            "25/08/01 05:24:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/01 05:24:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e9396b4389d3, 46625, None)\n",
            "25/08/01 05:24:52 INFO BlockManagerMasterEndpoint: Registering block manager e9396b4389d3:46625 with 434.4 MiB RAM, BlockManagerId(driver, e9396b4389d3, 46625, None)\n",
            "25/08/01 05:24:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e9396b4389d3, 46625, None)\n",
            "25/08/01 05:24:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e9396b4389d3, 46625, None)\n",
            "4\n",
            "16\n",
            "25/08/01 05:24:53 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/01 05:24:53 INFO SparkUI: Stopped Spark web UI at http://e9396b4389d3:4041\n",
            "25/08/01 05:24:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/01 05:24:53 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/01 05:24:53 INFO BlockManager: BlockManager stopped\n",
            "25/08/01 05:24:53 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/01 05:24:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/01 05:24:53 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/01 05:24:53 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/01 05:24:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-84a658d3-caf2-49ef-bf31-715208d030eb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile distinct.scala\n",
        "import org.apache.spark.sql.SparkSession\n",
        "\n",
        "object distinct {\n",
        "  def main(args: Array[String]): Unit = {\n",
        "    val spark = SparkSession.builder()\n",
        "      .appName(\"ScalaAdvancedTransformations\")\n",
        "      .master(\"local[*]\")\n",
        "      .config(\"spark.driver.memory\", \"1g\")  // Increase memory to 1 GB\n",
        "      .getOrCreate()\n",
        "\n",
        "    val sc = spark.sparkContext\n",
        "\n",
        "    // Sample data\n",
        "     val numbers = List(1, 2, 2, 3, 4, 4, 5, 6)\n",
        "     val result = numbers.filter(_ % 2 == 0).map(x => x * x).distinct\n",
        "     println(result)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    spark.stop()\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5KfGeLUFwJs",
        "outputId": "c287c318-df83-48c4-cf4f-b75f870ff086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting distinct.scala\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scalac -classpath \"$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" distinct.scala"
      ],
      "metadata": {
        "id": "zMLHDnkvGfgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LRZIsSl2G1Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "becbaace",
        "outputId": "bc370176-a118-4460-e70a-86e6fe73f803"
      },
      "source": [
        "!scala -J-Xmx1g -classpath \".:$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" distinct"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/01 06:29:52 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/01 06:29:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/01 06:29:53 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 06:29:53 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/01 06:29:53 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 06:29:53 INFO SparkContext: Submitted application: ScalaAdvancedTransformations\n",
            "25/08/01 06:29:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/01 06:29:54 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/01 06:29:54 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/01 06:29:54 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/01 06:29:54 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/01 06:29:54 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/01 06:29:54 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/01 06:29:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/01 06:29:54 INFO Utils: Successfully started service 'sparkDriver' on port 41107.\n",
            "25/08/01 06:29:54 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/01 06:29:55 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/01 06:29:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/01 06:29:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/01 06:29:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/01 06:29:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-eb0b529b-5e8c-42cf-816f-af2d2db81e7c\n",
            "25/08/01 06:29:55 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "25/08/01 06:29:55 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/01 06:29:55 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/01 06:29:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/01 06:29:55 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/01 06:29:55 INFO Executor: Starting executor ID driver on host e9396b4389d3\n",
            "25/08/01 06:29:55 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/01 06:29:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46041.\n",
            "25/08/01 06:29:56 INFO NettyBlockTransferService: Server created on e9396b4389d3:46041\n",
            "25/08/01 06:29:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/01 06:29:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e9396b4389d3, 46041, None)\n",
            "25/08/01 06:29:56 INFO BlockManagerMasterEndpoint: Registering block manager e9396b4389d3:46041 with 434.4 MiB RAM, BlockManagerId(driver, e9396b4389d3, 46041, None)\n",
            "25/08/01 06:29:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e9396b4389d3, 46041, None)\n",
            "25/08/01 06:29:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e9396b4389d3, 46041, None)\n",
            "List(4, 16, 36)\n",
            "25/08/01 06:29:57 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/01 06:29:57 INFO SparkUI: Stopped Spark web UI at http://e9396b4389d3:4041\n",
            "25/08/01 06:29:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/01 06:29:57 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/01 06:29:57 INFO BlockManager: BlockManager stopped\n",
            "25/08/01 06:29:57 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/01 06:29:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/01 06:29:57 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/01 06:29:57 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/01 06:29:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-d9d783f4-3805-4c2e-83b3-9027b2d32d53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Accumilator.scala\n",
        "import org.apache.spark.sql.SparkSession\n",
        "\n",
        "object Accumilator {\n",
        "  def main(args: Array[String]): Unit = {\n",
        "    val spark = SparkSession.builder()\n",
        "      .appName(\"ScalaAdvancedTransformations\")\n",
        "      .master(\"local[*]\")\n",
        "      .config(\"spark.driver.memory\", \"1g\")  // Increase memory to 1 GB\n",
        "      .getOrCreate()\n",
        "\n",
        "    val sc = spark.sparkContext\n",
        "\n",
        "   /////////////////////////////////////////cretae an Accumilator variables\n",
        "\n",
        "   val errorCount = sc.longAccumulator(\"Error Count\")\n",
        "\n",
        "   //sample RDD with some log lines\n",
        "   val data = sc.parallelize(Seq(\"INFO Start\", \"ERROR Failure1\", \"ERROR Failure2\", \"INFO End\"))\n",
        "\n",
        "   //use the Accumilator\n",
        "   data.foreach(line => {\n",
        "     if (line.contains(\"ERROR\")) {\n",
        "       errorCount.add(1)\n",
        "     }\n",
        "   })\n",
        "\n",
        "   println(\"Total errors: \" + errorCount.value)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    spark.stop()\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7afbkdaeUoRR",
        "outputId": "81e90592-d077-4f35-e824-c00d6a4cd3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Accumilator.scala\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scalac -classpath \"$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" Accumilator.scala"
      ],
      "metadata": {
        "id": "sJGQVu04Vfen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!scala -J-Xmx1g -classpath \".:$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" Accumilator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLZsF9fvVnwV",
        "outputId": "5c7316c3-c56a-400b-bc50-47d001906592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/01 06:34:47 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/01 06:34:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/01 06:34:48 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 06:34:48 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/01 06:34:48 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 06:34:48 INFO SparkContext: Submitted application: ScalaAdvancedTransformations\n",
            "25/08/01 06:34:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/01 06:34:48 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/01 06:34:48 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/01 06:34:48 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/01 06:34:48 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/01 06:34:48 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/01 06:34:48 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/01 06:34:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/01 06:34:49 INFO Utils: Successfully started service 'sparkDriver' on port 36853.\n",
            "25/08/01 06:34:49 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/01 06:34:49 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/01 06:34:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/01 06:34:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/01 06:34:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/01 06:34:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cce39b76-8dbc-469a-a01a-5f0dba31c3cb\n",
            "25/08/01 06:34:50 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "25/08/01 06:34:50 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/01 06:34:50 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/01 06:34:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/01 06:34:50 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/01 06:34:51 INFO Executor: Starting executor ID driver on host e9396b4389d3\n",
            "25/08/01 06:34:51 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/01 06:34:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46167.\n",
            "25/08/01 06:34:51 INFO NettyBlockTransferService: Server created on e9396b4389d3:46167\n",
            "25/08/01 06:34:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/01 06:34:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e9396b4389d3, 46167, None)\n",
            "25/08/01 06:34:51 INFO BlockManagerMasterEndpoint: Registering block manager e9396b4389d3:46167 with 434.4 MiB RAM, BlockManagerId(driver, e9396b4389d3, 46167, None)\n",
            "25/08/01 06:34:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e9396b4389d3, 46167, None)\n",
            "25/08/01 06:34:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e9396b4389d3, 46167, None)\n",
            "25/08/01 06:34:52 INFO SparkContext: Starting job: foreach at Accumilator.scala:21\n",
            "25/08/01 06:34:52 INFO DAGScheduler: Got job 0 (foreach at Accumilator.scala:21) with 2 output partitions\n",
            "25/08/01 06:34:52 INFO DAGScheduler: Final stage: ResultStage 0 (foreach at Accumilator.scala:21)\n",
            "25/08/01 06:34:52 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/01 06:34:52 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/01 06:34:52 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Accumilator.scala:18), which has no missing parents\n",
            "25/08/01 06:34:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.5 KiB, free 434.4 MiB)\n",
            "25/08/01 06:34:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.0 KiB, free 434.4 MiB)\n",
            "25/08/01 06:34:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on e9396b4389d3:46167 (size: 2.0 KiB, free: 434.4 MiB)\n",
            "25/08/01 06:34:53 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/01 06:34:53 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Accumilator.scala:18) (first 15 tasks are for partitions Vector(0, 1))\n",
            "25/08/01 06:34:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "25/08/01 06:34:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (e9396b4389d3, executor driver, partition 0, PROCESS_LOCAL, 7389 bytes) \n",
            "25/08/01 06:34:53 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (e9396b4389d3, executor driver, partition 1, PROCESS_LOCAL, 7387 bytes) \n",
            "25/08/01 06:34:53 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "25/08/01 06:34:53 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/01 06:34:53 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1004 bytes result sent to driver\n",
            "25/08/01 06:34:53 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1004 bytes result sent to driver\n",
            "25/08/01 06:34:53 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 262 ms on e9396b4389d3 (executor driver) (1/2)\n",
            "25/08/01 06:34:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 385 ms on e9396b4389d3 (executor driver) (2/2)\n",
            "25/08/01 06:34:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/01 06:34:53 INFO DAGScheduler: ResultStage 0 (foreach at Accumilator.scala:21) finished in 0.853 s\n",
            "25/08/01 06:34:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/01 06:34:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/01 06:34:53 INFO DAGScheduler: Job 0 finished: foreach at Accumilator.scala:21, took 0.937204 s\n",
            "Total errors: 2\n",
            "25/08/01 06:34:53 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/01 06:34:53 INFO SparkUI: Stopped Spark web UI at http://e9396b4389d3:4041\n",
            "25/08/01 06:34:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/01 06:34:53 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/01 06:34:53 INFO BlockManager: BlockManager stopped\n",
            "25/08/01 06:34:53 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/01 06:34:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/01 06:34:53 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/01 06:34:53 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/01 06:34:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-5ea98c6c-941a-469d-8f87-b506dbd238e6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile broadcast.scala\n",
        "import org.apache.spark.sql.SparkSession\n",
        "\n",
        "object broadcast {\n",
        "  def main(args: Array[String]): Unit = {\n",
        "    val spark = SparkSession.builder()\n",
        "      .appName(\"ScalaAdvancedTransformations\")\n",
        "      .master(\"local[*]\")\n",
        "      .config(\"spark.driver.memory\", \"1g\")  // Increase memory to 1 GB\n",
        "      .getOrCreate()\n",
        "\n",
        "    val sc = spark.sparkContext\n",
        "\n",
        "   //create a lookup map to broadcast\n",
        "   val CountryCodes = Map(\"USA\" -> \"US\", \"India\" -> \"IN\", \"Germany\" -> \"DE\")\n",
        "   val lookup = sc.broadcast(CountryCodes)\n",
        "\n",
        "   //broadcast the variables\n",
        "   val broadcastCountryCodes = sc.broadcast(CountryCodes)\n",
        "\n",
        "   //Sample RDD of country codes\n",
        "   val countries = sc.parallelize(\"USA\", \"IN\", \"Germany\")\n",
        "\n",
        "   //Map using the broadcast variable\n",
        "   val  fullNames = countries.map(country => (country, lookup.value.getOrElse(country, \"Unknown\")))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    spark.stop()\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0xnmyH8Vw8H",
        "outputId": "d8b8c206-45c9-4e4e-be71-f5eb0846c2da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting broadcast.scala\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scalac -classpath \"$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" broadcast.scala"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F5CPlUPX_iO",
        "outputId": "d08d9500-6f07-4ede-bed4-23e487a8a45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "broadcast.scala:21: error: too many arguments (3) for method parallelize: (seq: Seq[T], numSlices: Int)(implicit evidence$1: scala.reflect.ClassTag[T])org.apache.spark.rdd.RDD[T]\n",
            "   val countries = sc.parallelize(\"USA\", \"IN\", \"Germany\")\n",
            "                                               ^\n",
            "one error found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Persist.scala\n",
        "import org.apache.spark.sql.SparkSession\n",
        "\n",
        "object Persist {\n",
        "  def main(args: Array[String]): Unit = {\n",
        "    val spark = SparkSession.builder()\n",
        "      .appName(\"ScalaAdvancedTransformations\")\n",
        "      .master(\"local[*]\")\n",
        "      .config(\"spark.driver.memory\", \"1g\")  // Increase memory to 1 GB\n",
        "      .getOrCreate()\n",
        "\n",
        "    val sc = spark.sparkContext\n",
        "\n",
        "   val bigData = sc.parallelize(1 to 10000000)\n",
        "\n",
        "// Persist to memory and spill to disk if needed\n",
        "val squares = bigData.map(x => x * x).persist(org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "// First action triggers persistence\n",
        "println(\"First 5: \" + squares.take(5).mkString(\", \"))\n",
        "\n",
        "// Reuse persisted data\n",
        "println(\"Count: \" + squares.count())\n",
        "\n",
        "\n",
        "\n",
        "    spark.stop()\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9W8FMucYZ5Z",
        "outputId": "554d3c3b-7bdc-421f-e46c-c401d1ee55eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Persist.scala\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scalac -classpath \"$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" Persist.scala"
      ],
      "metadata": {
        "id": "FFWRDcjhd1UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!scala -J-Xmx1g -classpath \".:$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" Persist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzkZfMWmd8aa",
        "outputId": "0939ec14-bae4-490d-9461-127187351904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/01 07:11:09 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/01 07:11:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/01 07:11:09 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 07:11:09 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/01 07:11:09 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 07:11:09 INFO SparkContext: Submitted application: ScalaAdvancedTransformations\n",
            "25/08/01 07:11:10 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/01 07:11:10 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/01 07:11:10 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/01 07:11:10 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/01 07:11:10 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/01 07:11:10 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/01 07:11:10 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/01 07:11:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/01 07:11:10 INFO Utils: Successfully started service 'sparkDriver' on port 44675.\n",
            "25/08/01 07:11:10 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/01 07:11:10 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/01 07:11:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/01 07:11:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/01 07:11:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/01 07:11:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3ffa17d5-8ed4-4c22-a79c-ccf6393d78e7\n",
            "25/08/01 07:11:11 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "25/08/01 07:11:11 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/01 07:11:11 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/01 07:11:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/01 07:11:11 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/01 07:11:11 INFO Executor: Starting executor ID driver on host e9396b4389d3\n",
            "25/08/01 07:11:11 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/01 07:11:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41473.\n",
            "25/08/01 07:11:11 INFO NettyBlockTransferService: Server created on e9396b4389d3:41473\n",
            "25/08/01 07:11:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/01 07:11:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e9396b4389d3, 41473, None)\n",
            "25/08/01 07:11:11 INFO BlockManagerMasterEndpoint: Registering block manager e9396b4389d3:41473 with 434.4 MiB RAM, BlockManagerId(driver, e9396b4389d3, 41473, None)\n",
            "25/08/01 07:11:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e9396b4389d3, 41473, None)\n",
            "25/08/01 07:11:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e9396b4389d3, 41473, None)\n",
            "25/08/01 07:11:13 INFO SparkContext: Starting job: take at Persist.scala:19\n",
            "25/08/01 07:11:13 INFO DAGScheduler: Got job 0 (take at Persist.scala:19) with 1 output partitions\n",
            "25/08/01 07:11:13 INFO DAGScheduler: Final stage: ResultStage 0 (take at Persist.scala:19)\n",
            "25/08/01 07:11:13 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/01 07:11:13 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/01 07:11:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Persist.scala:16), which has no missing parents\n",
            "25/08/01 07:11:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.8 KiB, free 434.4 MiB)\n",
            "25/08/01 07:11:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.2 KiB, free 434.4 MiB)\n",
            "25/08/01 07:11:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on e9396b4389d3:41473 (size: 2.2 KiB, free: 434.4 MiB)\n",
            "25/08/01 07:11:14 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/01 07:11:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Persist.scala:16) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/01 07:11:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/01 07:11:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (e9396b4389d3, executor driver, partition 0, PROCESS_LOCAL, 7488 bytes) \n",
            "25/08/01 07:11:14 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/01 07:11:15 INFO MemoryStore: Block rdd_1_0 stored as values in memory (estimated size 19.1 MiB, free 415.3 MiB)\n",
            "25/08/01 07:11:15 INFO BlockManagerInfo: Added rdd_1_0 in memory on e9396b4389d3:41473 (size: 19.1 MiB, free: 415.3 MiB)\n",
            "25/08/01 07:11:15 INFO Executor: 1 block locks were not released by task 0.0 in stage 0.0 (TID 0)\n",
            "[rdd_1_0]\n",
            "25/08/01 07:11:15 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 938 bytes result sent to driver\n",
            "25/08/01 07:11:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1288 ms on e9396b4389d3 (executor driver) (1/1)\n",
            "25/08/01 07:11:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/01 07:11:15 INFO DAGScheduler: ResultStage 0 (take at Persist.scala:19) finished in 2.088 s\n",
            "25/08/01 07:11:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/01 07:11:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/01 07:11:15 INFO DAGScheduler: Job 0 finished: take at Persist.scala:19, took 2.330327 s\n",
            "First 5: 1, 4, 9, 16, 25\n",
            "25/08/01 07:11:16 INFO SparkContext: Starting job: count at Persist.scala:22\n",
            "25/08/01 07:11:16 INFO DAGScheduler: Got job 1 (count at Persist.scala:22) with 2 output partitions\n",
            "25/08/01 07:11:16 INFO DAGScheduler: Final stage: ResultStage 1 (count at Persist.scala:22)\n",
            "25/08/01 07:11:16 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/01 07:11:16 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/01 07:11:16 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[1] at map at Persist.scala:16), which has no missing parents\n",
            "25/08/01 07:11:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.8 KiB, free 415.3 MiB)\n",
            "25/08/01 07:11:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KiB, free 415.3 MiB)\n",
            "25/08/01 07:11:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on e9396b4389d3:41473 (size: 2.2 KiB, free: 415.3 MiB)\n",
            "25/08/01 07:11:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/01 07:11:16 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[1] at map at Persist.scala:16) (first 15 tasks are for partitions Vector(0, 1))\n",
            "25/08/01 07:11:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "25/08/01 07:11:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (e9396b4389d3, executor driver, partition 0, PROCESS_LOCAL, 7488 bytes) \n",
            "25/08/01 07:11:16 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (e9396b4389d3, executor driver, partition 1, PROCESS_LOCAL, 7488 bytes) \n",
            "25/08/01 07:11:16 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/01 07:11:16 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "25/08/01 07:11:16 INFO BlockManager: Found block rdd_1_0 locally\n",
            "25/08/01 07:11:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on e9396b4389d3:41473 in memory (size: 2.2 KiB, free: 415.3 MiB)\n",
            "25/08/01 07:11:17 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1059 bytes result sent to driver\n",
            "25/08/01 07:11:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1169 ms on e9396b4389d3 (executor driver) (1/2)\n",
            "25/08/01 07:11:17 INFO MemoryStore: Block rdd_1_1 stored as values in memory (estimated size 19.1 MiB, free 396.2 MiB)\n",
            "25/08/01 07:11:17 INFO BlockManagerInfo: Added rdd_1_1 in memory on e9396b4389d3:41473 (size: 19.1 MiB, free: 396.3 MiB)\n",
            "25/08/01 07:11:17 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 930 bytes result sent to driver\n",
            "25/08/01 07:11:17 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1792 ms on e9396b4389d3 (executor driver) (2/2)\n",
            "25/08/01 07:11:17 INFO DAGScheduler: ResultStage 1 (count at Persist.scala:22) finished in 1.833 s\n",
            "25/08/01 07:11:17 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/01 07:11:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/01 07:11:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/01 07:11:17 INFO DAGScheduler: Job 1 finished: count at Persist.scala:22, took 1.852877 s\n",
            "Count: 10000000\n",
            "25/08/01 07:11:17 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/01 07:11:17 INFO SparkUI: Stopped Spark web UI at http://e9396b4389d3:4041\n",
            "25/08/01 07:11:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/01 07:11:18 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/01 07:11:18 INFO BlockManager: BlockManager stopped\n",
            "25/08/01 07:11:18 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/01 07:11:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/01 07:11:18 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/01 07:11:18 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/01 07:11:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-d3844104-e31e-4999-b46b-fa00388be575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gh6bpQnFeFtf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}