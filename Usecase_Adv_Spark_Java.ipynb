{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuoZw0iVzrGGgqE5n5lDjF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishekkm8088/Pyspark-Abhi/blob/main/Usecase_Adv_Spark_Java.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Environment** Setup Instructions"
      ],
      "metadata": {
        "id": "3ah8jM-x_G2g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9QfPIS3q1Ec",
        "outputId": "b9a06745-4b90-4dde-8640-aa9b3003aaac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to store cached files\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set variables\n",
        "strBasePath=\"/content/drive/MyDrive/IBM-DE-Spark-Scala\"\n",
        "scala_deb_path = strBasePath+\"/scala-2.12.18.deb\"\n",
        "spark_tgz_path = strBasePath+\"/spark-3.4.1-bin-hadoop3.tgz\"\n",
        "\n",
        "!mkdir -p /content/tmp\n",
        "import os\n",
        "# Download Scala .deb if not cached\n",
        "if not os.path.exists(scala_deb_path):\n",
        "    !wget -O \"{scala_deb_path}\" https://github.com/scala/scala/releases/download/v2.12.18/scala-2.12.18.deb\n",
        "\n",
        "# Download Spark tgz if not cached\n",
        "if not os.path.exists(spark_tgz_path):\n",
        "    !wget -O \"{spark_tgz_path}\" https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Copy cached files to working dir\n",
        "!cp \"{scala_deb_path}\" /content/tmp/scala-2.12.18.deb\n",
        "!cp \"{spark_tgz_path}\" /content/tmp/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install Java if not already present\n",
        "!java -version || apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install Scala\n",
        "!dpkg -i /content/tmp/scala-2.12.18.deb\n",
        "\n",
        "# Extract Spark\n",
        "!tar xf /content/tmp/spark-3.4.1-bin-hadoop3.tgz -C /content\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += f\":{os.environ['SPARK_HOME']}/bin\"\n",
        "\n",
        "# Confirm installation\n",
        "!java -version\n",
        "!scala -version\n",
        "!scalac -version\n",
        "!echo \"Spark path: $SPARK_HOME\"\n",
        "!ls $SPARK_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXEEq3qxq2Oo",
        "outputId": "de35a25a-48c3-4c8e-9a2f-9719b9001936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Selecting previously unselected package scala.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack /content/tmp/scala-2.12.18.deb ...\n",
            "Unpacking scala (2.12.18-400) ...\n",
            "Setting up scala (2.12.18-400) ...\n",
            "Creating system group: scala\n",
            "Creating system user: scala in scala with scala daemon-user and shell /bin/false\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Scala code runner version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Scala compiler version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Spark path: /content/spark-3.4.1-bin-hadoop3\n",
            "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\n",
            "conf  examples\tkubernetes  licenses  python  README.md  sbin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 1: Data Ingestion & Setup"
      ],
      "metadata": {
        "id": "rPH5PecD_eBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Schema.java\n",
        "import org.apache.spark.sql.*;\n",
        "import org.apache.spark.sql.types.*;\n",
        "\n",
        "public class Schema {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Data Ingestion Assignment\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        // Define schemas manually\n",
        "        StructType productLinesSchema = new StructType()\n",
        "                .add(\"productLine\", DataTypes.StringType)\n",
        "                .add(\"textDescription\", DataTypes.StringType);\n",
        "\n",
        "        StructType productsSchema = new StructType()\n",
        "                .add(\"productCode\", DataTypes.StringType)\n",
        "                .add(\"productName\", DataTypes.StringType)\n",
        "                .add(\"productLine\", DataTypes.StringType);\n",
        "\n",
        "        StructType officesSchema = new StructType()\n",
        "                .add(\"officeCode\", DataTypes.StringType)\n",
        "                .add(\"city\", DataTypes.StringType)\n",
        "                .add(\"country\", DataTypes.StringType);\n",
        "\n",
        "        StructType employeesSchema = new StructType()\n",
        "                .add(\"employeeNumber\", DataTypes.IntegerType)\n",
        "                .add(\"lastName\", DataTypes.StringType)\n",
        "                .add(\"officeCode\", DataTypes.StringType);\n",
        "\n",
        "        StructType customersSchema = new StructType()\n",
        "                .add(\"customerNumber\", DataTypes.IntegerType, true)\n",
        "                .add(\"customerName\", DataTypes.StringType, true)\n",
        "                .add(\"contactLastName\", DataTypes.StringType, true)\n",
        "                .add(\"contactFirstName\", DataTypes.StringType, true)\n",
        "                .add(\"phone\", DataTypes.StringType, true)\n",
        "                .add(\"addressLine1\", DataTypes.StringType, true)\n",
        "                .add(\"addressLine2\", DataTypes.StringType, true)\n",
        "                .add(\"city\", DataTypes.StringType, true)\n",
        "                .add(\"state\", DataTypes.StringType, true)\n",
        "                .add(\"postalCode\", DataTypes.StringType, true)\n",
        "                .add(\"country\", DataTypes.StringType, true)\n",
        "                .add(\"salesRepEmployeeNumber\", DataTypes.IntegerType, true)\n",
        "                .add(\"creditLimit\", DataTypes.DoubleType, true);\n",
        "\n",
        "        StructType paymentsSchema = new StructType()\n",
        "              .add(\"customerNumber\", DataTypes.IntegerType)\n",
        "              .add(\"checkNumber\", DataTypes.StringType)\n",
        "              .add(\"paymentDate\", DataTypes.DateType)\n",
        "              .add(\"amount\", DataTypes.DoubleType);\n",
        "\n",
        "        StructType ordersSchema = new StructType()\n",
        "                .add(\"orderNumber\", DataTypes.IntegerType)\n",
        "                .add(\"orderDate\", DataTypes.StringType)\n",
        "                .add(\"customerNumber\", DataTypes.IntegerType);\n",
        "\n",
        "        StructType orderDetailsSchema = new StructType()\n",
        "                .add(\"orderNumber\", DataTypes.IntegerType)\n",
        "                .add(\"productCode\", DataTypes.StringType)\n",
        "                .add(\"quantityOrdered\", DataTypes.IntegerType)\n",
        "                .add(\"priceEach\", DataTypes.DoubleType)\n",
        "                .add(\"orderLineNumber\", DataTypes.IntegerType);\n",
        "\n",
        "        // Base paths\n",
        "        String inputPath = \"\";\n",
        "        String outputPath = \"data/parquet/\";\n",
        "\n",
        "        // Read and write all tables\n",
        "        readAndSave(spark, inputPath + \"productlines.csv\", outputPath + \"productlines\", productLinesSchema);\n",
        "        readAndSave(spark, inputPath + \"products.csv\", outputPath + \"products\", productsSchema);\n",
        "        readAndSave(spark, inputPath + \"offices.csv\", outputPath + \"offices\", officesSchema);\n",
        "        readAndSave(spark, inputPath + \"employees.csv\", outputPath + \"employees\", employeesSchema);\n",
        "        readAndSave(spark, inputPath + \"customers.csv\", outputPath + \"customers\", customersSchema);\n",
        "        readAndSave(spark, inputPath + \"payments.csv\", outputPath + \"payments\", paymentsSchema);\n",
        "        readAndSave(spark, inputPath + \"orders.csv\", outputPath + \"orders\", ordersSchema);\n",
        "        readAndSave(spark, inputPath + \"orderdetails.csv\", outputPath + \"orderdetails\", orderDetailsSchema);\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "\n",
        "    private static void readAndSave(SparkSession spark, String inputCsvPath, String outputParquetPath, StructType schema) {\n",
        "        Dataset<Row> df = spark.read()\n",
        "                .option(\"header\", \"true\")\n",
        "                .schema(schema)\n",
        "                .csv(inputCsvPath);\n",
        "\n",
        "        df.write()\n",
        "                .mode(SaveMode.Overwrite)\n",
        "                .parquet(outputParquetPath);\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tmj6SMcyJsR",
        "outputId": "935aa289-fe69-4fc4-8c70-182c8e3f22aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Schema.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" Schema.java\n"
      ],
      "metadata": {
        "id": "qm3kTW9I0HEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$SPARK_HOME/jars/*\" Schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWEz7wfQ0mQ5",
        "outputId": "ca4b37bc-d039-475b-d98b-2841b2187aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 08:38:43 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 08:38:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 08:38:43 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:38:43 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 08:38:43 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:38:43 INFO SparkContext: Submitted application: Data Ingestion Assignment\n",
            "25/08/06 08:38:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 08:38:43 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 08:38:43 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 08:38:43 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 08:38:43 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 08:38:43 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 08:38:43 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 08:38:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 08:38:44 INFO Utils: Successfully started service 'sparkDriver' on port 33967.\n",
            "25/08/06 08:38:44 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 08:38:44 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 08:38:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 08:38:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 08:38:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 08:38:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b61e3e9c-54a7-4d48-819e-c9b0b1645910\n",
            "25/08/06 08:38:44 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 08:38:44 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 08:38:44 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 08:38:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 08:38:45 INFO Executor: Starting executor ID driver on host fa3f124442cb\n",
            "25/08/06 08:38:45 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 08:38:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36345.\n",
            "25/08/06 08:38:45 INFO NettyBlockTransferService: Server created on fa3f124442cb:36345\n",
            "25/08/06 08:38:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 08:38:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fa3f124442cb, 36345, None)\n",
            "25/08/06 08:38:45 INFO BlockManagerMasterEndpoint: Registering block manager fa3f124442cb:36345 with 1767.6 MiB RAM, BlockManagerId(driver, fa3f124442cb, 36345, None)\n",
            "25/08/06 08:38:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fa3f124442cb, 36345, None)\n",
            "25/08/06 08:38:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fa3f124442cb, 36345, None)\n",
            "25/08/06 08:38:47 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 08:38:47 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 08:38:49 INFO InMemoryFileIndex: It took 92 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:38:53 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 08:38:53 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 08:38:53 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.5 KiB, free 1767.4 MiB)\n",
            "25/08/06 08:38:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 08:38:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fa3f124442cb:36345 (size: 34.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:38:54 INFO SparkContext: Created broadcast 0 from parquet at Schema.java:89\n",
            "25/08/06 08:38:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:38:54 INFO SparkContext: Starting job: parquet at Schema.java:89\n",
            "25/08/06 08:38:54 INFO DAGScheduler: Got job 0 (parquet at Schema.java:89) with 1 output partitions\n",
            "25/08/06 08:38:54 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at Schema.java:89)\n",
            "25/08/06 08:38:54 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:38:54 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:38:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at parquet at Schema.java:89), which has no missing parents\n",
            "25/08/06 08:38:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 210.4 KiB, free 1767.2 MiB)\n",
            "25/08/06 08:38:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 75.9 KiB, free 1767.1 MiB)\n",
            "25/08/06 08:38:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fa3f124442cb:36345 (size: 75.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:38:54 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:38:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at parquet at Schema.java:89) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:38:54 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:38:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes) \n",
            "25/08/06 08:38:54 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 08:38:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:55 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:38:55 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:38:55 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:38:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productLine\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"textDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productLine (STRING);\n",
            "  optional binary textDescription (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:38:55 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 08:38:55 INFO FileScanRDD: Reading File path: file:///content/productlines.csv, range: 0-3446, partition values: [empty row]\n",
            "25/08/06 08:38:56 INFO CodeGenerator: Code generated in 428.984219 ms\n",
            "25/08/06 08:38:56 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 4, schema size: 2\n",
            "CSV file: file:///content/productlines.csv\n",
            "25/08/06 08:38:57 INFO FileOutputCommitter: Saved output of task 'attempt_202508060838544477542622222286450_0000_m_000000_0' to file:/content/data/parquet/productlines/_temporary/0/task_202508060838544477542622222286450_0000_m_000000\n",
            "25/08/06 08:38:57 INFO SparkHadoopMapRedUtil: attempt_202508060838544477542622222286450_0000_m_000000_0: Committed. Elapsed time: 2 ms.\n",
            "25/08/06 08:38:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2545 bytes result sent to driver\n",
            "25/08/06 08:38:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2496 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:38:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:38:57 INFO DAGScheduler: ResultStage 0 (parquet at Schema.java:89) finished in 2.985 s\n",
            "25/08/06 08:38:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:38:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 08:38:57 INFO DAGScheduler: Job 0 finished: parquet at Schema.java:89, took 3.103037 s\n",
            "25/08/06 08:38:57 INFO FileFormatWriter: Start to commit write Job b5ae6f2f-ec13-4687-b826-c5cd5d64eaab.\n",
            "25/08/06 08:38:57 INFO FileFormatWriter: Write Job b5ae6f2f-ec13-4687-b826-c5cd5d64eaab committed. Elapsed time: 33 ms.\n",
            "25/08/06 08:38:57 INFO FileFormatWriter: Finished processing stats for write job b5ae6f2f-ec13-4687-b826-c5cd5d64eaab.\n",
            "25/08/06 08:38:57 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:38:57 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 08:38:57 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 08:38:57 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:57 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.5 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:38:57 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:38:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fa3f124442cb:36345 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:38:57 INFO SparkContext: Created broadcast 2 from parquet at Schema.java:89\n",
            "25/08/06 08:38:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:38:57 INFO SparkContext: Starting job: parquet at Schema.java:89\n",
            "25/08/06 08:38:57 INFO DAGScheduler: Got job 1 (parquet at Schema.java:89) with 1 output partitions\n",
            "25/08/06 08:38:57 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at Schema.java:89)\n",
            "25/08/06 08:38:57 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:38:57 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:38:57 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at Schema.java:89), which has no missing parents\n",
            "25/08/06 08:38:57 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 210.6 KiB, free 1766.7 MiB)\n",
            "25/08/06 08:38:57 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 76.2 KiB, free 1766.6 MiB)\n",
            "25/08/06 08:38:57 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fa3f124442cb:36345 (size: 76.2 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:57 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:38:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at Schema.java:89) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:38:57 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:38:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7908 bytes) \n",
            "25/08/06 08:38:57 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 08:38:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:57 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:38:57 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:38:57 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:38:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productLine\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productCode (STRING);\n",
            "  optional binary productName (STRING);\n",
            "  optional binary productLine (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:38:57 INFO FileScanRDD: Reading File path: file:///content/products.csv, range: 0-29309, partition values: [empty row]\n",
            "25/08/06 08:38:57 INFO CodeGenerator: Code generated in 19.809562 ms\n",
            "25/08/06 08:38:57 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 9, schema size: 3\n",
            "CSV file: file:///content/products.csv\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: Saved output of task 'attempt_2025080608385720383787787213519_0001_m_000000_1' to file:/content/data/parquet/products/_temporary/0/task_2025080608385720383787787213519_0001_m_000000\n",
            "25/08/06 08:38:58 INFO SparkHadoopMapRedUtil: attempt_2025080608385720383787787213519_0001_m_000000_1: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 08:38:58 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2502 bytes result sent to driver\n",
            "25/08/06 08:38:58 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 214 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:38:58 INFO DAGScheduler: ResultStage 1 (parquet at Schema.java:89) finished in 0.285 s\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:38:58 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:38:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Job 1 finished: parquet at Schema.java:89, took 0.293800 s\n",
            "25/08/06 08:38:58 INFO FileFormatWriter: Start to commit write Job 9008f36c-4aed-4995-a591-0d54f2be0a7e.\n",
            "25/08/06 08:38:58 INFO FileFormatWriter: Write Job 9008f36c-4aed-4995-a591-0d54f2be0a7e committed. Elapsed time: 26 ms.\n",
            "25/08/06 08:38:58 INFO FileFormatWriter: Finished processing stats for write job 9008f36c-4aed-4995-a591-0d54f2be0a7e.\n",
            "25/08/06 08:38:58 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:38:58 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 08:38:58 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 08:38:58 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:58 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 198.5 KiB, free 1766.4 MiB)\n",
            "25/08/06 08:38:58 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.4 MiB)\n",
            "25/08/06 08:38:58 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on fa3f124442cb:36345 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:58 INFO SparkContext: Created broadcast 4 from parquet at Schema.java:89\n",
            "25/08/06 08:38:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:38:58 INFO SparkContext: Starting job: parquet at Schema.java:89\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Got job 2 (parquet at Schema.java:89) with 1 output partitions\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at Schema.java:89)\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at parquet at Schema.java:89), which has no missing parents\n",
            "25/08/06 08:38:58 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 210.6 KiB, free 1766.2 MiB)\n",
            "25/08/06 08:38:58 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.2 KiB, free 1766.1 MiB)\n",
            "25/08/06 08:38:58 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on fa3f124442cb:36345 (size: 76.2 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:38:58 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at parquet at Schema.java:89) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:38:58 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:38:58 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7907 bytes) \n",
            "25/08/06 08:38:58 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:58 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:38:58 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:38:58 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:38:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary officeCode (STRING);\n",
            "  optional binary city (STRING);\n",
            "  optional binary country (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:38:58 INFO FileScanRDD: Reading File path: file:///content/offices.csv, range: 0-585, partition values: [empty row]\n",
            "25/08/06 08:38:58 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 9, schema size: 3\n",
            "CSV file: file:///content/offices.csv\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: Saved output of task 'attempt_202508060838588209706136034116204_0002_m_000000_2' to file:/content/data/parquet/offices/_temporary/0/task_202508060838588209706136034116204_0002_m_000000\n",
            "25/08/06 08:38:58 INFO SparkHadoopMapRedUtil: attempt_202508060838588209706136034116204_0002_m_000000_2: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 08:38:58 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2502 bytes result sent to driver\n",
            "25/08/06 08:38:58 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 117 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:38:58 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:38:58 INFO DAGScheduler: ResultStage 2 (parquet at Schema.java:89) finished in 0.175 s\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:38:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Job 2 finished: parquet at Schema.java:89, took 0.181995 s\n",
            "25/08/06 08:38:58 INFO FileFormatWriter: Start to commit write Job a854ceed-f689-46cc-97cf-da0ee1efcc3a.\n",
            "25/08/06 08:38:58 INFO FileFormatWriter: Write Job a854ceed-f689-46cc-97cf-da0ee1efcc3a committed. Elapsed time: 15 ms.\n",
            "25/08/06 08:38:58 INFO FileFormatWriter: Finished processing stats for write job a854ceed-f689-46cc-97cf-da0ee1efcc3a.\n",
            "25/08/06 08:38:58 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:38:58 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 08:38:58 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 08:38:58 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:58 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 198.5 KiB, free 1765.9 MiB)\n",
            "25/08/06 08:38:58 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.9 MiB)\n",
            "25/08/06 08:38:58 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on fa3f124442cb:36345 (size: 34.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:38:58 INFO SparkContext: Created broadcast 6 from parquet at Schema.java:89\n",
            "25/08/06 08:38:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:38:58 INFO SparkContext: Starting job: parquet at Schema.java:89\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Got job 3 (parquet at Schema.java:89) with 1 output partitions\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at Schema.java:89)\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at parquet at Schema.java:89), which has no missing parents\n",
            "25/08/06 08:38:58 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 210.7 KiB, free 1765.6 MiB)\n",
            "25/08/06 08:38:58 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 1765.6 MiB)\n",
            "25/08/06 08:38:58 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on fa3f124442cb:36345 (size: 76.3 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:38:58 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at parquet at Schema.java:89) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:38:58 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:38:58 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7909 bytes) \n",
            "25/08/06 08:38:58 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:58 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:38:58 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:38:58 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:38:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"employeeNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"lastName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 employeeNumber;\n",
            "  optional binary lastName (STRING);\n",
            "  optional binary officeCode (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:38:58 INFO FileScanRDD: Reading File path: file:///content/employees.csv, range: 0-1781, partition values: [empty row]\n",
            "25/08/06 08:38:58 INFO CodeGenerator: Code generated in 23.009784 ms\n",
            "25/08/06 08:38:58 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 8, schema size: 3\n",
            "CSV file: file:///content/employees.csv\n",
            "25/08/06 08:38:58 INFO FileOutputCommitter: Saved output of task 'attempt_202508060838584398973154324623975_0003_m_000000_3' to file:/content/data/parquet/employees/_temporary/0/task_202508060838584398973154324623975_0003_m_000000\n",
            "25/08/06 08:38:58 INFO SparkHadoopMapRedUtil: attempt_202508060838584398973154324623975_0003_m_000000_3: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 08:38:58 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2502 bytes result sent to driver\n",
            "25/08/06 08:38:58 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 196 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:38:58 INFO DAGScheduler: ResultStage 3 (parquet at Schema.java:89) finished in 0.259 s\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:38:58 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:38:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 08:38:58 INFO DAGScheduler: Job 3 finished: parquet at Schema.java:89, took 0.271961 s\n",
            "25/08/06 08:38:58 INFO FileFormatWriter: Start to commit write Job 85dc6a4b-d7b5-4782-9526-f5e59a52d7a7.\n",
            "25/08/06 08:38:58 INFO FileFormatWriter: Write Job 85dc6a4b-d7b5-4782-9526-f5e59a52d7a7 committed. Elapsed time: 17 ms.\n",
            "25/08/06 08:38:58 INFO FileFormatWriter: Finished processing stats for write job 85dc6a4b-d7b5-4782-9526-f5e59a52d7a7.\n",
            "25/08/06 08:38:58 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:38:59 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 08:38:59 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 08:38:59 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:59 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 198.5 KiB, free 1765.4 MiB)\n",
            "25/08/06 08:38:59 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.3 MiB)\n",
            "25/08/06 08:38:59 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on fa3f124442cb:36345 (size: 34.0 KiB, free: 1767.1 MiB)\n",
            "25/08/06 08:38:59 INFO SparkContext: Created broadcast 8 from parquet at Schema.java:89\n",
            "25/08/06 08:38:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:38:59 INFO SparkContext: Starting job: parquet at Schema.java:89\n",
            "25/08/06 08:38:59 INFO DAGScheduler: Got job 4 (parquet at Schema.java:89) with 1 output partitions\n",
            "25/08/06 08:38:59 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at Schema.java:89)\n",
            "25/08/06 08:38:59 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:38:59 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:38:59 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at parquet at Schema.java:89), which has no missing parents\n",
            "25/08/06 08:38:59 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 213.1 KiB, free 1765.1 MiB)\n",
            "25/08/06 08:38:59 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 77.0 KiB, free 1765.1 MiB)\n",
            "25/08/06 08:38:59 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on fa3f124442cb:36345 (size: 77.0 KiB, free: 1767.1 MiB)\n",
            "25/08/06 08:38:59 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:38:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at parquet at Schema.java:89) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:38:59 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:38:59 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7909 bytes) \n",
            "25/08/06 08:38:59 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 08:38:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:59 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:38:59 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:38:59 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:38:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"contactLastName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"contactFirstName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"phone\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine1\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine2\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"state\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"postalCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"salesRepEmployeeNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"creditLimit\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 customerNumber;\n",
            "  optional binary customerName (STRING);\n",
            "  optional binary contactLastName (STRING);\n",
            "  optional binary contactFirstName (STRING);\n",
            "  optional binary phone (STRING);\n",
            "  optional binary addressLine1 (STRING);\n",
            "  optional binary addressLine2 (STRING);\n",
            "  optional binary city (STRING);\n",
            "  optional binary state (STRING);\n",
            "  optional binary postalCode (STRING);\n",
            "  optional binary country (STRING);\n",
            "  optional int32 salesRepEmployeeNumber;\n",
            "  optional double creditLimit;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:38:59 INFO FileScanRDD: Reading File path: file:///content/customers.csv, range: 0-13923, partition values: [empty row]\n",
            "25/08/06 08:38:59 INFO CodeGenerator: Code generated in 88.797554 ms\n",
            "25/08/06 08:38:59 INFO FileOutputCommitter: Saved output of task 'attempt_202508060838594191797386611401465_0004_m_000000_4' to file:/content/data/parquet/customers/_temporary/0/task_202508060838594191797386611401465_0004_m_000000\n",
            "25/08/06 08:38:59 INFO SparkHadoopMapRedUtil: attempt_202508060838594191797386611401465_0004_m_000000_4: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 08:38:59 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2502 bytes result sent to driver\n",
            "25/08/06 08:38:59 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 384 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:38:59 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:38:59 INFO DAGScheduler: ResultStage 4 (parquet at Schema.java:89) finished in 0.463 s\n",
            "25/08/06 08:38:59 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:38:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 08:38:59 INFO DAGScheduler: Job 4 finished: parquet at Schema.java:89, took 0.469262 s\n",
            "25/08/06 08:38:59 INFO FileFormatWriter: Start to commit write Job 347c57e2-6d4e-4a5b-936b-e0b2d47e1173.\n",
            "25/08/06 08:38:59 INFO FileFormatWriter: Write Job 347c57e2-6d4e-4a5b-936b-e0b2d47e1173 committed. Elapsed time: 20 ms.\n",
            "25/08/06 08:38:59 INFO FileFormatWriter: Finished processing stats for write job 347c57e2-6d4e-4a5b-936b-e0b2d47e1173.\n",
            "25/08/06 08:38:59 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:38:59 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 08:38:59 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 08:38:59 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:59 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 198.5 KiB, free 1764.9 MiB)\n",
            "25/08/06 08:38:59 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1764.8 MiB)\n",
            "25/08/06 08:38:59 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on fa3f124442cb:36345 (size: 34.0 KiB, free: 1767.0 MiB)\n",
            "25/08/06 08:38:59 INFO SparkContext: Created broadcast 10 from parquet at Schema.java:89\n",
            "25/08/06 08:38:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:39:00 INFO BlockManagerInfo: Removed broadcast_1_piece0 on fa3f124442cb:36345 in memory (size: 75.9 KiB, free: 1767.1 MiB)\n",
            "25/08/06 08:39:00 INFO SparkContext: Starting job: parquet at Schema.java:89\n",
            "25/08/06 08:39:00 INFO DAGScheduler: Got job 5 (parquet at Schema.java:89) with 1 output partitions\n",
            "25/08/06 08:39:00 INFO DAGScheduler: Final stage: ResultStage 5 (parquet at Schema.java:89)\n",
            "25/08/06 08:39:00 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:39:00 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:39:00 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[17] at parquet at Schema.java:89), which has no missing parents\n",
            "25/08/06 08:39:00 INFO BlockManagerInfo: Removed broadcast_3_piece0 on fa3f124442cb:36345 in memory (size: 76.2 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:39:00 INFO BlockManagerInfo: Removed broadcast_9_piece0 on fa3f124442cb:36345 in memory (size: 77.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:39:00 INFO BlockManagerInfo: Removed broadcast_6_piece0 on fa3f124442cb:36345 in memory (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:39:00 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 211.0 KiB, free 1765.7 MiB)\n",
            "25/08/06 08:39:00 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 76.4 KiB, free 1765.6 MiB)\n",
            "25/08/06 08:39:00 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on fa3f124442cb:36345 (size: 76.4 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:39:00 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:39:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at parquet at Schema.java:89) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:39:00 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:39:00 INFO BlockManagerInfo: Removed broadcast_7_piece0 on fa3f124442cb:36345 in memory (size: 76.3 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:39:00 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7908 bytes) \n",
            "25/08/06 08:39:00 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 08:39:00 INFO BlockManagerInfo: Removed broadcast_5_piece0 on fa3f124442cb:36345 in memory (size: 76.2 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:39:00 INFO BlockManagerInfo: Removed broadcast_8_piece0 on fa3f124442cb:36345 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:39:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:39:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:39:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:39:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:39:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:39:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:39:00 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:39:00 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:39:00 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:39:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"checkNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"paymentDate\",\n",
            "    \"type\" : \"date\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"amount\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 customerNumber;\n",
            "  optional binary checkNumber (STRING);\n",
            "  optional int32 paymentDate (DATE);\n",
            "  optional double amount;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:39:00 INFO BlockManagerInfo: Removed broadcast_0_piece0 on fa3f124442cb:36345 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:39:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on fa3f124442cb:36345 in memory (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:39:00 INFO FileScanRDD: Reading File path: file:///content/payments.csv, range: 0-8968, partition values: [empty row]\n",
            "25/08/06 08:39:00 INFO BlockManagerInfo: Removed broadcast_4_piece0 on fa3f124442cb:36345 in memory (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:39:00 INFO CodeGenerator: Code generated in 71.72487 ms\n",
            "25/08/06 08:39:00 INFO FileOutputCommitter: Saved output of task 'attempt_202508060838592950857340240737938_0005_m_000000_5' to file:/content/data/parquet/payments/_temporary/0/task_202508060838592950857340240737938_0005_m_000000\n",
            "25/08/06 08:39:00 INFO SparkHadoopMapRedUtil: attempt_202508060838592950857340240737938_0005_m_000000_5: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 08:39:00 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2459 bytes result sent to driver\n",
            "25/08/06 08:39:00 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 741 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:39:00 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:39:00 INFO DAGScheduler: ResultStage 5 (parquet at Schema.java:89) finished in 0.868 s\n",
            "25/08/06 08:39:00 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:39:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 08:39:00 INFO DAGScheduler: Job 5 finished: parquet at Schema.java:89, took 0.887121 s\n",
            "25/08/06 08:39:00 INFO FileFormatWriter: Start to commit write Job 815a9afc-c200-4f0b-b2e3-1244f5b544c5.\n",
            "25/08/06 08:39:00 INFO FileFormatWriter: Write Job 815a9afc-c200-4f0b-b2e3-1244f5b544c5 committed. Elapsed time: 23 ms.\n",
            "25/08/06 08:39:00 INFO FileFormatWriter: Finished processing stats for write job 815a9afc-c200-4f0b-b2e3-1244f5b544c5.\n",
            "25/08/06 08:39:00 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:39:01 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 08:39:01 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 08:39:01 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:39:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:39:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:39:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:39:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:39:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:39:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:39:01 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 198.5 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:39:01 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:39:01 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on fa3f124442cb:36345 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:39:01 INFO SparkContext: Created broadcast 12 from parquet at Schema.java:89\n",
            "25/08/06 08:39:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:39:01 INFO SparkContext: Starting job: parquet at Schema.java:89\n",
            "25/08/06 08:39:01 INFO DAGScheduler: Got job 6 (parquet at Schema.java:89) with 1 output partitions\n",
            "25/08/06 08:39:01 INFO DAGScheduler: Final stage: ResultStage 6 (parquet at Schema.java:89)\n",
            "25/08/06 08:39:01 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:39:01 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:39:01 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[20] at parquet at Schema.java:89), which has no missing parents\n",
            "25/08/06 08:39:01 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 210.7 KiB, free 1766.7 MiB)\n",
            "25/08/06 08:39:01 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 1766.6 MiB)\n",
            "25/08/06 08:39:01 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on fa3f124442cb:36345 (size: 76.3 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:39:01 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:39:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at parquet at Schema.java:89) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:39:01 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:39:01 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7906 bytes) \n",
            "25/08/06 08:39:01 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "25/08/06 08:39:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:39:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:39:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:39:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:39:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:39:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:39:01 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:39:01 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:39:01 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:39:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"orderNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 orderNumber;\n",
            "  optional binary orderDate (STRING);\n",
            "  optional int32 customerNumber;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:39:01 INFO FileScanRDD: Reading File path: file:///content/orders.csv, range: 0-23548, partition values: [empty row]\n",
            "25/08/06 08:39:01 INFO CodeGenerator: Code generated in 62.026593 ms\n",
            "25/08/06 08:39:01 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 7, schema size: 3\n",
            "CSV file: file:///content/orders.csv\n",
            "25/08/06 08:39:01 INFO FileOutputCommitter: Saved output of task 'attempt_202508060839015528722161132682635_0006_m_000000_6' to file:/content/data/parquet/orders/_temporary/0/task_202508060839015528722161132682635_0006_m_000000\n",
            "25/08/06 08:39:01 INFO SparkHadoopMapRedUtil: attempt_202508060839015528722161132682635_0006_m_000000_6: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 08:39:01 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2459 bytes result sent to driver\n",
            "25/08/06 08:39:01 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 360 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:39:01 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:39:01 INFO DAGScheduler: ResultStage 6 (parquet at Schema.java:89) finished in 0.447 s\n",
            "25/08/06 08:39:01 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:39:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 08:39:01 INFO DAGScheduler: Job 6 finished: parquet at Schema.java:89, took 0.464558 s\n",
            "25/08/06 08:39:01 INFO FileFormatWriter: Start to commit write Job e78560b8-b2c1-427b-abe2-9be1a0e608d6.\n",
            "25/08/06 08:39:01 INFO FileFormatWriter: Write Job e78560b8-b2c1-427b-abe2-9be1a0e608d6 committed. Elapsed time: 30 ms.\n",
            "25/08/06 08:39:01 INFO FileFormatWriter: Finished processing stats for write job e78560b8-b2c1-427b-abe2-9be1a0e608d6.\n",
            "25/08/06 08:39:01 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:39:01 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 08:39:01 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 08:39:01 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:39:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:39:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:39:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:39:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:39:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:39:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:39:02 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 198.5 KiB, free 1766.4 MiB)\n",
            "25/08/06 08:39:02 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.4 MiB)\n",
            "25/08/06 08:39:02 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on fa3f124442cb:36345 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:39:02 INFO SparkContext: Created broadcast 14 from parquet at Schema.java:89\n",
            "25/08/06 08:39:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:39:02 INFO SparkContext: Starting job: parquet at Schema.java:89\n",
            "25/08/06 08:39:02 INFO DAGScheduler: Got job 7 (parquet at Schema.java:89) with 1 output partitions\n",
            "25/08/06 08:39:02 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at Schema.java:89)\n",
            "25/08/06 08:39:02 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:39:02 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:39:02 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[23] at parquet at Schema.java:89), which has no missing parents\n",
            "25/08/06 08:39:02 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 211.2 KiB, free 1766.2 MiB)\n",
            "25/08/06 08:39:02 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 76.5 KiB, free 1766.1 MiB)\n",
            "25/08/06 08:39:02 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on fa3f124442cb:36345 (size: 76.5 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:39:02 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:39:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at parquet at Schema.java:89) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:39:02 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:39:02 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes) \n",
            "25/08/06 08:39:02 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "25/08/06 08:39:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:39:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:39:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:39:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:39:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:39:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:39:02 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:39:02 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:39:02 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:39:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"orderNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"quantityOrdered\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"priceEach\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderLineNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 orderNumber;\n",
            "  optional binary productCode (STRING);\n",
            "  optional int32 quantityOrdered;\n",
            "  optional double priceEach;\n",
            "  optional int32 orderLineNumber;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:39:02 INFO FileScanRDD: Reading File path: file:///content/orderdetails.csv, range: 0-79703, partition values: [empty row]\n",
            "25/08/06 08:39:02 INFO CodeGenerator: Code generated in 60.02297 ms\n",
            "25/08/06 08:39:02 INFO FileOutputCommitter: Saved output of task 'attempt_202508060839028415213456989814990_0007_m_000000_7' to file:/content/data/parquet/orderdetails/_temporary/0/task_202508060839028415213456989814990_0007_m_000000\n",
            "25/08/06 08:39:02 INFO SparkHadoopMapRedUtil: attempt_202508060839028415213456989814990_0007_m_000000_7: Committed. Elapsed time: 10 ms.\n",
            "25/08/06 08:39:02 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2502 bytes result sent to driver\n",
            "25/08/06 08:39:02 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 399 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:39:02 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:39:02 INFO DAGScheduler: ResultStage 7 (parquet at Schema.java:89) finished in 0.508 s\n",
            "25/08/06 08:39:02 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:39:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 08:39:02 INFO DAGScheduler: Job 7 finished: parquet at Schema.java:89, took 0.517737 s\n",
            "25/08/06 08:39:02 INFO FileFormatWriter: Start to commit write Job b2d0a3bc-0cde-4d2a-89f7-a93987c3d24f.\n",
            "25/08/06 08:39:02 INFO FileFormatWriter: Write Job b2d0a3bc-0cde-4d2a-89f7-a93987c3d24f committed. Elapsed time: 28 ms.\n",
            "25/08/06 08:39:02 INFO FileFormatWriter: Finished processing stats for write job b2d0a3bc-0cde-4d2a-89f7-a93987c3d24f.\n",
            "25/08/06 08:39:02 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 08:39:02 INFO SparkUI: Stopped Spark web UI at http://fa3f124442cb:4040\n",
            "25/08/06 08:39:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 08:39:02 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 08:39:02 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 08:39:02 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 08:39:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 08:39:02 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 08:39:02 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 08:39:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-e0c8b299-2043-48b5-8a3d-949c4087c112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2: Product & Order Analysis"
      ],
      "metadata": {
        "id": "jDLb4BgB2J-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile SparkAnalysis.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class SparkAnalysis {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"Product and Order Analysis\")\n",
        "            .master(\"local[*]\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        Dataset<Row> orderDetails = spark.read().parquet(\"data/parquet/orderdetails\");\n",
        "        Dataset<Row> orders = spark.read().parquet(\"data/parquet/orders\");\n",
        "        Dataset<Row> products = spark.read().parquet(\"data/parquet/products\");\n",
        "\n",
        "        // Top 10 products by quantity sold\n",
        "        Dataset<Row> topProducts = orderDetails.groupBy(\"productCode\")\n",
        "            .sum(\"quantityOrdered\")\n",
        "            .orderBy(functions.desc(\"sum(quantityOrdered)\"))\n",
        "            .limit(10);\n",
        "\n",
        "        topProducts.show();\n",
        "\n",
        "        // Join for product-wise revenue\n",
        "        Dataset<Row> revenueData = orderDetails\n",
        "            .join(products, \"productCode\")\n",
        "            .join(orders, \"orderNumber\")\n",
        "            .withColumn(\"revenue\", functions.expr(\"quantityOrdered * priceEach\"));\n",
        "\n",
        "        Dataset<Row> productRevenue = revenueData.groupBy(\"productCode\", \"productName\")\n",
        "            .agg(functions.sum(\"revenue\").alias(\"totalRevenue\"))\n",
        "            .orderBy(functions.desc(\"totalRevenue\"));\n",
        "\n",
        "        productRevenue.show();\n",
        "\n",
        "        // Average order value per customer\n",
        "        Dataset<Row> customerAOV = revenueData.groupBy(\"customerNumber\")\n",
        "            .agg(functions.sum(\"revenue\").alias(\"totalSpent\"),\n",
        "                 functions.countDistinct(\"orderNumber\").alias(\"orderCount\"))\n",
        "            .withColumn(\"averageOrderValue\", functions.expr(\"totalSpent / orderCount\"));\n",
        "\n",
        "        customerAOV.show();\n",
        "\n",
        "        // Save results\n",
        "        topProducts.write().mode(\"overwrite\").parquet(\"data/results/top_products\");\n",
        "        productRevenue.write().mode(\"overwrite\").parquet(\"data/results/product_revenue\");\n",
        "        customerAOV.write().mode(\"overwrite\").parquet(\"data/results/customer_aov\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87AU_Vb634dC",
        "outputId": "914a05d9-5ed2-45b0-9431-f566be794a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing SparkAnalysis.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" SparkAnalysis.java\n"
      ],
      "metadata": {
        "id": "MW0e0NI04SS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$SPARK_HOME/jars/*\" SparkAnalysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIRPF58Q9zgh",
        "outputId": "6150fc75-b26d-41fe-b7dc-1a95aa5f5356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 08:40:01 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 08:40:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 08:40:01 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:40:01 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 08:40:01 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:40:01 INFO SparkContext: Submitted application: Product and Order Analysis\n",
            "25/08/06 08:40:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 08:40:01 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 08:40:01 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 08:40:01 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 08:40:01 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 08:40:01 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 08:40:01 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 08:40:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 08:40:02 INFO Utils: Successfully started service 'sparkDriver' on port 45035.\n",
            "25/08/06 08:40:02 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 08:40:02 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 08:40:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 08:40:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 08:40:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 08:40:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9a2afb4d-df00-4539-a408-56a26fbfda35\n",
            "25/08/06 08:40:02 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 08:40:02 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 08:40:03 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 08:40:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 08:40:03 INFO Executor: Starting executor ID driver on host fa3f124442cb\n",
            "25/08/06 08:40:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 08:40:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41989.\n",
            "25/08/06 08:40:03 INFO NettyBlockTransferService: Server created on fa3f124442cb:41989\n",
            "25/08/06 08:40:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 08:40:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fa3f124442cb, 41989, None)\n",
            "25/08/06 08:40:03 INFO BlockManagerMasterEndpoint: Registering block manager fa3f124442cb:41989 with 1767.6 MiB RAM, BlockManagerId(driver, fa3f124442cb, 41989, None)\n",
            "25/08/06 08:40:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fa3f124442cb, 41989, None)\n",
            "25/08/06 08:40:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fa3f124442cb, 41989, None)\n",
            "25/08/06 08:40:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 08:40:04 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 08:40:06 INFO InMemoryFileIndex: It took 228 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:40:08 INFO SparkContext: Starting job: parquet at SparkAnalysis.java:10\n",
            "25/08/06 08:40:08 INFO DAGScheduler: Got job 0 (parquet at SparkAnalysis.java:10) with 1 output partitions\n",
            "25/08/06 08:40:08 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at SparkAnalysis.java:10)\n",
            "25/08/06 08:40:08 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:40:08 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at SparkAnalysis.java:10), which has no missing parents\n",
            "25/08/06 08:40:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:40:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:40:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fa3f124442cb:41989 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:40:08 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at SparkAnalysis.java:10) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7565 bytes) \n",
            "25/08/06 08:40:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 08:40:10 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2099 bytes result sent to driver\n",
            "25/08/06 08:40:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1197 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:10 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:10 INFO DAGScheduler: ResultStage 0 (parquet at SparkAnalysis.java:10) finished in 1.810 s\n",
            "25/08/06 08:40:10 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 08:40:10 INFO DAGScheduler: Job 0 finished: parquet at SparkAnalysis.java:10, took 1.985169 s\n",
            "25/08/06 08:40:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on fa3f124442cb:41989 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:40:12 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:40:12 INFO SparkContext: Starting job: parquet at SparkAnalysis.java:11\n",
            "25/08/06 08:40:12 INFO DAGScheduler: Got job 1 (parquet at SparkAnalysis.java:11) with 1 output partitions\n",
            "25/08/06 08:40:12 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at SparkAnalysis.java:11)\n",
            "25/08/06 08:40:12 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:40:12 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at SparkAnalysis.java:11), which has no missing parents\n",
            "25/08/06 08:40:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:40:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:40:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fa3f124442cb:41989 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:40:12 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at SparkAnalysis.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7559 bytes) \n",
            "25/08/06 08:40:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 08:40:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1874 bytes result sent to driver\n",
            "25/08/06 08:40:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 64 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:12 INFO DAGScheduler: ResultStage 1 (parquet at SparkAnalysis.java:11) finished in 0.123 s\n",
            "25/08/06 08:40:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 08:40:12 INFO DAGScheduler: Job 1 finished: parquet at SparkAnalysis.java:11, took 0.135993 s\n",
            "25/08/06 08:40:12 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:40:12 INFO SparkContext: Starting job: parquet at SparkAnalysis.java:12\n",
            "25/08/06 08:40:12 INFO DAGScheduler: Got job 2 (parquet at SparkAnalysis.java:12) with 1 output partitions\n",
            "25/08/06 08:40:12 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at SparkAnalysis.java:12)\n",
            "25/08/06 08:40:12 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:40:13 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:13 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at SparkAnalysis.java:12), which has no missing parents\n",
            "25/08/06 08:40:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 102.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 08:40:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.3 MiB)\n",
            "25/08/06 08:40:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fa3f124442cb:41989 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:40:13 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at SparkAnalysis.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:13 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7561 bytes) \n",
            "25/08/06 08:40:13 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 08:40:13 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1863 bytes result sent to driver\n",
            "25/08/06 08:40:13 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 37 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:13 INFO DAGScheduler: ResultStage 2 (parquet at SparkAnalysis.java:12) finished in 0.097 s\n",
            "25/08/06 08:40:13 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:13 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 08:40:13 INFO DAGScheduler: Job 2 finished: parquet at SparkAnalysis.java:12, took 0.107590 s\n",
            "25/08/06 08:40:14 INFO BlockManagerInfo: Removed broadcast_2_piece0 on fa3f124442cb:41989 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:40:14 INFO BlockManagerInfo: Removed broadcast_1_piece0 on fa3f124442cb:41989 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:40:14 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 08:40:14 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 08:40:15 INFO CodeGenerator: Code generated in 608.733433 ms\n",
            "25/08/06 08:40:15 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.2 KiB, free 1767.4 MiB)\n",
            "25/08/06 08:40:15 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1767.4 MiB)\n",
            "25/08/06 08:40:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fa3f124442cb:41989 (size: 34.8 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:40:15 INFO SparkContext: Created broadcast 3 from show at SparkAnalysis.java:20\n",
            "25/08/06 08:40:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:40:15 INFO DAGScheduler: Registering RDD 9 (show at SparkAnalysis.java:20) as input to shuffle 0\n",
            "25/08/06 08:40:15 INFO DAGScheduler: Got map stage job 3 (show at SparkAnalysis.java:20) with 1 output partitions\n",
            "25/08/06 08:40:15 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (show at SparkAnalysis.java:20)\n",
            "25/08/06 08:40:15 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:40:15 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:15 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[9] at show at SparkAnalysis.java:20), which has no missing parents\n",
            "25/08/06 08:40:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 40.4 KiB, free 1767.3 MiB)\n",
            "25/08/06 08:40:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 18.2 KiB, free 1767.3 MiB)\n",
            "25/08/06 08:40:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on fa3f124442cb:41989 (size: 18.2 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:40:15 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[9] at show at SparkAnalysis.java:20) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:15 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:15 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7978 bytes) \n",
            "25/08/06 08:40:15 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 08:40:15 INFO CodeGenerator: Code generated in 23.622937 ms\n",
            "25/08/06 08:40:15 INFO CodeGenerator: Code generated in 13.356741 ms\n",
            "25/08/06 08:40:15 INFO CodeGenerator: Code generated in 31.246486 ms\n",
            "25/08/06 08:40:15 INFO CodeGenerator: Code generated in 22.193189 ms\n",
            "25/08/06 08:40:15 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-e416c70d-cd9e-43af-986e-3ba02e998a1e-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 08:40:16 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 08:40:16 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3016 bytes result sent to driver\n",
            "25/08/06 08:40:16 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1193 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:16 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:16 INFO DAGScheduler: ShuffleMapStage 3 (show at SparkAnalysis.java:20) finished in 1.230 s\n",
            "25/08/06 08:40:16 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 08:40:16 INFO DAGScheduler: running: Set()\n",
            "25/08/06 08:40:16 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 08:40:16 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 08:40:16 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 08:40:16 INFO CodeGenerator: Code generated in 32.777346 ms\n",
            "25/08/06 08:40:16 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 08:40:17 INFO CodeGenerator: Code generated in 75.755514 ms\n",
            "25/08/06 08:40:17 INFO SparkContext: Starting job: show at SparkAnalysis.java:20\n",
            "25/08/06 08:40:17 INFO DAGScheduler: Got job 4 (show at SparkAnalysis.java:20) with 1 output partitions\n",
            "25/08/06 08:40:17 INFO DAGScheduler: Final stage: ResultStage 5 (show at SparkAnalysis.java:20)\n",
            "25/08/06 08:40:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "25/08/06 08:40:17 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:17 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[13] at show at SparkAnalysis.java:20), which has no missing parents\n",
            "25/08/06 08:40:17 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 41.2 KiB, free 1767.3 MiB)\n",
            "25/08/06 08:40:17 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 1767.3 MiB)\n",
            "25/08/06 08:40:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on fa3f124442cb:41989 (size: 19.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:40:17 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[13] at show at SparkAnalysis.java:20) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:17 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:17 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (fa3f124442cb, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 08:40:17 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)\n",
            "25/08/06 08:40:17 INFO ShuffleBlockFetcherIterator: Getting 1 (6.4 KiB) non-empty blocks including 1 (6.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:40:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms\n",
            "25/08/06 08:40:17 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 5818 bytes result sent to driver\n",
            "25/08/06 08:40:17 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 195 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:17 INFO DAGScheduler: ResultStage 5 (show at SparkAnalysis.java:20) finished in 0.219 s\n",
            "25/08/06 08:40:17 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:17 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 08:40:17 INFO DAGScheduler: Job 4 finished: show at SparkAnalysis.java:20, took 0.248536 s\n",
            "25/08/06 08:40:17 INFO CodeGenerator: Code generated in 51.65958 ms\n",
            "25/08/06 08:40:17 INFO CodeGenerator: Code generated in 29.325619 ms\n",
            "+-----------+--------------------+\n",
            "|productCode|sum(quantityOrdered)|\n",
            "+-----------+--------------------+\n",
            "|   S18_3232|                1808|\n",
            "|   S18_1342|                1111|\n",
            "|  S700_4002|                1085|\n",
            "|   S18_3856|                1076|\n",
            "|   S50_1341|                1074|\n",
            "|   S18_4600|                1061|\n",
            "|   S10_1678|                1057|\n",
            "|   S12_4473|                1056|\n",
            "|   S18_2319|                1053|\n",
            "|   S24_3856|                1052|\n",
            "+-----------+--------------------+\n",
            "\n",
            "25/08/06 08:40:18 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 08:40:18 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 08:40:18 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 08:40:18 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#16)\n",
            "25/08/06 08:40:18 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)\n",
            "25/08/06 08:40:18 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#10)\n",
            "25/08/06 08:40:18 INFO CodeGenerator: Code generated in 27.579247 ms\n",
            "25/08/06 08:40:18 INFO CodeGenerator: Code generated in 29.334533 ms\n",
            "25/08/06 08:40:18 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 08:40:18 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 200.2 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:40:18 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 1766.8 MiB)\n",
            "25/08/06 08:40:18 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on fa3f124442cb:41989 (size: 34.6 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:40:18 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:40:18 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1766.8 MiB)\n",
            "25/08/06 08:40:18 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on fa3f124442cb:41989 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:40:18 INFO SparkContext: Created broadcast 7 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:40:18 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:18 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:18 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:40:18 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:40:18 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:40:18 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:18 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:40:18 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 13.6 KiB, free 1766.8 MiB)\n",
            "25/08/06 08:40:18 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.8 MiB)\n",
            "25/08/06 08:40:18 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on fa3f124442cb:41989 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:40:18 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:18 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:18 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 08:40:18 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:40:18 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:40:18 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:40:18 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)\n",
            "25/08/06 08:40:18 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:18 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:40:18 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 12.9 KiB, free 1766.8 MiB)\n",
            "25/08/06 08:40:18 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1766.8 MiB)\n",
            "25/08/06 08:40:18 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on fa3f124442cb:41989 (size: 6.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:40:18 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:18 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:18 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7983 bytes) \n",
            "25/08/06 08:40:18 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)\n",
            "25/08/06 08:40:18 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products/part-00000-014e40be-2e5d-4608-8367-4ae8c0d15af6-c000.snappy.parquet, range: 0-4216, partition values: [empty row]\n",
            "25/08/06 08:40:18 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orders/part-00000-ab60cd2c-3d7b-43fd-98bb-649efba8034c-c000.snappy.parquet, range: 0-3888, partition values: [empty row]\n",
            "25/08/06 08:40:18 INFO FilterCompat: Filtering using predicate: noteq(orderNumber, null)\n",
            "25/08/06 08:40:18 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 08:40:18 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 5462 bytes result sent to driver\n",
            "25/08/06 08:40:18 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 255 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:18 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:18 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.277 s\n",
            "25/08/06 08:40:18 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 08:40:18 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.286614 s\n",
            "25/08/06 08:40:18 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 3502 bytes result sent to driver\n",
            "25/08/06 08:40:18 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 227 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:18 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:18 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.251 s\n",
            "25/08/06 08:40:18 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 08:40:18 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.295589 s\n",
            "25/08/06 08:40:18 INFO CodeGenerator: Code generated in 12.173005 ms\n",
            "25/08/06 08:40:18 INFO CodeGenerator: Code generated in 18.26044 ms\n",
            "25/08/06 08:40:18 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 32.0 MiB, free 1734.8 MiB)\n",
            "25/08/06 08:40:18 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 1026.6 KiB, free 1733.8 MiB)\n",
            "25/08/06 08:40:18 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 1733.7 MiB)\n",
            "25/08/06 08:40:18 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on fa3f124442cb:41989 (size: 3.4 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:18 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 1733.7 MiB)\n",
            "25/08/06 08:40:18 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on fa3f124442cb:41989 (size: 4.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:18 INFO SparkContext: Created broadcast 11 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:18 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:18 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 08:40:18 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 08:40:18 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 08:40:18 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 08:40:19 INFO CodeGenerator: Code generated in 185.10731 ms\n",
            "25/08/06 08:40:19 INFO BlockManagerInfo: Removed broadcast_4_piece0 on fa3f124442cb:41989 in memory (size: 18.2 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:40:19 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 200.5 KiB, free 1733.6 MiB)\n",
            "25/08/06 08:40:19 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1733.6 MiB)\n",
            "25/08/06 08:40:19 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on fa3f124442cb:41989 (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:19 INFO SparkContext: Created broadcast 12 from show at SparkAnalysis.java:32\n",
            "25/08/06 08:40:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:40:19 INFO BlockManagerInfo: Removed broadcast_8_piece0 on fa3f124442cb:41989 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:19 INFO BlockManagerInfo: Removed broadcast_5_piece0 on fa3f124442cb:41989 in memory (size: 19.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:40:19 INFO DAGScheduler: Registering RDD 25 (show at SparkAnalysis.java:32) as input to shuffle 1\n",
            "25/08/06 08:40:19 INFO DAGScheduler: Got map stage job 7 (show at SparkAnalysis.java:32) with 1 output partitions\n",
            "25/08/06 08:40:19 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (show at SparkAnalysis.java:32)\n",
            "25/08/06 08:40:19 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:40:19 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:19 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[25] at show at SparkAnalysis.java:32), which has no missing parents\n",
            "25/08/06 08:40:19 INFO BlockManagerInfo: Removed broadcast_3_piece0 on fa3f124442cb:41989 in memory (size: 34.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:40:19 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 56.2 KiB, free 1733.8 MiB)\n",
            "25/08/06 08:40:19 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 1733.8 MiB)\n",
            "25/08/06 08:40:19 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on fa3f124442cb:41989 (size: 24.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:40:19 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:19 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[25] at show at SparkAnalysis.java:32) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:19 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:19 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7978 bytes) \n",
            "25/08/06 08:40:19 INFO Executor: Running task 0.0 in stage 8.0 (TID 7)\n",
            "25/08/06 08:40:19 INFO BlockManagerInfo: Removed broadcast_9_piece0 on fa3f124442cb:41989 in memory (size: 6.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:40:19 INFO CodeGenerator: Code generated in 48.269588 ms\n",
            "25/08/06 08:40:19 INFO CodeGenerator: Code generated in 18.418736 ms\n",
            "25/08/06 08:40:19 INFO CodeGenerator: Code generated in 20.175903 ms\n",
            "25/08/06 08:40:19 INFO CodeGenerator: Code generated in 12.757552 ms\n",
            "25/08/06 08:40:19 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-e416c70d-cd9e-43af-986e-3ba02e998a1e-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 08:40:19 INFO FilterCompat: Filtering using predicate: and(noteq(productCode, null), noteq(orderNumber, null))\n",
            "25/08/06 08:40:19 INFO Executor: Finished task 0.0 in stage 8.0 (TID 7). 4723 bytes result sent to driver\n",
            "25/08/06 08:40:19 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 412 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:19 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:19 INFO DAGScheduler: ShuffleMapStage 8 (show at SparkAnalysis.java:32) finished in 0.440 s\n",
            "25/08/06 08:40:19 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 08:40:19 INFO DAGScheduler: running: Set()\n",
            "25/08/06 08:40:19 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 08:40:19 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 08:40:19 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 08:40:19 INFO CodeGenerator: Code generated in 22.807465 ms\n",
            "25/08/06 08:40:19 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 08:40:20 INFO CodeGenerator: Code generated in 93.759848 ms\n",
            "25/08/06 08:40:20 INFO SparkContext: Starting job: show at SparkAnalysis.java:32\n",
            "25/08/06 08:40:20 INFO DAGScheduler: Got job 8 (show at SparkAnalysis.java:32) with 1 output partitions\n",
            "25/08/06 08:40:20 INFO DAGScheduler: Final stage: ResultStage 10 (show at SparkAnalysis.java:32)\n",
            "25/08/06 08:40:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\n",
            "25/08/06 08:40:20 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:20 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[29] at show at SparkAnalysis.java:32), which has no missing parents\n",
            "25/08/06 08:40:20 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 52.3 KiB, free 1733.8 MiB)\n",
            "25/08/06 08:40:20 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 1733.7 MiB)\n",
            "25/08/06 08:40:20 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on fa3f124442cb:41989 (size: 23.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:20 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[29] at show at SparkAnalysis.java:32) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:20 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:20 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (fa3f124442cb, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 08:40:20 INFO Executor: Running task 0.0 in stage 10.0 (TID 8)\n",
            "25/08/06 08:40:20 INFO ShuffleBlockFetcherIterator: Getting 1 (11.1 KiB) non-empty blocks including 1 (11.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:40:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "25/08/06 08:40:20 INFO Executor: Finished task 0.0 in stage 10.0 (TID 8). 8800 bytes result sent to driver\n",
            "25/08/06 08:40:20 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 95 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:20 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:20 INFO DAGScheduler: ResultStage 10 (show at SparkAnalysis.java:32) finished in 0.123 s\n",
            "25/08/06 08:40:20 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
            "25/08/06 08:40:20 INFO DAGScheduler: Job 8 finished: show at SparkAnalysis.java:32, took 0.152971 s\n",
            "25/08/06 08:40:20 INFO CodeGenerator: Code generated in 41.140881 ms\n",
            "25/08/06 08:40:20 INFO CodeGenerator: Code generated in 31.687734 ms\n",
            "+-----------+--------------------+------------------+\n",
            "|productCode|         productName|      totalRevenue|\n",
            "+-----------+--------------------+------------------+\n",
            "|   S18_3232|1992 Ferrari 360 ...|         276839.98|\n",
            "|   S12_1108|   2001 Ferrari Enzo|         190755.86|\n",
            "|   S10_1949|1952 Alpine Renau...|190017.95999999996|\n",
            "|   S10_4698|2003 Harley-David...|170685.99999999997|\n",
            "|   S12_1099|   1968 Ford Mustang|161531.47999999992|\n",
            "|   S12_3891|    1969 Ford Falcon|         152543.02|\n",
            "|   S18_1662|1980s Black Hawk ...|144959.90999999997|\n",
            "|   S18_2238|1998 Chrysler Ply...|142530.62999999998|\n",
            "|   S18_1749|1917 Grand Tourin...|140535.60000000003|\n",
            "|   S12_2823|    2002 Suzuki XREO|135767.03000000003|\n",
            "|   S24_3856|1956 Porsche 356A...|         134240.71|\n",
            "|   S12_3148|  1969 Corvair Monza|132363.78999999998|\n",
            "|   S18_2795|1928 Mercedes-Ben...|132275.97999999998|\n",
            "|   S18_4721|1957 Corvette Con...|130749.31000000001|\n",
            "|   S10_4757| 1972 Alfa Romeo GTA|127924.31999999999|\n",
            "|   S10_4962|1962 LanciaA Delt...|123123.00999999998|\n",
            "|   S18_4027|1970 Triumph Spit...|         122254.75|\n",
            "|   S18_3482|1976 Ford Gran To...|          121890.6|\n",
            "|   S18_3685|1948 Porsche Type...|         121653.46|\n",
            "|   S12_1666|      1958 Setra Bus|119085.24999999999|\n",
            "+-----------+--------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "25/08/06 08:40:20 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 08:40:20 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 08:40:20 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 08:40:20 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#16)\n",
            "25/08/06 08:40:20 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)\n",
            "25/08/06 08:40:20 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#10)\n",
            "25/08/06 08:40:21 INFO CodeGenerator: Code generated in 41.598413 ms\n",
            "25/08/06 08:40:21 INFO CodeGenerator: Code generated in 43.966565 ms\n",
            "25/08/06 08:40:21 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 200.2 KiB, free 1733.4 MiB)\n",
            "25/08/06 08:40:21 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 200.1 KiB, free 1733.4 MiB)\n",
            "25/08/06 08:40:21 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1733.3 MiB)\n",
            "25/08/06 08:40:21 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on fa3f124442cb:41989 (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:21 INFO SparkContext: Created broadcast 16 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:40:21 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1733.3 MiB)\n",
            "25/08/06 08:40:21 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on fa3f124442cb:41989 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:21 INFO SparkContext: Created broadcast 15 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:40:21 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Final stage: ResultStage 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:40:21 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 13.6 KiB, free 1733.3 MiB)\n",
            "25/08/06 08:40:21 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1733.3 MiB)\n",
            "25/08/06 08:40:21 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on fa3f124442cb:41989 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:21 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:21 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:21 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7983 bytes) \n",
            "25/08/06 08:40:21 INFO Executor: Running task 0.0 in stage 11.0 (TID 9)\n",
            "25/08/06 08:40:21 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orders/part-00000-ab60cd2c-3d7b-43fd-98bb-649efba8034c-c000.snappy.parquet, range: 0-3888, partition values: [empty row]\n",
            "25/08/06 08:40:21 INFO FilterCompat: Filtering using predicate: noteq(orderNumber, null)\n",
            "25/08/06 08:40:21 INFO Executor: Finished task 0.0 in stage 11.0 (TID 9). 3460 bytes result sent to driver\n",
            "25/08/06 08:40:21 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Got job 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Final stage: ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[37] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:40:21 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 91 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:21 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:21 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 13.0 KiB, free 1733.3 MiB)\n",
            "25/08/06 08:40:21 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1733.2 MiB)\n",
            "25/08/06 08:40:21 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on fa3f124442cb:41989 (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:21 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[37] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:21 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:21 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 10) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 08:40:21 INFO Executor: Running task 0.0 in stage 12.0 (TID 10)\n",
            "25/08/06 08:40:21 INFO DAGScheduler: ResultStage 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.148 s\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Job 9 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.152264 s\n",
            "25/08/06 08:40:21 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products/part-00000-014e40be-2e5d-4608-8367-4ae8c0d15af6-c000.snappy.parquet, range: 0-4216, partition values: [empty row]\n",
            "25/08/06 08:40:21 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 1026.6 KiB, free 1732.2 MiB)\n",
            "25/08/06 08:40:21 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 1732.2 MiB)\n",
            "25/08/06 08:40:21 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 08:40:21 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on fa3f124442cb:41989 (size: 3.2 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:21 INFO SparkContext: Created broadcast 19 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:21 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 08:40:21 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 08:40:21 INFO Executor: Finished task 0.0 in stage 12.0 (TID 10). 2711 bytes result sent to driver\n",
            "25/08/06 08:40:21 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 10) in 78 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:21 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:21 INFO DAGScheduler: ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.103 s\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Job 10 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.108833 s\n",
            "25/08/06 08:40:21 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 32.0 MiB, free 1700.2 MiB)\n",
            "25/08/06 08:40:21 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 1853.0 B, free 1700.2 MiB)\n",
            "25/08/06 08:40:21 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on fa3f124442cb:41989 (size: 1853.0 B, free: 1767.4 MiB)\n",
            "25/08/06 08:40:21 INFO SparkContext: Created broadcast 20 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:21 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 08:40:21 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 08:40:21 INFO CodeGenerator: Code generated in 85.897018 ms\n",
            "25/08/06 08:40:21 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 200.5 KiB, free 1700.0 MiB)\n",
            "25/08/06 08:40:21 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1700.0 MiB)\n",
            "25/08/06 08:40:21 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on fa3f124442cb:41989 (size: 34.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:21 INFO SparkContext: Created broadcast 21 from show at SparkAnalysis.java:40\n",
            "25/08/06 08:40:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Registering RDD 41 (show at SparkAnalysis.java:40) as input to shuffle 2\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Got map stage job 11 (show at SparkAnalysis.java:40) with 1 output partitions\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (show at SparkAnalysis.java:40)\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[41] at show at SparkAnalysis.java:40), which has no missing parents\n",
            "25/08/06 08:40:21 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 55.5 KiB, free 1700.0 MiB)\n",
            "25/08/06 08:40:21 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 1699.9 MiB)\n",
            "25/08/06 08:40:21 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on fa3f124442cb:41989 (size: 23.9 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:21 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[41] at show at SparkAnalysis.java:40) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:21 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:21 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 11) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7978 bytes) \n",
            "25/08/06 08:40:21 INFO Executor: Running task 0.0 in stage 13.0 (TID 11)\n",
            "25/08/06 08:40:22 INFO CodeGenerator: Code generated in 22.894351 ms\n",
            "25/08/06 08:40:22 INFO CodeGenerator: Code generated in 16.498538 ms\n",
            "25/08/06 08:40:22 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-e416c70d-cd9e-43af-986e-3ba02e998a1e-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 08:40:22 INFO FilterCompat: Filtering using predicate: and(noteq(productCode, null), noteq(orderNumber, null))\n",
            "25/08/06 08:40:22 INFO BlockManagerInfo: Removed broadcast_14_piece0 on fa3f124442cb:41989 in memory (size: 23.5 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:22 INFO BlockManagerInfo: Removed broadcast_17_piece0 on fa3f124442cb:41989 in memory (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:22 INFO BlockManagerInfo: Removed broadcast_7_piece0 on fa3f124442cb:41989 in memory (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:22 INFO BlockManagerInfo: Removed broadcast_18_piece0 on fa3f124442cb:41989 in memory (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:22 INFO BlockManagerInfo: Removed broadcast_12_piece0 on fa3f124442cb:41989 in memory (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:22 INFO BlockManagerInfo: Removed broadcast_11_piece0 on fa3f124442cb:41989 in memory (size: 3.4 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:22 INFO BlockManagerInfo: Removed broadcast_10_piece0 on fa3f124442cb:41989 in memory (size: 4.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:22 INFO BlockManagerInfo: Removed broadcast_6_piece0 on fa3f124442cb:41989 in memory (size: 34.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:22 INFO Executor: Finished task 0.0 in stage 13.0 (TID 11). 4766 bytes result sent to driver\n",
            "25/08/06 08:40:22 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 11) in 845 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:22 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:22 INFO DAGScheduler: ShuffleMapStage 13 (show at SparkAnalysis.java:40) finished in 0.871 s\n",
            "25/08/06 08:40:22 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 08:40:22 INFO DAGScheduler: running: Set()\n",
            "25/08/06 08:40:22 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 08:40:22 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 08:40:22 INFO BlockManagerInfo: Removed broadcast_13_piece0 on fa3f124442cb:41989 in memory (size: 24.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:40:22 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 08:40:23 INFO CodeGenerator: Code generated in 127.860492 ms\n",
            "25/08/06 08:40:23 INFO DAGScheduler: Registering RDD 44 (show at SparkAnalysis.java:40) as input to shuffle 3\n",
            "25/08/06 08:40:23 INFO DAGScheduler: Got map stage job 12 (show at SparkAnalysis.java:40) with 1 output partitions\n",
            "25/08/06 08:40:23 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (show at SparkAnalysis.java:40)\n",
            "25/08/06 08:40:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\n",
            "25/08/06 08:40:23 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:23 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[44] at show at SparkAnalysis.java:40), which has no missing parents\n",
            "25/08/06 08:40:23 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 69.6 KiB, free 1733.8 MiB)\n",
            "25/08/06 08:40:23 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 1733.7 MiB)\n",
            "25/08/06 08:40:23 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on fa3f124442cb:41989 (size: 27.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:23 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[44] at show at SparkAnalysis.java:40) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:23 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:23 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (fa3f124442cb, executor driver, partition 0, NODE_LOCAL, 7352 bytes) \n",
            "25/08/06 08:40:23 INFO Executor: Running task 0.0 in stage 15.0 (TID 12)\n",
            "25/08/06 08:40:23 INFO ShuffleBlockFetcherIterator: Getting 1 (13.2 KiB) non-empty blocks including 1 (13.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:40:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 08:40:23 INFO CodeGenerator: Code generated in 40.279414 ms\n",
            "25/08/06 08:40:23 INFO CodeGenerator: Code generated in 16.378793 ms\n",
            "25/08/06 08:40:23 INFO CodeGenerator: Code generated in 31.892176 ms\n",
            "25/08/06 08:40:23 INFO CodeGenerator: Code generated in 17.221985 ms\n",
            "25/08/06 08:40:23 INFO Executor: Finished task 0.0 in stage 15.0 (TID 12). 7798 bytes result sent to driver\n",
            "25/08/06 08:40:23 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 313 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:23 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:23 INFO DAGScheduler: ShuffleMapStage 15 (show at SparkAnalysis.java:40) finished in 0.352 s\n",
            "25/08/06 08:40:23 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 08:40:23 INFO DAGScheduler: running: Set()\n",
            "25/08/06 08:40:23 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 08:40:23 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 08:40:23 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 08:40:23 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 08:40:23 INFO CodeGenerator: Code generated in 58.828026 ms\n",
            "25/08/06 08:40:23 INFO SparkContext: Starting job: show at SparkAnalysis.java:40\n",
            "25/08/06 08:40:23 INFO DAGScheduler: Got job 13 (show at SparkAnalysis.java:40) with 1 output partitions\n",
            "25/08/06 08:40:23 INFO DAGScheduler: Final stage: ResultStage 18 (show at SparkAnalysis.java:40)\n",
            "25/08/06 08:40:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\n",
            "25/08/06 08:40:23 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:23 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[47] at show at SparkAnalysis.java:40), which has no missing parents\n",
            "25/08/06 08:40:23 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 61.8 KiB, free 1733.7 MiB)\n",
            "25/08/06 08:40:23 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 26.4 KiB, free 1733.6 MiB)\n",
            "25/08/06 08:40:23 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on fa3f124442cb:41989 (size: 26.4 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:23 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[47] at show at SparkAnalysis.java:40) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:23 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:23 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 13) (fa3f124442cb, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 08:40:23 INFO Executor: Running task 0.0 in stage 18.0 (TID 13)\n",
            "25/08/06 08:40:23 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:40:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
            "25/08/06 08:40:24 INFO Executor: Finished task 0.0 in stage 18.0 (TID 13). 9225 bytes result sent to driver\n",
            "25/08/06 08:40:24 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 13) in 183 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:24 INFO DAGScheduler: ResultStage 18 (show at SparkAnalysis.java:40) finished in 0.216 s\n",
            "25/08/06 08:40:24 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:24 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
            "25/08/06 08:40:24 INFO DAGScheduler: Job 13 finished: show at SparkAnalysis.java:40, took 0.245951 s\n",
            "25/08/06 08:40:24 INFO BlockManagerInfo: Removed broadcast_22_piece0 on fa3f124442cb:41989 in memory (size: 23.9 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:24 INFO CodeGenerator: Code generated in 48.878551 ms\n",
            "+--------------+-----------------+----------+-----------------+\n",
            "|customerNumber|       totalSpent|orderCount|averageOrderValue|\n",
            "+--------------+-----------------+----------+-----------------+\n",
            "|          null|9604190.609999998|       326|29460.70739263803|\n",
            "+--------------+-----------------+----------+-----------------+\n",
            "\n",
            "25/08/06 08:40:24 INFO BlockManagerInfo: Removed broadcast_23_piece0 on fa3f124442cb:41989 in memory (size: 27.6 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:40:24 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 08:40:24 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 08:40:24 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 200.2 KiB, free 1733.6 MiB)\n",
            "25/08/06 08:40:24 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1733.6 MiB)\n",
            "25/08/06 08:40:24 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on fa3f124442cb:41989 (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:24 INFO SparkContext: Created broadcast 25 from parquet at SparkAnalysis.java:43\n",
            "25/08/06 08:40:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:40:24 INFO DAGScheduler: Registering RDD 51 (parquet at SparkAnalysis.java:43) as input to shuffle 4\n",
            "25/08/06 08:40:24 INFO DAGScheduler: Got map stage job 14 (parquet at SparkAnalysis.java:43) with 1 output partitions\n",
            "25/08/06 08:40:24 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (parquet at SparkAnalysis.java:43)\n",
            "25/08/06 08:40:24 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:40:24 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:24 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[51] at parquet at SparkAnalysis.java:43), which has no missing parents\n",
            "25/08/06 08:40:24 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 40.5 KiB, free 1733.5 MiB)\n",
            "25/08/06 08:40:24 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 18.2 KiB, free 1733.5 MiB)\n",
            "25/08/06 08:40:24 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on fa3f124442cb:41989 (size: 18.2 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:24 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:24 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[51] at parquet at SparkAnalysis.java:43) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:24 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:24 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 14) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7978 bytes) \n",
            "25/08/06 08:40:24 INFO Executor: Running task 0.0 in stage 19.0 (TID 14)\n",
            "25/08/06 08:40:24 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-e416c70d-cd9e-43af-986e-3ba02e998a1e-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 08:40:24 INFO Executor: Finished task 0.0 in stage 19.0 (TID 14). 2973 bytes result sent to driver\n",
            "25/08/06 08:40:24 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 14) in 146 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:24 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:24 INFO DAGScheduler: ShuffleMapStage 19 (parquet at SparkAnalysis.java:43) finished in 0.171 s\n",
            "25/08/06 08:40:24 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 08:40:24 INFO DAGScheduler: running: Set()\n",
            "25/08/06 08:40:24 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 08:40:24 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 08:40:24 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 08:40:24 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:40:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:40:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:40:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:40:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:40:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:40:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:40:24 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 08:40:24 INFO SparkContext: Starting job: parquet at SparkAnalysis.java:43\n",
            "25/08/06 08:40:24 INFO DAGScheduler: Got job 15 (parquet at SparkAnalysis.java:43) with 1 output partitions\n",
            "25/08/06 08:40:24 INFO DAGScheduler: Final stage: ResultStage 21 (parquet at SparkAnalysis.java:43)\n",
            "25/08/06 08:40:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)\n",
            "25/08/06 08:40:24 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:24 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[55] at parquet at SparkAnalysis.java:43), which has no missing parents\n",
            "25/08/06 08:40:24 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 242.4 KiB, free 1733.3 MiB)\n",
            "25/08/06 08:40:24 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 90.0 KiB, free 1733.2 MiB)\n",
            "25/08/06 08:40:24 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on fa3f124442cb:41989 (size: 90.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:24 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[55] at parquet at SparkAnalysis.java:43) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:24 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:24 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 15) (fa3f124442cb, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 08:40:24 INFO Executor: Running task 0.0 in stage 21.0 (TID 15)\n",
            "25/08/06 08:40:25 INFO ShuffleBlockFetcherIterator: Getting 1 (6.4 KiB) non-empty blocks including 1 (6.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:40:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "25/08/06 08:40:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:40:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:40:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:40:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:40:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:40:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:40:25 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:40:25 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:40:25 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:40:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"sum(quantityOrdered)\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productCode (STRING);\n",
            "  optional int64 sum(quantityOrdered);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:40:25 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 08:40:25 INFO FileOutputCommitter: Saved output of task 'attempt_202508060840244971894503809185128_0021_m_000000_15' to file:/content/data/results/top_products/_temporary/0/task_202508060840244971894503809185128_0021_m_000000\n",
            "25/08/06 08:40:25 INFO SparkHadoopMapRedUtil: attempt_202508060840244971894503809185128_0021_m_000000_15: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 08:40:25 INFO Executor: Finished task 0.0 in stage 21.0 (TID 15). 7465 bytes result sent to driver\n",
            "25/08/06 08:40:25 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 15) in 313 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:25 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:25 INFO DAGScheduler: ResultStage 21 (parquet at SparkAnalysis.java:43) finished in 0.396 s\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Job 15 finished: parquet at SparkAnalysis.java:43, took 0.414490 s\n",
            "25/08/06 08:40:25 INFO FileFormatWriter: Start to commit write Job 978223dd-a644-4fc1-ab6c-df5f324e56a5.\n",
            "25/08/06 08:40:25 INFO FileFormatWriter: Write Job 978223dd-a644-4fc1-ab6c-df5f324e56a5 committed. Elapsed time: 32 ms.\n",
            "25/08/06 08:40:25 INFO FileFormatWriter: Finished processing stats for write job 978223dd-a644-4fc1-ab6c-df5f324e56a5.\n",
            "25/08/06 08:40:25 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 08:40:25 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 08:40:25 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 08:40:25 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#16)\n",
            "25/08/06 08:40:25 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)\n",
            "25/08/06 08:40:25 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#10)\n",
            "25/08/06 08:40:25 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 200.1 KiB, free 1733.0 MiB)\n",
            "25/08/06 08:40:25 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 200.2 KiB, free 1732.8 MiB)\n",
            "25/08/06 08:40:25 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 1732.8 MiB)\n",
            "25/08/06 08:40:25 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on fa3f124442cb:41989 (size: 34.6 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:25 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1732.7 MiB)\n",
            "25/08/06 08:40:25 INFO SparkContext: Created broadcast 29 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:40:25 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on fa3f124442cb:41989 (size: 34.7 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:25 INFO SparkContext: Created broadcast 28 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:40:25 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Got job 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Final stage: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[61] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:40:25 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 13.6 KiB, free 1732.7 MiB)\n",
            "25/08/06 08:40:25 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1732.7 MiB)\n",
            "25/08/06 08:40:25 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on fa3f124442cb:41989 (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:25 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[61] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:25 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:25 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 16) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 08:40:25 INFO Executor: Running task 0.0 in stage 22.0 (TID 16)\n",
            "25/08/06 08:40:25 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products/part-00000-014e40be-2e5d-4608-8367-4ae8c0d15af6-c000.snappy.parquet, range: 0-4216, partition values: [empty row]\n",
            "25/08/06 08:40:25 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Got job 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Final stage: ResultStage 23 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[63] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:40:25 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 12.9 KiB, free 1732.7 MiB)\n",
            "25/08/06 08:40:25 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 08:40:25 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1732.7 MiB)\n",
            "25/08/06 08:40:25 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on fa3f124442cb:41989 (size: 6.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:25 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[63] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:25 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:25 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 17) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7983 bytes) \n",
            "25/08/06 08:40:25 INFO Executor: Running task 0.0 in stage 23.0 (TID 17)\n",
            "25/08/06 08:40:25 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orders/part-00000-ab60cd2c-3d7b-43fd-98bb-649efba8034c-c000.snappy.parquet, range: 0-3888, partition values: [empty row]\n",
            "25/08/06 08:40:25 INFO FilterCompat: Filtering using predicate: noteq(orderNumber, null)\n",
            "25/08/06 08:40:25 INFO Executor: Finished task 0.0 in stage 22.0 (TID 16). 5419 bytes result sent to driver\n",
            "25/08/06 08:40:25 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 16) in 65 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:25 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:25 INFO DAGScheduler: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.076 s\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Job 16 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.080787 s\n",
            "25/08/06 08:40:25 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 32.0 MiB, free 1700.7 MiB)\n",
            "25/08/06 08:40:25 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 1700.7 MiB)\n",
            "25/08/06 08:40:25 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on fa3f124442cb:41989 (size: 4.7 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:25 INFO SparkContext: Created broadcast 32 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:25 INFO Executor: Finished task 0.0 in stage 23.0 (TID 17). 3459 bytes result sent to driver\n",
            "25/08/06 08:40:25 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 17) in 61 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:25 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:25 INFO DAGScheduler: ResultStage 23 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.080 s\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Job 17 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.090158 s\n",
            "25/08/06 08:40:25 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 08:40:25 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 08:40:25 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 1026.6 KiB, free 1699.7 MiB)\n",
            "25/08/06 08:40:25 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 1699.7 MiB)\n",
            "25/08/06 08:40:25 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on fa3f124442cb:41989 (size: 3.4 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:25 INFO SparkContext: Created broadcast 33 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:25 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 08:40:25 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 08:40:25 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 200.5 KiB, free 1699.5 MiB)\n",
            "25/08/06 08:40:25 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1699.5 MiB)\n",
            "25/08/06 08:40:25 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on fa3f124442cb:41989 (size: 34.8 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:25 INFO SparkContext: Created broadcast 34 from parquet at SparkAnalysis.java:44\n",
            "25/08/06 08:40:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Registering RDD 67 (parquet at SparkAnalysis.java:44) as input to shuffle 5\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Got map stage job 18 (parquet at SparkAnalysis.java:44) with 1 output partitions\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Final stage: ShuffleMapStage 24 (parquet at SparkAnalysis.java:44)\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[67] at parquet at SparkAnalysis.java:44), which has no missing parents\n",
            "25/08/06 08:40:25 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 56.3 KiB, free 1699.4 MiB)\n",
            "25/08/06 08:40:25 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 1699.4 MiB)\n",
            "25/08/06 08:40:25 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on fa3f124442cb:41989 (size: 24.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:25 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:25 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[67] at parquet at SparkAnalysis.java:44) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:25 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:25 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 18) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7978 bytes) \n",
            "25/08/06 08:40:25 INFO Executor: Running task 0.0 in stage 24.0 (TID 18)\n",
            "25/08/06 08:40:25 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-e416c70d-cd9e-43af-986e-3ba02e998a1e-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 08:40:25 INFO FilterCompat: Filtering using predicate: and(noteq(productCode, null), noteq(orderNumber, null))\n",
            "25/08/06 08:40:26 INFO Executor: Finished task 0.0 in stage 24.0 (TID 18). 4723 bytes result sent to driver\n",
            "25/08/06 08:40:26 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 18) in 129 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:26 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:26 INFO DAGScheduler: ShuffleMapStage 24 (parquet at SparkAnalysis.java:44) finished in 0.145 s\n",
            "25/08/06 08:40:26 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 08:40:26 INFO DAGScheduler: running: Set()\n",
            "25/08/06 08:40:26 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 08:40:26 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 08:40:26 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 08:40:26 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 08:40:26 INFO CodeGenerator: Code generated in 15.32541 ms\n",
            "25/08/06 08:40:26 INFO SparkContext: Starting job: parquet at SparkAnalysis.java:44\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Got job 19 (parquet at SparkAnalysis.java:44) with 1 output partitions\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Final stage: ResultStage 26 (parquet at SparkAnalysis.java:44)\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[72] at parquet at SparkAnalysis.java:44), which has no missing parents\n",
            "25/08/06 08:40:26 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 52.8 KiB, free 1699.3 MiB)\n",
            "25/08/06 08:40:26 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 1699.3 MiB)\n",
            "25/08/06 08:40:26 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on fa3f124442cb:41989 (size: 23.5 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:26 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[72] at parquet at SparkAnalysis.java:44) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:26 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:26 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 19) (fa3f124442cb, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 08:40:26 INFO Executor: Running task 0.0 in stage 26.0 (TID 19)\n",
            "25/08/06 08:40:26 INFO ShuffleBlockFetcherIterator: Getting 1 (11.1 KiB) non-empty blocks including 1 (11.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:40:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 08:40:26 INFO CodeGenerator: Code generated in 10.197569 ms\n",
            "25/08/06 08:40:26 INFO Executor: Finished task 0.0 in stage 26.0 (TID 19). 10838 bytes result sent to driver\n",
            "25/08/06 08:40:26 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 19) in 75 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:26 INFO DAGScheduler: ResultStage 26 (parquet at SparkAnalysis.java:44) finished in 0.086 s\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:26 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Job 19 finished: parquet at SparkAnalysis.java:44, took 0.097315 s\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Registering RDD 73 (parquet at SparkAnalysis.java:44) as input to shuffle 6\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Got map stage job 20 (parquet at SparkAnalysis.java:44) with 1 output partitions\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Final stage: ShuffleMapStage 28 (parquet at SparkAnalysis.java:44)\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[73] at parquet at SparkAnalysis.java:44), which has no missing parents\n",
            "25/08/06 08:40:26 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 56.2 KiB, free 1699.3 MiB)\n",
            "25/08/06 08:40:26 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 24.8 KiB, free 1699.2 MiB)\n",
            "25/08/06 08:40:26 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on fa3f124442cb:41989 (size: 24.8 KiB, free: 1767.1 MiB)\n",
            "25/08/06 08:40:26 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[73] at parquet at SparkAnalysis.java:44) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:26 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:26 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 20) (fa3f124442cb, executor driver, partition 0, NODE_LOCAL, 7352 bytes) \n",
            "25/08/06 08:40:26 INFO Executor: Running task 0.0 in stage 28.0 (TID 20)\n",
            "25/08/06 08:40:26 INFO ShuffleBlockFetcherIterator: Getting 1 (11.1 KiB) non-empty blocks including 1 (11.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:40:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 08:40:26 INFO BlockManagerInfo: Removed broadcast_19_piece0 on fa3f124442cb:41989 in memory (size: 3.2 KiB, free: 1767.1 MiB)\n",
            "25/08/06 08:40:26 INFO Executor: Finished task 0.0 in stage 28.0 (TID 20). 7411 bytes result sent to driver\n",
            "25/08/06 08:40:26 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 20) in 130 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:26 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:26 INFO DAGScheduler: ShuffleMapStage 28 (parquet at SparkAnalysis.java:44) finished in 0.151 s\n",
            "25/08/06 08:40:26 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 08:40:26 INFO DAGScheduler: running: Set()\n",
            "25/08/06 08:40:26 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 08:40:26 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 08:40:26 INFO BlockManagerInfo: Removed broadcast_20_piece0 on fa3f124442cb:41989 in memory (size: 1853.0 B, free: 1767.1 MiB)\n",
            "25/08/06 08:40:26 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 08:40:26 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:40:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:40:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:40:26 INFO BlockManagerInfo: Removed broadcast_26_piece0 on fa3f124442cb:41989 in memory (size: 18.2 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:26 INFO BlockManagerInfo: Removed broadcast_15_piece0 on fa3f124442cb:41989 in memory (size: 34.7 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:26 INFO BlockManagerInfo: Removed broadcast_36_piece0 on fa3f124442cb:41989 in memory (size: 23.5 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:26 INFO CodeGenerator: Code generated in 14.413946 ms\n",
            "25/08/06 08:40:26 INFO BlockManagerInfo: Removed broadcast_35_piece0 on fa3f124442cb:41989 in memory (size: 24.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:26 INFO SparkContext: Starting job: parquet at SparkAnalysis.java:44\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Got job 21 (parquet at SparkAnalysis.java:44) with 1 output partitions\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Final stage: ResultStage 31 (parquet at SparkAnalysis.java:44)\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[76] at parquet at SparkAnalysis.java:44), which has no missing parents\n",
            "25/08/06 08:40:26 INFO BlockManagerInfo: Removed broadcast_21_piece0 on fa3f124442cb:41989 in memory (size: 34.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:26 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 248.8 KiB, free 1732.7 MiB)\n",
            "25/08/06 08:40:26 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 92.8 KiB, free 1732.6 MiB)\n",
            "25/08/06 08:40:26 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on fa3f124442cb:41989 (size: 92.8 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:26 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:26 INFO BlockManagerInfo: Removed broadcast_25_piece0 on fa3f124442cb:41989 in memory (size: 34.8 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[76] at parquet at SparkAnalysis.java:44) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:26 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:26 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 21) (fa3f124442cb, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 08:40:26 INFO Executor: Running task 0.0 in stage 31.0 (TID 21)\n",
            "25/08/06 08:40:26 INFO BlockManagerInfo: Removed broadcast_30_piece0 on fa3f124442cb:41989 in memory (size: 6.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:26 INFO BlockManagerInfo: Removed broadcast_31_piece0 on fa3f124442cb:41989 in memory (size: 6.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:26 INFO ShuffleBlockFetcherIterator: Getting 1 (12.5 KiB) non-empty blocks including 1 (12.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:40:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 08:40:26 INFO BlockManagerInfo: Removed broadcast_16_piece0 on fa3f124442cb:41989 in memory (size: 34.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:26 INFO CodeGenerator: Code generated in 23.890845 ms\n",
            "25/08/06 08:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:40:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:40:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:40:26 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:40:26 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:40:26 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:40:26 INFO BlockManagerInfo: Removed broadcast_24_piece0 on fa3f124442cb:41989 in memory (size: 26.4 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalRevenue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productCode (STRING);\n",
            "  optional binary productName (STRING);\n",
            "  optional double totalRevenue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:40:26 INFO BlockManagerInfo: Removed broadcast_27_piece0 on fa3f124442cb:41989 in memory (size: 90.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:26 INFO FileOutputCommitter: Saved output of task 'attempt_202508060840263301753607492964995_0031_m_000000_21' to file:/content/data/results/product_revenue/_temporary/0/task_202508060840263301753607492964995_0031_m_000000\n",
            "25/08/06 08:40:26 INFO SparkHadoopMapRedUtil: attempt_202508060840263301753607492964995_0031_m_000000_21: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 08:40:26 INFO Executor: Finished task 0.0 in stage 31.0 (TID 21). 9564 bytes result sent to driver\n",
            "25/08/06 08:40:26 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 21) in 256 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:26 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:26 INFO DAGScheduler: ResultStage 31 (parquet at SparkAnalysis.java:44) finished in 0.315 s\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished\n",
            "25/08/06 08:40:26 INFO DAGScheduler: Job 21 finished: parquet at SparkAnalysis.java:44, took 0.320469 s\n",
            "25/08/06 08:40:26 INFO FileFormatWriter: Start to commit write Job d3e92255-b275-42fe-8539-459db0dd1fba.\n",
            "25/08/06 08:40:26 INFO FileFormatWriter: Write Job d3e92255-b275-42fe-8539-459db0dd1fba committed. Elapsed time: 29 ms.\n",
            "25/08/06 08:40:26 INFO FileFormatWriter: Finished processing stats for write job d3e92255-b275-42fe-8539-459db0dd1fba.\n",
            "25/08/06 08:40:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 08:40:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 08:40:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 08:40:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#16)\n",
            "25/08/06 08:40:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)\n",
            "25/08/06 08:40:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#10)\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 200.1 KiB, free 1733.3 MiB)\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 200.2 KiB, free 1733.1 MiB)\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1733.1 MiB)\n",
            "25/08/06 08:40:27 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on fa3f124442cb:41989 (size: 34.7 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:27 INFO SparkContext: Created broadcast 39 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1733.0 MiB)\n",
            "25/08/06 08:40:27 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on fa3f124442cb:41989 (size: 34.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:27 INFO SparkContext: Created broadcast 40 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:40:27 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:27 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Got job 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Final stage: ResultStage 32 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[80] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 13.0 KiB, free 1733.0 MiB)\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1733.0 MiB)\n",
            "25/08/06 08:40:27 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on fa3f124442cb:41989 (size: 6.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:27 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[80] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:27 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Got job 23 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Final stage: ResultStage 33 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:27 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 22) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 08:40:27 INFO Executor: Running task 0.0 in stage 32.0 (TID 22)\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[84] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:40:27 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products/part-00000-014e40be-2e5d-4608-8367-4ae8c0d15af6-c000.snappy.parquet, range: 0-4216, partition values: [empty row]\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 13.6 KiB, free 1733.0 MiB)\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1733.0 MiB)\n",
            "25/08/06 08:40:27 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on fa3f124442cb:41989 (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:27 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:27 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[84] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:27 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:27 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 23) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7983 bytes) \n",
            "25/08/06 08:40:27 INFO Executor: Running task 0.0 in stage 33.0 (TID 23)\n",
            "25/08/06 08:40:27 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orders/part-00000-ab60cd2c-3d7b-43fd-98bb-649efba8034c-c000.snappy.parquet, range: 0-3888, partition values: [empty row]\n",
            "25/08/06 08:40:27 INFO FilterCompat: Filtering using predicate: noteq(orderNumber, null)\n",
            "25/08/06 08:40:27 INFO Executor: Finished task 0.0 in stage 32.0 (TID 22). 2711 bytes result sent to driver\n",
            "25/08/06 08:40:27 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 22) in 42 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:27 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:27 INFO DAGScheduler: ResultStage 32 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.051 s\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Job 22 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.054780 s\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 32.0 MiB, free 1701.0 MiB)\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 1853.0 B, free 1701.0 MiB)\n",
            "25/08/06 08:40:27 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on fa3f124442cb:41989 (size: 1853.0 B, free: 1767.3 MiB)\n",
            "25/08/06 08:40:27 INFO SparkContext: Created broadcast 43 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:27 INFO Executor: Finished task 0.0 in stage 33.0 (TID 23). 3460 bytes result sent to driver\n",
            "25/08/06 08:40:27 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 23) in 47 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:27 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:27 INFO DAGScheduler: ResultStage 33 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.058 s\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Job 23 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.077643 s\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 1026.6 KiB, free 1700.0 MiB)\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 1700.0 MiB)\n",
            "25/08/06 08:40:27 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on fa3f124442cb:41989 (size: 3.2 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:27 INFO SparkContext: Created broadcast 44 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:40:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 08:40:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 08:40:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 08:40:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 200.5 KiB, free 1699.8 MiB)\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1699.7 MiB)\n",
            "25/08/06 08:40:27 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on fa3f124442cb:41989 (size: 34.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:27 INFO SparkContext: Created broadcast 45 from parquet at SparkAnalysis.java:45\n",
            "25/08/06 08:40:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Registering RDD 88 (parquet at SparkAnalysis.java:45) as input to shuffle 7\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Got map stage job 24 (parquet at SparkAnalysis.java:45) with 1 output partitions\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Final stage: ShuffleMapStage 34 (parquet at SparkAnalysis.java:45)\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Submitting ShuffleMapStage 34 (MapPartitionsRDD[88] at parquet at SparkAnalysis.java:45), which has no missing parents\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 55.7 KiB, free 1699.7 MiB)\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 1699.7 MiB)\n",
            "25/08/06 08:40:27 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on fa3f124442cb:41989 (size: 24.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:27 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[88] at parquet at SparkAnalysis.java:45) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:27 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:27 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 24) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7978 bytes) \n",
            "25/08/06 08:40:27 INFO Executor: Running task 0.0 in stage 34.0 (TID 24)\n",
            "25/08/06 08:40:27 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-e416c70d-cd9e-43af-986e-3ba02e998a1e-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 08:40:27 INFO FilterCompat: Filtering using predicate: and(noteq(productCode, null), noteq(orderNumber, null))\n",
            "25/08/06 08:40:27 INFO Executor: Finished task 0.0 in stage 34.0 (TID 24). 4766 bytes result sent to driver\n",
            "25/08/06 08:40:27 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 24) in 126 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:27 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:27 INFO DAGScheduler: ShuffleMapStage 34 (parquet at SparkAnalysis.java:45) finished in 0.144 s\n",
            "25/08/06 08:40:27 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 08:40:27 INFO DAGScheduler: running: Set()\n",
            "25/08/06 08:40:27 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 08:40:27 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 08:40:27 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Registering RDD 91 (parquet at SparkAnalysis.java:45) as input to shuffle 8\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Got map stage job 25 (parquet at SparkAnalysis.java:45) with 1 output partitions\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Final stage: ShuffleMapStage 36 (parquet at SparkAnalysis.java:45)\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 35)\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[91] at parquet at SparkAnalysis.java:45), which has no missing parents\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 69.6 KiB, free 1699.6 MiB)\n",
            "25/08/06 08:40:27 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 1699.6 MiB)\n",
            "25/08/06 08:40:27 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on fa3f124442cb:41989 (size: 27.5 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:27 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[91] at parquet at SparkAnalysis.java:45) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:27 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:27 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 25) (fa3f124442cb, executor driver, partition 0, NODE_LOCAL, 7352 bytes) \n",
            "25/08/06 08:40:27 INFO Executor: Running task 0.0 in stage 36.0 (TID 25)\n",
            "25/08/06 08:40:27 INFO ShuffleBlockFetcherIterator: Getting 1 (13.2 KiB) non-empty blocks including 1 (13.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:40:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\n",
            "25/08/06 08:40:27 INFO BlockManagerInfo: Removed broadcast_46_piece0 on fa3f124442cb:41989 in memory (size: 24.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:27 INFO BlockManagerInfo: Removed broadcast_42_piece0 on fa3f124442cb:41989 in memory (size: 6.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:40:27 INFO Executor: Finished task 0.0 in stage 36.0 (TID 25). 7841 bytes result sent to driver\n",
            "25/08/06 08:40:27 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 25) in 115 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:27 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:27 INFO DAGScheduler: ShuffleMapStage 36 (parquet at SparkAnalysis.java:45) finished in 0.152 s\n",
            "25/08/06 08:40:27 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 08:40:27 INFO DAGScheduler: running: Set()\n",
            "25/08/06 08:40:27 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 08:40:27 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 08:40:27 INFO BlockManagerInfo: Removed broadcast_34_piece0 on fa3f124442cb:41989 in memory (size: 34.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:40:27 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 08:40:27 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:40:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:40:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:40:27 INFO BlockManagerInfo: Removed broadcast_38_piece0 on fa3f124442cb:41989 in memory (size: 92.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:40:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:40:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:40:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:40:27 INFO BlockManagerInfo: Removed broadcast_32_piece0 on fa3f124442cb:41989 in memory (size: 4.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:27 INFO BlockManagerInfo: Removed broadcast_37_piece0 on fa3f124442cb:41989 in memory (size: 24.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:27 INFO BlockManagerInfo: Removed broadcast_33_piece0 on fa3f124442cb:41989 in memory (size: 3.4 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:27 INFO BlockManagerInfo: Removed broadcast_28_piece0 on fa3f124442cb:41989 in memory (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:28 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 08:40:28 INFO BlockManagerInfo: Removed broadcast_29_piece0 on fa3f124442cb:41989 in memory (size: 34.6 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:40:28 INFO BlockManagerInfo: Removed broadcast_41_piece0 on fa3f124442cb:41989 in memory (size: 6.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:40:28 INFO CodeGenerator: Code generated in 95.382756 ms\n",
            "25/08/06 08:40:28 INFO SparkContext: Starting job: parquet at SparkAnalysis.java:45\n",
            "25/08/06 08:40:28 INFO DAGScheduler: Got job 26 (parquet at SparkAnalysis.java:45) with 1 output partitions\n",
            "25/08/06 08:40:28 INFO DAGScheduler: Final stage: ResultStage 39 (parquet at SparkAnalysis.java:45)\n",
            "25/08/06 08:40:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)\n",
            "25/08/06 08:40:28 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:40:28 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[94] at parquet at SparkAnalysis.java:45), which has no missing parents\n",
            "25/08/06 08:40:28 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 260.0 KiB, free 1733.6 MiB)\n",
            "25/08/06 08:40:28 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 96.4 KiB, free 1733.5 MiB)\n",
            "25/08/06 08:40:28 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on fa3f124442cb:41989 (size: 96.4 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:40:28 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:40:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[94] at parquet at SparkAnalysis.java:45) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:40:28 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:40:28 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 26) (fa3f124442cb, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 08:40:28 INFO Executor: Running task 0.0 in stage 39.0 (TID 26)\n",
            "25/08/06 08:40:28 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:40:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/08/06 08:40:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:40:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:40:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:40:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:40:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:40:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:40:28 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:40:28 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:40:28 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:40:28 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalSpent\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderCount\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : false,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"averageOrderValue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 customerNumber;\n",
            "  optional double totalSpent;\n",
            "  required int64 orderCount;\n",
            "  optional double averageOrderValue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:40:28 INFO FileOutputCommitter: Saved output of task 'attempt_202508060840289114397750092220892_0039_m_000000_26' to file:/content/data/results/customer_aov/_temporary/0/task_202508060840289114397750092220892_0039_m_000000\n",
            "25/08/06 08:40:28 INFO SparkHadoopMapRedUtil: attempt_202508060840289114397750092220892_0039_m_000000_26: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 08:40:28 INFO Executor: Finished task 0.0 in stage 39.0 (TID 26). 10055 bytes result sent to driver\n",
            "25/08/06 08:40:28 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 26) in 115 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:40:28 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:40:28 INFO DAGScheduler: ResultStage 39 (parquet at SparkAnalysis.java:45) finished in 0.178 s\n",
            "25/08/06 08:40:28 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:40:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished\n",
            "25/08/06 08:40:28 INFO DAGScheduler: Job 26 finished: parquet at SparkAnalysis.java:45, took 0.188165 s\n",
            "25/08/06 08:40:28 INFO FileFormatWriter: Start to commit write Job afce7b2e-0bc1-4ae5-8bb5-d43a75f8d1c7.\n",
            "25/08/06 08:40:28 INFO FileFormatWriter: Write Job afce7b2e-0bc1-4ae5-8bb5-d43a75f8d1c7 committed. Elapsed time: 13 ms.\n",
            "25/08/06 08:40:28 INFO FileFormatWriter: Finished processing stats for write job afce7b2e-0bc1-4ae5-8bb5-d43a75f8d1c7.\n",
            "25/08/06 08:40:28 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 08:40:28 INFO SparkUI: Stopped Spark web UI at http://fa3f124442cb:4040\n",
            "25/08/06 08:40:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 08:40:28 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 08:40:28 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 08:40:28 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 08:40:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 08:40:28 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 08:40:28 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 08:40:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-c6c32e93-7ed6-4f0a-9fd3-c41fa860bc4c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 3: Regional Sales Insights"
      ],
      "metadata": {
        "id": "Q2vgfmW6_sGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Task3.java\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "\n",
        "public class Task3 {\n",
        "    public static void main(String[] args) {\n",
        "        // Create Spark session\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Task 3 - Regional Sales Insights\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        // Read Parquet files\n",
        "        Dataset<Row> offices = spark.read().parquet(\"data/parquet/offices\");\n",
        "        Dataset<Row> employees = spark.read().parquet(\"data/parquet/employees\");\n",
        "        Dataset<Row> customers = spark.read().parquet(\"data/parquet/customers\");\n",
        "        Dataset<Row> payments = spark.read().parquet(\"data/parquet/payments\");\n",
        "\n",
        "        // Register Temp Views\n",
        "        offices.createOrReplaceTempView(\"offices\");\n",
        "        employees.createOrReplaceTempView(\"employees\");\n",
        "        customers.createOrReplaceTempView(\"customers\");\n",
        "        payments.createOrReplaceTempView(\"payments\");\n",
        "\n",
        "        // -------------------------\n",
        "        // Task 3.1: Sales per region\n",
        "        // -------------------------\n",
        "        Dataset<Row> salesPerRegion = spark.sql(\n",
        "            \"SELECT o.country, o.city, \" +\n",
        "            \"       COUNT(DISTINCT e.employeeNumber) AS totalEmployees, \" +\n",
        "            \"       COUNT(DISTINCT c.customerNumber) AS totalCustomers \" +\n",
        "            \"FROM offices o \" +\n",
        "            \"JOIN employees e ON o.officeCode = e.officeCode \" +\n",
        "            \"JOIN customers c ON e.employeeNumber = c.salesRepEmployeeNumber \" +\n",
        "            \"GROUP BY o.country, o.city \" +\n",
        "            \"ORDER BY o.country, o.city\"\n",
        "        );\n",
        "\n",
        "        salesPerRegion.show();\n",
        "        salesPerRegion.write().mode(\"overwrite\").parquet(\"data/output/task3/sales_per_region\");\n",
        "\n",
        "        // -------------------------------\n",
        "        // Task 3.2: Total revenue by country\n",
        "        // -------------------------------\n",
        "        Dataset<Row> revenueByCountry = spark.sql(\n",
        "            \"SELECT c.country, \" +\n",
        "            \"       SUM(p.amount) AS totalRevenue \" +\n",
        "            \"FROM customers c \" +\n",
        "            \"JOIN payments p ON c.customerNumber = p.customerNumber \" +\n",
        "            \"GROUP BY c.country \" +\n",
        "            \"ORDER BY totalRevenue DESC\"\n",
        "        );\n",
        "\n",
        "        revenueByCountry.show();\n",
        "        revenueByCountry.write().mode(\"overwrite\").parquet(\"data/output/task3/revenue_by_country\");\n",
        "\n",
        "        // -------------------------------------\n",
        "        // Task 3.3: Top-performing offices by revenue\n",
        "        // -------------------------------------\n",
        "        Dataset<Row> topOffices = spark.sql(\n",
        "            \"SELECT o.officeCode, o.city, o.country, \" +\n",
        "            \"       SUM(p.amount) AS officeRevenue \" +\n",
        "            \"FROM offices o \" +\n",
        "            \"JOIN employees e ON o.officeCode = e.officeCode \" +\n",
        "            \"JOIN customers c ON e.employeeNumber = c.salesRepEmployeeNumber \" +\n",
        "            \"JOIN payments p ON c.customerNumber = p.customerNumber \" +\n",
        "            \"GROUP BY o.officeCode, o.city, o.country \" +\n",
        "            \"ORDER BY officeRevenue DESC\"\n",
        "        );\n",
        "\n",
        "        topOffices.show();\n",
        "        topOffices.write().mode(\"overwrite\").parquet(\"data/output/task3/top_offices\");\n",
        "\n",
        "        // Stop Spark session\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A03667hCIDbi",
        "outputId": "0586de20-207d-4676-b29f-1b5cf0817cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Task3.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!javac -cp \"$SPARK_HOME/jars/*\" Task3.java\n"
      ],
      "metadata": {
        "id": "6g9H-A7kF1Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$SPARK_HOME/jars/*\" Task3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qiD3FAOIQAo",
        "outputId": "a2abd2b6-94d4-4e4d-92c3-ca15e085b3f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 08:41:07 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 08:41:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 08:41:08 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:41:08 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 08:41:08 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:41:08 INFO SparkContext: Submitted application: Task 3 - Regional Sales Insights\n",
            "25/08/06 08:41:08 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 08:41:08 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 08:41:08 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 08:41:08 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 08:41:08 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 08:41:08 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 08:41:08 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 08:41:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 08:41:08 INFO Utils: Successfully started service 'sparkDriver' on port 44031.\n",
            "25/08/06 08:41:09 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 08:41:09 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 08:41:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 08:41:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 08:41:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 08:41:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-48a78a59-036f-45ce-9f42-8c5180b27f91\n",
            "25/08/06 08:41:09 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 08:41:09 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 08:41:09 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 08:41:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 08:41:09 INFO Executor: Starting executor ID driver on host fa3f124442cb\n",
            "25/08/06 08:41:09 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 08:41:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33719.\n",
            "25/08/06 08:41:09 INFO NettyBlockTransferService: Server created on fa3f124442cb:33719\n",
            "25/08/06 08:41:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 08:41:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fa3f124442cb, 33719, None)\n",
            "25/08/06 08:41:09 INFO BlockManagerMasterEndpoint: Registering block manager fa3f124442cb:33719 with 1767.6 MiB RAM, BlockManagerId(driver, fa3f124442cb, 33719, None)\n",
            "25/08/06 08:41:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fa3f124442cb, 33719, None)\n",
            "25/08/06 08:41:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fa3f124442cb, 33719, None)\n",
            "25/08/06 08:41:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 08:41:10 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 08:41:12 INFO InMemoryFileIndex: It took 105 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:41:13 INFO SparkContext: Starting job: parquet at Task3.java:14\n",
            "25/08/06 08:41:13 INFO DAGScheduler: Got job 0 (parquet at Task3.java:14) with 1 output partitions\n",
            "25/08/06 08:41:13 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at Task3.java:14)\n",
            "25/08/06 08:41:13 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:13 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at Task3.java:14), which has no missing parents\n",
            "25/08/06 08:41:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:41:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:41:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fa3f124442cb:33719 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:41:13 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at Task3.java:14) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7560 bytes) \n",
            "25/08/06 08:41:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 08:41:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1851 bytes result sent to driver\n",
            "25/08/06 08:41:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 818 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:14 INFO DAGScheduler: ResultStage 0 (parquet at Task3.java:14) finished in 1.146 s\n",
            "25/08/06 08:41:14 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 08:41:14 INFO DAGScheduler: Job 0 finished: parquet at Task3.java:14, took 1.238034 s\n",
            "25/08/06 08:41:14 INFO BlockManagerInfo: Removed broadcast_0_piece0 on fa3f124442cb:33719 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:41:18 INFO InMemoryFileIndex: It took 12 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:41:19 INFO SparkContext: Starting job: parquet at Task3.java:15\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Got job 1 (parquet at Task3.java:15) with 1 output partitions\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at Task3.java:15)\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at Task3.java:15), which has no missing parents\n",
            "25/08/06 08:41:19 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:41:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:41:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fa3f124442cb:33719 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:41:19 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at Task3.java:15) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:19 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:19 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7562 bytes) \n",
            "25/08/06 08:41:19 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 08:41:19 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1915 bytes result sent to driver\n",
            "25/08/06 08:41:19 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 79 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:19 INFO DAGScheduler: ResultStage 1 (parquet at Task3.java:15) finished in 0.147 s\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Job 1 finished: parquet at Task3.java:15, took 0.157897 s\n",
            "25/08/06 08:41:19 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:41:19 INFO SparkContext: Starting job: parquet at Task3.java:16\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Got job 2 (parquet at Task3.java:16) with 1 output partitions\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at Task3.java:16)\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at Task3.java:16), which has no missing parents\n",
            "25/08/06 08:41:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 102.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 08:41:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.3 MiB)\n",
            "25/08/06 08:41:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fa3f124442cb:33719 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:41:19 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at Task3.java:16) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:19 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:19 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7562 bytes) \n",
            "25/08/06 08:41:19 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 08:41:19 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2391 bytes result sent to driver\n",
            "25/08/06 08:41:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 61 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:19 INFO DAGScheduler: ResultStage 2 (parquet at Task3.java:16) finished in 0.116 s\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Job 2 finished: parquet at Task3.java:16, took 0.124623 s\n",
            "25/08/06 08:41:19 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:41:19 INFO BlockManagerInfo: Removed broadcast_2_piece0 on fa3f124442cb:33719 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:41:19 INFO SparkContext: Starting job: parquet at Task3.java:17\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Got job 3 (parquet at Task3.java:17) with 1 output partitions\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at Task3.java:17)\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at parquet at Task3.java:17), which has no missing parents\n",
            "25/08/06 08:41:19 INFO BlockManagerInfo: Removed broadcast_1_piece0 on fa3f124442cb:33719 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:41:19 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:41:19 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:41:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fa3f124442cb:33719 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:41:19 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at parquet at Task3.java:17) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:19 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:19 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7561 bytes) \n",
            "25/08/06 08:41:19 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 08:41:19 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2054 bytes result sent to driver\n",
            "25/08/06 08:41:19 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 57 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:19 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:19 INFO DAGScheduler: ResultStage 3 (parquet at Task3.java:17) finished in 0.103 s\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 08:41:19 INFO DAGScheduler: Job 3 finished: parquet at Task3.java:17, took 0.120161 s\n",
            "25/08/06 08:41:19 INFO BlockManagerInfo: Removed broadcast_3_piece0 on fa3f124442cb:33719 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:41:21 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 08:41:21 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 08:41:21 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 08:41:21 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#8),isnotnull(employeeNumber#6)\n",
            "25/08/06 08:41:21 INFO FileSourceStrategy: Pushed Filters: IsNotNull(salesRepEmployeeNumber)\n",
            "25/08/06 08:41:21 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(salesRepEmployeeNumber#23)\n",
            "25/08/06 08:41:22 INFO CodeGenerator: Code generated in 446.430443 ms\n",
            "25/08/06 08:41:22 INFO CodeGenerator: Code generated in 446.318892 ms\n",
            "25/08/06 08:41:22 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.2 KiB, free 1767.4 MiB)\n",
            "25/08/06 08:41:22 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 200.2 KiB, free 1767.2 MiB)\n",
            "25/08/06 08:41:22 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1767.2 MiB)\n",
            "25/08/06 08:41:22 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1767.1 MiB)\n",
            "25/08/06 08:41:22 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on fa3f124442cb:33719 (size: 34.7 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:41:22 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on fa3f124442cb:33719 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:41:22 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:41:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:41:23 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:23 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:41:23 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:41:23 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:23 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:23 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:41:23 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 13.6 KiB, free 1767.1 MiB)\n",
            "25/08/06 08:41:23 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 08:41:23 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on fa3f124442cb:33719 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:41:23 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:23 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:23 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:23 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 08:41:23 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 08:41:23 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:41:23 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:41:23 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:23 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:23 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[15] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:41:23 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.6 KiB, free 1767.1 MiB)\n",
            "25/08/06 08:41:23 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 08:41:23 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on fa3f124442cb:33719 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:41:23 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[15] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:23 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:23 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 08:41:23 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 08:41:23 INFO FileScanRDD: Reading File path: file:///content/data/parquet/employees/part-00000-dcd7ad0b-72a1-44fd-a1a1-f1d3add83c01-c000.snappy.parquet, range: 0-1518, partition values: [empty row]\n",
            "25/08/06 08:41:23 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-23671a68-7d7e-43a5-987e-fb6eb20921c3-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 08:41:23 INFO FilterCompat: Filtering using predicate: noteq(salesRepEmployeeNumber, null)\n",
            "25/08/06 08:41:23 INFO FilterCompat: Filtering using predicate: and(noteq(officeCode, null), noteq(employeeNumber, null))\n",
            "25/08/06 08:41:23 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1795 bytes result sent to driver\n",
            "25/08/06 08:41:23 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 485 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:23 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:23 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.500 s\n",
            "25/08/06 08:41:23 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 08:41:23 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.514768 s\n",
            "25/08/06 08:41:23 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.0 B, free 1767.1 MiB)\n",
            "25/08/06 08:41:23 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 120.0 B, free 1767.1 MiB)\n",
            "25/08/06 08:41:23 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on fa3f124442cb:33719 (size: 120.0 B, free: 1767.5 MiB)\n",
            "25/08/06 08:41:23 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "+-------+----+--------------+--------------+\n",
            "|country|city|totalEmployees|totalCustomers|\n",
            "+-------+----+--------------+--------------+\n",
            "+-------+----+--------------+--------------+\n",
            "\n",
            "25/08/06 08:41:23 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 08:41:24 INFO BlockManagerInfo: Removed broadcast_6_piece0 on fa3f124442cb:33719 in memory (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:41:24 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 08:41:24 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 08:41:24 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 08:41:24 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#8),isnotnull(employeeNumber#6)\n",
            "25/08/06 08:41:24 INFO FileSourceStrategy: Pushed Filters: IsNotNull(salesRepEmployeeNumber)\n",
            "25/08/06 08:41:24 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(salesRepEmployeeNumber#23)\n",
            "25/08/06 08:41:24 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 200.2 KiB, free 1766.7 MiB)\n",
            "25/08/06 08:41:24 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.2 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:41:24 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1766.7 MiB)\n",
            "25/08/06 08:41:24 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on fa3f124442cb:33719 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:41:24 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1766.7 MiB)\n",
            "25/08/06 08:41:24 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on fa3f124442cb:33719 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:41:24 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:41:24 INFO SparkContext: Created broadcast 9 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:41:24 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[23] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:41:24 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:24 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 13.6 KiB, free 1766.6 MiB)\n",
            "25/08/06 08:41:24 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.6 MiB)\n",
            "25/08/06 08:41:24 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on fa3f124442cb:33719 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:41:24 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[23] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:24 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:24 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 08:41:24 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Got job 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:24 INFO FileScanRDD: Reading File path: file:///content/data/parquet/employees/part-00000-dcd7ad0b-72a1-44fd-a1a1-f1d3add83c01-c000.snappy.parquet, range: 0-1518, partition values: [empty row]\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:41:24 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 13.6 KiB, free 1766.6 MiB)\n",
            "25/08/06 08:41:24 INFO FilterCompat: Filtering using predicate: and(noteq(officeCode, null), noteq(employeeNumber, null))\n",
            "25/08/06 08:41:24 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.6 MiB)\n",
            "25/08/06 08:41:24 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on fa3f124442cb:33719 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:24 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:24 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2243 bytes result sent to driver\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:24 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:24 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1368 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:24 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:24 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 08:41:24 INFO DAGScheduler: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 1.409 s\n",
            "25/08/06 08:41:24 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.438390 s\n",
            "25/08/06 08:41:24 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2200 bytes result sent to driver\n",
            "25/08/06 08:41:24 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 100 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:24 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:24 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-23671a68-7d7e-43a5-987e-fb6eb20921c3-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 08:41:24 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.118 s\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.126213 s\n",
            "25/08/06 08:41:24 INFO FilterCompat: Filtering using predicate: noteq(salesRepEmployeeNumber, null)\n",
            "25/08/06 08:41:24 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1795 bytes result sent to driver\n",
            "25/08/06 08:41:24 INFO CodeGenerator: Code generated in 36.663843 ms\n",
            "25/08/06 08:41:24 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 96 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:24 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:24 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.132 s\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Job 7 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.192434 s\n",
            "25/08/06 08:41:24 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 24.0 B, free 1766.6 MiB)\n",
            "25/08/06 08:41:24 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 32.0 MiB, free 1734.6 MiB)\n",
            "25/08/06 08:41:24 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 32.0 MiB, free 1702.6 MiB)\n",
            "25/08/06 08:41:24 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 120.0 B, free 1702.6 MiB)\n",
            "25/08/06 08:41:24 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 864.0 B, free 1702.6 MiB)\n",
            "25/08/06 08:41:24 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 864.0 B, free 1702.6 MiB)\n",
            "25/08/06 08:41:24 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on fa3f124442cb:33719 (size: 120.0 B, free: 1767.4 MiB)\n",
            "25/08/06 08:41:24 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on fa3f124442cb:33719 (size: 864.0 B, free: 1767.4 MiB)\n",
            "25/08/06 08:41:24 INFO SparkContext: Created broadcast 13 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:24 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on fa3f124442cb:33719 (size: 864.0 B, free: 1767.4 MiB)\n",
            "25/08/06 08:41:24 INFO SparkContext: Created broadcast 15 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:24 INFO SparkContext: Created broadcast 14 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:24 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:41:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:41:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:41:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:41:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:41:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:41:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:41:24 INFO SparkContext: Starting job: parquet at Task3.java:40\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Got job 8 (parquet at Task3.java:40) with 1 output partitions\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Final stage: ResultStage 8 (parquet at Task3.java:40)\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:24 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[27] at parquet at Task3.java:40), which has no missing parents\n",
            "25/08/06 08:41:25 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 205.1 KiB, free 1702.4 MiB)\n",
            "25/08/06 08:41:25 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 73.1 KiB, free 1702.3 MiB)\n",
            "25/08/06 08:41:25 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on fa3f124442cb:33719 (size: 73.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:25 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[27] at parquet at Task3.java:40) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:25 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:25 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7384 bytes) \n",
            "25/08/06 08:41:25 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
            "25/08/06 08:41:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:41:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:41:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:41:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:41:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:41:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:41:25 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:41:25 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:41:25 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:41:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalEmployees\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : false,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalCustomers\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : false,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary country (STRING);\n",
            "  optional binary city (STRING);\n",
            "  required int64 totalEmployees;\n",
            "  required int64 totalCustomers;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:41:25 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 08:41:25 INFO FileOutputCommitter: Saved output of task 'attempt_202508060841246524705171880644690_0008_m_000000_8' to file:/content/data/output/task3/sales_per_region/_temporary/0/task_202508060841246524705171880644690_0008_m_000000\n",
            "25/08/06 08:41:25 INFO SparkHadoopMapRedUtil: attempt_202508060841246524705171880644690_0008_m_000000_8: Committed. Elapsed time: 11 ms.\n",
            "25/08/06 08:41:25 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2291 bytes result sent to driver\n",
            "25/08/06 08:41:25 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 284 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:25 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:25 INFO DAGScheduler: ResultStage 8 (parquet at Task3.java:40) finished in 0.343 s\n",
            "25/08/06 08:41:25 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
            "25/08/06 08:41:25 INFO DAGScheduler: Job 8 finished: parquet at Task3.java:40, took 0.351465 s\n",
            "25/08/06 08:41:25 INFO FileFormatWriter: Start to commit write Job 9fe221ae-55aa-4ae7-b488-3094f13e4a67.\n",
            "25/08/06 08:41:25 INFO FileFormatWriter: Write Job 9fe221ae-55aa-4ae7-b488-3094f13e4a67 committed. Elapsed time: 32 ms.\n",
            "25/08/06 08:41:25 INFO FileFormatWriter: Finished processing stats for write job 9fe221ae-55aa-4ae7-b488-3094f13e4a67.\n",
            "25/08/06 08:41:25 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 08:41:25 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#12)\n",
            "25/08/06 08:41:25 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 08:41:25 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#38)\n",
            "25/08/06 08:41:25 INFO CodeGenerator: Code generated in 37.627424 ms\n",
            "25/08/06 08:41:25 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 200.2 KiB, free 1702.2 MiB)\n",
            "25/08/06 08:41:25 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1702.1 MiB)\n",
            "25/08/06 08:41:25 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on fa3f124442cb:33719 (size: 34.7 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:41:25 INFO SparkContext: Created broadcast 17 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:41:25 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:25 INFO DAGScheduler: Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:41:25 INFO DAGScheduler: Final stage: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:41:25 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:25 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:25 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:41:25 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 13.6 KiB, free 1702.1 MiB)\n",
            "25/08/06 08:41:25 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1702.1 MiB)\n",
            "25/08/06 08:41:25 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on fa3f124442cb:33719 (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:41:25 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:25 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:25 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 08:41:25 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
            "25/08/06 08:41:25 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-23671a68-7d7e-43a5-987e-fb6eb20921c3-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 08:41:25 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 08:41:25 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 3277 bytes result sent to driver\n",
            "25/08/06 08:41:25 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 54 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:25 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:25 INFO DAGScheduler: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.066 s\n",
            "25/08/06 08:41:25 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "25/08/06 08:41:25 INFO DAGScheduler: Job 9 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.071428 s\n",
            "25/08/06 08:41:25 INFO CodeGenerator: Code generated in 29.748577 ms\n",
            "25/08/06 08:41:25 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 1027.1 KiB, free 1701.1 MiB)\n",
            "25/08/06 08:41:25 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 1701.1 MiB)\n",
            "25/08/06 08:41:25 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on fa3f124442cb:33719 (size: 2.5 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:41:25 INFO SparkContext: Created broadcast 19 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:25 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 08:41:25 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#38)\n",
            "25/08/06 08:41:26 INFO CodeGenerator: Code generated in 207.543645 ms\n",
            "25/08/06 08:41:26 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 200.2 KiB, free 1700.9 MiB)\n",
            "25/08/06 08:41:26 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1700.9 MiB)\n",
            "25/08/06 08:41:26 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on fa3f124442cb:33719 (size: 34.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:41:26 INFO SparkContext: Created broadcast 20 from show at Task3.java:54\n",
            "25/08/06 08:41:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:41:26 INFO DAGScheduler: Registering RDD 35 (show at Task3.java:54) as input to shuffle 0\n",
            "25/08/06 08:41:26 INFO DAGScheduler: Got map stage job 10 (show at Task3.java:54) with 1 output partitions\n",
            "25/08/06 08:41:26 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (show at Task3.java:54)\n",
            "25/08/06 08:41:26 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:26 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:26 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[35] at show at Task3.java:54), which has no missing parents\n",
            "25/08/06 08:41:26 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 48.0 KiB, free 1700.8 MiB)\n",
            "25/08/06 08:41:26 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 21.5 KiB, free 1700.8 MiB)\n",
            "25/08/06 08:41:26 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on fa3f124442cb:33719 (size: 21.5 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:41:26 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[35] at show at Task3.java:54) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:26 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:26 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7974 bytes) \n",
            "25/08/06 08:41:26 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
            "25/08/06 08:41:26 INFO CodeGenerator: Code generated in 28.432197 ms\n",
            "25/08/06 08:41:26 INFO CodeGenerator: Code generated in 20.292799 ms\n",
            "25/08/06 08:41:26 INFO BlockManagerInfo: Removed broadcast_16_piece0 on fa3f124442cb:33719 in memory (size: 73.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:41:26 INFO CodeGenerator: Code generated in 32.838845 ms\n",
            "25/08/06 08:41:26 INFO BlockManagerInfo: Removed broadcast_11_piece0 on fa3f124442cb:33719 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:26 INFO CodeGenerator: Code generated in 27.138213 ms\n",
            "25/08/06 08:41:26 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments/part-00000-d589cdd0-28a9-4494-a3a5-0147b264c1d8-c000.snappy.parquet, range: 0-7229, partition values: [empty row]\n",
            "25/08/06 08:41:26 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 08:41:26 INFO BlockManagerInfo: Removed broadcast_4_piece0 on fa3f124442cb:33719 in memory (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:26 INFO BlockManagerInfo: Removed broadcast_8_piece0 on fa3f124442cb:33719 in memory (size: 120.0 B, free: 1767.4 MiB)\n",
            "25/08/06 08:41:26 INFO BlockManagerInfo: Removed broadcast_10_piece0 on fa3f124442cb:33719 in memory (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:27 INFO BlockManagerInfo: Removed broadcast_12_piece0 on fa3f124442cb:33719 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:27 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 3969 bytes result sent to driver\n",
            "25/08/06 08:41:27 INFO BlockManagerInfo: Removed broadcast_13_piece0 on fa3f124442cb:33719 in memory (size: 120.0 B, free: 1767.4 MiB)\n",
            "25/08/06 08:41:27 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 549 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:27 INFO DAGScheduler: ShuffleMapStage 10 (show at Task3.java:54) finished in 0.582 s\n",
            "25/08/06 08:41:27 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 08:41:27 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:27 INFO DAGScheduler: running: Set()\n",
            "25/08/06 08:41:27 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 08:41:27 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 08:41:27 INFO BlockManagerInfo: Removed broadcast_5_piece0 on fa3f124442cb:33719 in memory (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:41:27 INFO BlockManagerInfo: Removed broadcast_7_piece0 on fa3f124442cb:33719 in memory (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:41:27 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 08:41:27 INFO BlockManagerInfo: Removed broadcast_9_piece0 on fa3f124442cb:33719 in memory (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:41:27 INFO BlockManagerInfo: Removed broadcast_14_piece0 on fa3f124442cb:33719 in memory (size: 864.0 B, free: 1767.5 MiB)\n",
            "25/08/06 08:41:27 INFO BlockManagerInfo: Removed broadcast_18_piece0 on fa3f124442cb:33719 in memory (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:41:27 INFO BlockManagerInfo: Removed broadcast_15_piece0 on fa3f124442cb:33719 in memory (size: 864.0 B, free: 1767.5 MiB)\n",
            "25/08/06 08:41:27 INFO CodeGenerator: Code generated in 55.446795 ms\n",
            "25/08/06 08:41:27 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 08:41:27 INFO CodeGenerator: Code generated in 34.273927 ms\n",
            "25/08/06 08:41:27 INFO SparkContext: Starting job: show at Task3.java:54\n",
            "25/08/06 08:41:27 INFO DAGScheduler: Got job 11 (show at Task3.java:54) with 1 output partitions\n",
            "25/08/06 08:41:27 INFO DAGScheduler: Final stage: ResultStage 12 (show at Task3.java:54)\n",
            "25/08/06 08:41:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
            "25/08/06 08:41:27 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:27 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[39] at show at Task3.java:54), which has no missing parents\n",
            "25/08/06 08:41:27 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 46.9 KiB, free 1766.0 MiB)\n",
            "25/08/06 08:41:27 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 1766.0 MiB)\n",
            "25/08/06 08:41:27 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on fa3f124442cb:33719 (size: 21.6 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:41:27 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[39] at show at Task3.java:54) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:27 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:27 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 11) (fa3f124442cb, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 08:41:27 INFO Executor: Running task 0.0 in stage 12.0 (TID 11)\n",
            "25/08/06 08:41:27 INFO ShuffleBlockFetcherIterator: Getting 1 (1808.0 B) non-empty blocks including 1 (1808.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:41:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 28 ms\n",
            "25/08/06 08:41:27 INFO Executor: Finished task 0.0 in stage 12.0 (TID 11). 7347 bytes result sent to driver\n",
            "25/08/06 08:41:27 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 11) in 215 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:27 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:27 INFO DAGScheduler: ResultStage 12 (show at Task3.java:54) finished in 0.269 s\n",
            "25/08/06 08:41:27 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
            "25/08/06 08:41:27 INFO DAGScheduler: Job 11 finished: show at Task3.java:54, took 0.309867 s\n",
            "25/08/06 08:41:27 INFO CodeGenerator: Code generated in 12.17892 ms\n",
            "25/08/06 08:41:27 INFO CodeGenerator: Code generated in 13.462934 ms\n",
            "+-----------+------------------+\n",
            "|    country|      totalRevenue|\n",
            "+-----------+------------------+\n",
            "|        USA|3040029.5199999996|\n",
            "|      Spain| 994438.5300000003|\n",
            "|     France| 965750.5800000001|\n",
            "|  Australia|509385.81999999995|\n",
            "|New Zealand|         392486.59|\n",
            "|         UK|391503.89999999997|\n",
            "|      Italy|325254.55000000005|\n",
            "|    Finland|         295149.35|\n",
            "|  Singapore|261671.59999999998|\n",
            "|     Canada|         205911.86|\n",
            "|    Denmark|          197356.3|\n",
            "|    Germany|         196470.99|\n",
            "|      Japan|         167909.95|\n",
            "|   Norway  |         166621.51|\n",
            "|    Austria|         136119.99|\n",
            "|     Sweden|         120457.09|\n",
            "|Switzerland|         108777.92|\n",
            "|     Norway|         104224.79|\n",
            "|    Belgium|          91471.03|\n",
            "|Philippines|           87468.3|\n",
            "+-----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "25/08/06 08:41:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 08:41:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#12)\n",
            "25/08/06 08:41:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 08:41:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#38)\n",
            "25/08/06 08:41:27 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 200.2 KiB, free 1765.8 MiB)\n",
            "25/08/06 08:41:27 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1765.8 MiB)\n",
            "25/08/06 08:41:27 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on fa3f124442cb:33719 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:41:27 INFO SparkContext: Created broadcast 23 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:41:27 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:27 INFO DAGScheduler: Got job 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:41:27 INFO DAGScheduler: Final stage: ResultStage 13 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:41:27 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:27 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:27 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[43] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:41:27 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 13.6 KiB, free 1765.8 MiB)\n",
            "25/08/06 08:41:27 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1765.8 MiB)\n",
            "25/08/06 08:41:27 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on fa3f124442cb:33719 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:27 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[43] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:27 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:27 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 12) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 08:41:27 INFO Executor: Running task 0.0 in stage 13.0 (TID 12)\n",
            "25/08/06 08:41:27 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-23671a68-7d7e-43a5-987e-fb6eb20921c3-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 08:41:27 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 08:41:27 INFO Executor: Finished task 0.0 in stage 13.0 (TID 12). 3277 bytes result sent to driver\n",
            "25/08/06 08:41:27 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 12) in 44 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:27 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:27 INFO DAGScheduler: ResultStage 13 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.056 s\n",
            "25/08/06 08:41:27 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
            "25/08/06 08:41:27 INFO DAGScheduler: Job 12 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.060519 s\n",
            "25/08/06 08:41:28 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 1027.1 KiB, free 1764.7 MiB)\n",
            "25/08/06 08:41:28 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 1764.7 MiB)\n",
            "25/08/06 08:41:28 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on fa3f124442cb:33719 (size: 2.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:28 INFO SparkContext: Created broadcast 25 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:28 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 08:41:28 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#38)\n",
            "25/08/06 08:41:28 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 200.2 KiB, free 1764.6 MiB)\n",
            "25/08/06 08:41:28 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1764.5 MiB)\n",
            "25/08/06 08:41:28 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on fa3f124442cb:33719 (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:28 INFO SparkContext: Created broadcast 26 from parquet at Task3.java:55\n",
            "25/08/06 08:41:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Registering RDD 47 (parquet at Task3.java:55) as input to shuffle 1\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Got map stage job 13 (parquet at Task3.java:55) with 1 output partitions\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (parquet at Task3.java:55)\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[47] at parquet at Task3.java:55), which has no missing parents\n",
            "25/08/06 08:41:28 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 48.1 KiB, free 1764.5 MiB)\n",
            "25/08/06 08:41:28 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 1764.4 MiB)\n",
            "25/08/06 08:41:28 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on fa3f124442cb:33719 (size: 21.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:28 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[47] at parquet at Task3.java:55) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:28 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:28 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 13) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7974 bytes) \n",
            "25/08/06 08:41:28 INFO Executor: Running task 0.0 in stage 14.0 (TID 13)\n",
            "25/08/06 08:41:28 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments/part-00000-d589cdd0-28a9-4494-a3a5-0147b264c1d8-c000.snappy.parquet, range: 0-7229, partition values: [empty row]\n",
            "25/08/06 08:41:28 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 08:41:28 INFO Executor: Finished task 0.0 in stage 14.0 (TID 13). 3926 bytes result sent to driver\n",
            "25/08/06 08:41:28 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 13) in 117 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:28 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:28 INFO DAGScheduler: ShuffleMapStage 14 (parquet at Task3.java:55) finished in 0.136 s\n",
            "25/08/06 08:41:28 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 08:41:28 INFO DAGScheduler: running: Set()\n",
            "25/08/06 08:41:28 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 08:41:28 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 08:41:28 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 08:41:28 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 08:41:28 INFO CodeGenerator: Code generated in 14.093501 ms\n",
            "25/08/06 08:41:28 INFO SparkContext: Starting job: parquet at Task3.java:55\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Got job 14 (parquet at Task3.java:55) with 1 output partitions\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Final stage: ResultStage 16 (parquet at Task3.java:55)\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[52] at parquet at Task3.java:55), which has no missing parents\n",
            "25/08/06 08:41:28 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 47.3 KiB, free 1764.4 MiB)\n",
            "25/08/06 08:41:28 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 21.8 KiB, free 1764.4 MiB)\n",
            "25/08/06 08:41:28 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on fa3f124442cb:33719 (size: 21.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:28 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[52] at parquet at Task3.java:55) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:28 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:28 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 14) (fa3f124442cb, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 08:41:28 INFO Executor: Running task 0.0 in stage 16.0 (TID 14)\n",
            "25/08/06 08:41:28 INFO ShuffleBlockFetcherIterator: Getting 1 (1808.0 B) non-empty blocks including 1 (1808.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:41:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 08:41:28 INFO CodeGenerator: Code generated in 28.556599 ms\n",
            "25/08/06 08:41:28 INFO Executor: Finished task 0.0 in stage 16.0 (TID 14). 7157 bytes result sent to driver\n",
            "25/08/06 08:41:28 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 14) in 93 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:28 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:28 INFO DAGScheduler: ResultStage 16 (parquet at Task3.java:55) finished in 0.119 s\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Job 14 finished: parquet at Task3.java:55, took 0.130700 s\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Registering RDD 53 (parquet at Task3.java:55) as input to shuffle 2\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Got map stage job 15 (parquet at Task3.java:55) with 1 output partitions\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Final stage: ShuffleMapStage 18 (parquet at Task3.java:55)\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[53] at parquet at Task3.java:55), which has no missing parents\n",
            "25/08/06 08:41:28 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 47.9 KiB, free 1764.3 MiB)\n",
            "25/08/06 08:41:28 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 22.1 KiB, free 1764.3 MiB)\n",
            "25/08/06 08:41:28 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on fa3f124442cb:33719 (size: 22.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:41:28 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[53] at parquet at Task3.java:55) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:28 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:28 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 15) (fa3f124442cb, executor driver, partition 0, NODE_LOCAL, 7352 bytes) \n",
            "25/08/06 08:41:28 INFO Executor: Running task 0.0 in stage 18.0 (TID 15)\n",
            "25/08/06 08:41:28 INFO BlockManagerInfo: Removed broadcast_20_piece0 on fa3f124442cb:33719 in memory (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:28 INFO ShuffleBlockFetcherIterator: Getting 1 (1808.0 B) non-empty blocks including 1 (1808.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:41:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
            "25/08/06 08:41:28 INFO BlockManagerInfo: Removed broadcast_22_piece0 on fa3f124442cb:33719 in memory (size: 21.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:28 INFO BlockManagerInfo: Removed broadcast_19_piece0 on fa3f124442cb:33719 in memory (size: 2.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:28 INFO Executor: Finished task 0.0 in stage 18.0 (TID 15). 6441 bytes result sent to driver\n",
            "25/08/06 08:41:28 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 15) in 149 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:28 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:28 INFO DAGScheduler: ShuffleMapStage 18 (parquet at Task3.java:55) finished in 0.241 s\n",
            "25/08/06 08:41:28 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 08:41:28 INFO DAGScheduler: running: Set()\n",
            "25/08/06 08:41:28 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 08:41:28 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 08:41:28 INFO BlockManagerInfo: Removed broadcast_27_piece0 on fa3f124442cb:33719 in memory (size: 21.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:28 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 08:41:28 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:41:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:41:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:41:28 INFO BlockManagerInfo: Removed broadcast_21_piece0 on fa3f124442cb:33719 in memory (size: 21.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:41:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:41:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:41:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:41:29 INFO BlockManagerInfo: Removed broadcast_28_piece0 on fa3f124442cb:33719 in memory (size: 21.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:41:29 INFO BlockManagerInfo: Removed broadcast_17_piece0 on fa3f124442cb:33719 in memory (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:41:29 INFO BlockManagerInfo: Removed broadcast_24_piece0 on fa3f124442cb:33719 in memory (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:41:29 INFO CodeGenerator: Code generated in 28.950104 ms\n",
            "25/08/06 08:41:29 INFO SparkContext: Starting job: parquet at Task3.java:55\n",
            "25/08/06 08:41:29 INFO DAGScheduler: Got job 16 (parquet at Task3.java:55) with 1 output partitions\n",
            "25/08/06 08:41:29 INFO DAGScheduler: Final stage: ResultStage 21 (parquet at Task3.java:55)\n",
            "25/08/06 08:41:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)\n",
            "25/08/06 08:41:29 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:29 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[56] at parquet at Task3.java:55), which has no missing parents\n",
            "25/08/06 08:41:29 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 243.6 KiB, free 1765.8 MiB)\n",
            "25/08/06 08:41:29 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 91.0 KiB, free 1765.7 MiB)\n",
            "25/08/06 08:41:29 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on fa3f124442cb:33719 (size: 91.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:29 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[56] at parquet at Task3.java:55) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:29 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:29 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 16) (fa3f124442cb, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 08:41:29 INFO Executor: Running task 0.0 in stage 21.0 (TID 16)\n",
            "25/08/06 08:41:29 INFO ShuffleBlockFetcherIterator: Getting 1 (1808.0 B) non-empty blocks including 1 (1808.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:41:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
            "25/08/06 08:41:29 INFO CodeGenerator: Code generated in 29.33871 ms\n",
            "25/08/06 08:41:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:41:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:41:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:41:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:41:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:41:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:41:29 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:41:29 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:41:29 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:41:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalRevenue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary country (STRING);\n",
            "  optional double totalRevenue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:41:29 INFO FileOutputCommitter: Saved output of task 'attempt_202508060841298877216694541369899_0021_m_000000_16' to file:/content/data/output/task3/revenue_by_country/_temporary/0/task_202508060841298877216694541369899_0021_m_000000\n",
            "25/08/06 08:41:29 INFO SparkHadoopMapRedUtil: attempt_202508060841298877216694541369899_0021_m_000000_16: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 08:41:29 INFO Executor: Finished task 0.0 in stage 21.0 (TID 16). 8724 bytes result sent to driver\n",
            "25/08/06 08:41:29 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 16) in 335 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:29 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:29 INFO DAGScheduler: ResultStage 21 (parquet at Task3.java:55) finished in 0.383 s\n",
            "25/08/06 08:41:29 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
            "25/08/06 08:41:29 INFO DAGScheduler: Job 16 finished: parquet at Task3.java:55, took 0.394841 s\n",
            "25/08/06 08:41:29 INFO FileFormatWriter: Start to commit write Job c652312f-6c9c-4002-9365-5dcc7ef7883f.\n",
            "25/08/06 08:41:29 INFO FileFormatWriter: Write Job c652312f-6c9c-4002-9365-5dcc7ef7883f committed. Elapsed time: 34 ms.\n",
            "25/08/06 08:41:29 INFO FileFormatWriter: Finished processing stats for write job c652312f-6c9c-4002-9365-5dcc7ef7883f.\n",
            "25/08/06 08:41:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 08:41:29 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 08:41:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 08:41:29 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#8),isnotnull(employeeNumber#6)\n",
            "25/08/06 08:41:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(salesRepEmployeeNumber),IsNotNull(customerNumber)\n",
            "25/08/06 08:41:29 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(salesRepEmployeeNumber#23),isnotnull(customerNumber#12)\n",
            "25/08/06 08:41:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 08:41:29 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#38)\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 200.2 KiB, free 1765.5 MiB)\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1765.5 MiB)\n",
            "25/08/06 08:41:30 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on fa3f124442cb:33719 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:30 INFO SparkContext: Created broadcast 31 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:41:30 INFO CodeGenerator: Code generated in 60.343126 ms\n",
            "25/08/06 08:41:30 INFO CodeGenerator: Code generated in 65.467955 ms\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 200.2 KiB, free 1765.3 MiB)\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 200.2 KiB, free 1765.1 MiB)\n",
            "25/08/06 08:41:30 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Got job 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Final stage: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[60] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 13.6 KiB, free 1765.1 MiB)\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1765.1 MiB)\n",
            "25/08/06 08:41:30 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on fa3f124442cb:33719 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:30 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[60] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:30 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1765.1 MiB)\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1765.0 MiB)\n",
            "25/08/06 08:41:30 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on fa3f124442cb:33719 (size: 34.7 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:41:30 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 08:41:30 INFO Executor: Running task 0.0 in stage 22.0 (TID 17)\n",
            "25/08/06 08:41:30 INFO FileScanRDD: Reading File path: file:///content/data/parquet/employees/part-00000-dcd7ad0b-72a1-44fd-a1a1-f1d3add83c01-c000.snappy.parquet, range: 0-1518, partition values: [empty row]\n",
            "25/08/06 08:41:30 INFO SparkContext: Created broadcast 32 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:41:30 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on fa3f124442cb:33719 (size: 34.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:41:30 INFO SparkContext: Created broadcast 33 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:41:30 INFO FilterCompat: Filtering using predicate: and(noteq(officeCode, null), noteq(employeeNumber, null))\n",
            "25/08/06 08:41:30 INFO Executor: Finished task 0.0 in stage 22.0 (TID 17). 2200 bytes result sent to driver\n",
            "25/08/06 08:41:30 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 108 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:30 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:30 INFO DAGScheduler: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.127 s\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Job 17 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.132380 s\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 32.0 MiB, free 1733.0 MiB)\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 864.0 B, free 1733.0 MiB)\n",
            "25/08/06 08:41:30 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on fa3f124442cb:33719 (size: 864.0 B, free: 1767.3 MiB)\n",
            "25/08/06 08:41:30 INFO SparkContext: Created broadcast 35 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:30 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Got job 18 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Final stage: ResultStage 23 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[68] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 13.6 KiB, free 1733.0 MiB)\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1733.0 MiB)\n",
            "25/08/06 08:41:30 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on fa3f124442cb:33719 (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:41:30 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 08:41:30 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 08:41:30 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:30 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[68] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:30 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:30 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 08:41:30 INFO Executor: Running task 0.0 in stage 23.0 (TID 18)\n",
            "25/08/06 08:41:30 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments/part-00000-d589cdd0-28a9-4494-a3a5-0147b264c1d8-c000.snappy.parquet, range: 0-7229, partition values: [empty row]\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Got job 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Final stage: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[67] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 13.6 KiB, free 1733.0 MiB)\n",
            "25/08/06 08:41:30 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1733.0 MiB)\n",
            "25/08/06 08:41:30 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on fa3f124442cb:33719 (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:41:30 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[67] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:30 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:30 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 08:41:30 INFO Executor: Running task 0.0 in stage 24.0 (TID 19)\n",
            "25/08/06 08:41:30 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-23671a68-7d7e-43a5-987e-fb6eb20921c3-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 08:41:30 INFO Executor: Finished task 0.0 in stage 23.0 (TID 18). 4915 bytes result sent to driver\n",
            "25/08/06 08:41:30 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 88 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:30 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:30 INFO DAGScheduler: ResultStage 23 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.115 s\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Job 18 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.118867 s\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 1027.1 KiB, free 1732.0 MiB)\n",
            "25/08/06 08:41:30 INFO FilterCompat: Filtering using predicate: and(noteq(salesRepEmployeeNumber, null), noteq(customerNumber, null))\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 1732.0 MiB)\n",
            "25/08/06 08:41:30 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on fa3f124442cb:33719 (size: 5.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:41:30 INFO SparkContext: Created broadcast 38 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:30 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 08:41:30 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 08:41:30 INFO Executor: Finished task 0.0 in stage 24.0 (TID 19). 1752 bytes result sent to driver\n",
            "25/08/06 08:41:30 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 104 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:30 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:30 INFO DAGScheduler: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.152 s\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished\n",
            "25/08/06 08:41:30 INFO DAGScheduler: Job 19 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.169513 s\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 24.0 B, free 1732.0 MiB)\n",
            "25/08/06 08:41:30 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 120.0 B, free 1732.0 MiB)\n",
            "25/08/06 08:41:30 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on fa3f124442cb:33719 (size: 120.0 B, free: 1767.3 MiB)\n",
            "25/08/06 08:41:30 INFO SparkContext: Created broadcast 39 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "+----------+----+-------+-------------+\n",
            "|officeCode|city|country|officeRevenue|\n",
            "+----------+----+-------+-------------+\n",
            "+----------+----+-------+-------------+\n",
            "\n",
            "25/08/06 08:41:30 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 08:41:30 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 08:41:30 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 08:41:30 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#8),isnotnull(employeeNumber#6)\n",
            "25/08/06 08:41:30 INFO FileSourceStrategy: Pushed Filters: IsNotNull(salesRepEmployeeNumber),IsNotNull(customerNumber)\n",
            "25/08/06 08:41:30 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(salesRepEmployeeNumber#23),isnotnull(customerNumber#12)\n",
            "25/08/06 08:41:30 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 08:41:30 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#38)\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 200.2 KiB, free 1731.8 MiB)\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 200.2 KiB, free 1731.6 MiB)\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 200.2 KiB, free 1731.4 MiB)\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1731.4 MiB)\n",
            "25/08/06 08:41:31 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on fa3f124442cb:33719 (size: 34.7 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:41:31 INFO SparkContext: Created broadcast 40 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1731.3 MiB)\n",
            "25/08/06 08:41:31 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on fa3f124442cb:33719 (size: 34.8 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:41:31 INFO SparkContext: Created broadcast 41 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1731.3 MiB)\n",
            "25/08/06 08:41:31 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on fa3f124442cb:33719 (size: 34.7 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:41:31 INFO SparkContext: Created broadcast 42 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:41:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:41:31 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Got job 20 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Final stage: ResultStage 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[72] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 13.6 KiB, free 1731.3 MiB)\n",
            "25/08/06 08:41:31 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1731.3 MiB)\n",
            "25/08/06 08:41:31 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on fa3f124442cb:33719 (size: 6.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:41:31 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[72] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:31 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:31 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 20) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 08:41:31 INFO Executor: Running task 0.0 in stage 25.0 (TID 20)\n",
            "25/08/06 08:41:31 INFO FileScanRDD: Reading File path: file:///content/data/parquet/employees/part-00000-dcd7ad0b-72a1-44fd-a1a1-f1d3add83c01-c000.snappy.parquet, range: 0-1518, partition values: [empty row]\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Got job 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Final stage: ResultStage 26 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:31 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[78] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 13.6 KiB, free 1731.3 MiB)\n",
            "25/08/06 08:41:31 INFO FilterCompat: Filtering using predicate: and(noteq(officeCode, null), noteq(employeeNumber, null))\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1731.3 MiB)\n",
            "25/08/06 08:41:31 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on fa3f124442cb:33719 (size: 6.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:41:31 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[78] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:31 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:31 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 21) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 08:41:31 INFO Executor: Running task 0.0 in stage 26.0 (TID 21)\n",
            "25/08/06 08:41:31 INFO Executor: Finished task 0.0 in stage 25.0 (TID 20). 2200 bytes result sent to driver\n",
            "25/08/06 08:41:31 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-23671a68-7d7e-43a5-987e-fb6eb20921c3-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 08:41:31 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 20) in 60 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:31 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:31 INFO DAGScheduler: Got job 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Final stage: ResultStage 27 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[80] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 13.6 KiB, free 1731.2 MiB)\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1731.2 MiB)\n",
            "25/08/06 08:41:31 INFO FilterCompat: Filtering using predicate: and(noteq(salesRepEmployeeNumber, null), noteq(customerNumber, null))\n",
            "25/08/06 08:41:31 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on fa3f124442cb:33719 (size: 6.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:41:31 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[80] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:31 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:31 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 22) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 08:41:31 INFO Executor: Running task 0.0 in stage 27.0 (TID 22)\n",
            "25/08/06 08:41:31 INFO DAGScheduler: ResultStage 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.117 s\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Job 20 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.121811 s\n",
            "25/08/06 08:41:31 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments/part-00000-d589cdd0-28a9-4494-a3a5-0147b264c1d8-c000.snappy.parquet, range: 0-7229, partition values: [empty row]\n",
            "25/08/06 08:41:31 INFO Executor: Finished task 0.0 in stage 26.0 (TID 21). 1752 bytes result sent to driver\n",
            "25/08/06 08:41:31 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 21) in 188 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:31 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:31 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 08:41:31 INFO DAGScheduler: ResultStage 26 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.213 s\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Job 21 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.239633 s\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 24.0 B, free 1731.2 MiB)\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 120.0 B, free 1731.2 MiB)\n",
            "25/08/06 08:41:31 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on fa3f124442cb:33719 (size: 120.0 B, free: 1767.2 MiB)\n",
            "25/08/06 08:41:31 INFO SparkContext: Created broadcast 46 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:31 INFO BlockManagerInfo: Removed broadcast_37_piece0 on fa3f124442cb:33719 in memory (size: 6.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 32.0 MiB, free 1699.3 MiB)\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 864.0 B, free 1699.3 MiB)\n",
            "25/08/06 08:41:31 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on fa3f124442cb:33719 (size: 864.0 B, free: 1767.2 MiB)\n",
            "25/08/06 08:41:31 INFO SparkContext: Created broadcast 47 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:31 INFO Executor: Finished task 0.0 in stage 27.0 (TID 22). 4958 bytes result sent to driver\n",
            "25/08/06 08:41:31 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 22) in 192 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:31 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:31 INFO DAGScheduler: ResultStage 27 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.215 s\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Job 22 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.279826 s\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 1027.1 KiB, free 1698.3 MiB)\n",
            "25/08/06 08:41:31 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 08:41:31 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 08:41:31 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 1698.2 MiB)\n",
            "25/08/06 08:41:31 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on fa3f124442cb:33719 (size: 5.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:41:31 INFO SparkContext: Created broadcast 48 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:41:31 INFO BlockManagerInfo: Removed broadcast_39_piece0 on fa3f124442cb:33719 in memory (size: 120.0 B, free: 1767.2 MiB)\n",
            "25/08/06 08:41:31 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:41:31 INFO BlockManagerInfo: Removed broadcast_36_piece0 on fa3f124442cb:33719 in memory (size: 6.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:41:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:41:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:41:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:41:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:41:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:41:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:41:31 INFO BlockManagerInfo: Removed broadcast_32_piece0 on fa3f124442cb:33719 in memory (size: 34.7 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:41:31 INFO BlockManagerInfo: Removed broadcast_29_piece0 on fa3f124442cb:33719 in memory (size: 22.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 08:41:31 INFO SparkContext: Starting job: parquet at Task3.java:72\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Got job 23 (parquet at Task3.java:72) with 1 output partitions\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Final stage: ResultStage 28 (parquet at Task3.java:72)\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:41:31 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[84] at parquet at Task3.java:72), which has no missing parents\n",
            "25/08/06 08:41:31 INFO BlockManagerInfo: Removed broadcast_30_piece0 on fa3f124442cb:33719 in memory (size: 91.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:41:32 INFO BlockManagerInfo: Removed broadcast_31_piece0 on fa3f124442cb:33719 in memory (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:32 INFO BlockManagerInfo: Removed broadcast_33_piece0 on fa3f124442cb:33719 in memory (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:32 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 205.1 KiB, free 1699.2 MiB)\n",
            "25/08/06 08:41:32 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 73.1 KiB, free 1699.1 MiB)\n",
            "25/08/06 08:41:32 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on fa3f124442cb:33719 (size: 73.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:41:32 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:41:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[84] at parquet at Task3.java:72) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:41:32 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:41:32 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 23) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7384 bytes) \n",
            "25/08/06 08:41:32 INFO Executor: Running task 0.0 in stage 28.0 (TID 23)\n",
            "25/08/06 08:41:32 INFO BlockManagerInfo: Removed broadcast_38_piece0 on fa3f124442cb:33719 in memory (size: 5.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:41:32 INFO BlockManagerInfo: Removed broadcast_26_piece0 on fa3f124442cb:33719 in memory (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:32 INFO BlockManagerInfo: Removed broadcast_25_piece0 on fa3f124442cb:33719 in memory (size: 2.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:41:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:41:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:41:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:41:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:41:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:41:32 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:41:32 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:41:32 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:41:32 INFO BlockManagerInfo: Removed broadcast_34_piece0 on fa3f124442cb:33719 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"officeRevenue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary officeCode (STRING);\n",
            "  optional binary city (STRING);\n",
            "  optional binary country (STRING);\n",
            "  optional double officeRevenue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:41:32 INFO BlockManagerInfo: Removed broadcast_23_piece0 on fa3f124442cb:33719 in memory (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:32 INFO BlockManagerInfo: Removed broadcast_43_piece0 on fa3f124442cb:33719 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:41:32 INFO BlockManagerInfo: Removed broadcast_35_piece0 on fa3f124442cb:33719 in memory (size: 864.0 B, free: 1767.4 MiB)\n",
            "25/08/06 08:41:32 INFO FileOutputCommitter: Saved output of task 'attempt_202508060841311816251594673933668_0028_m_000000_23' to file:/content/data/output/task3/top_offices/_temporary/0/task_202508060841311816251594673933668_0028_m_000000\n",
            "25/08/06 08:41:32 INFO SparkHadoopMapRedUtil: attempt_202508060841311816251594673933668_0028_m_000000_23: Committed. Elapsed time: 2 ms.\n",
            "25/08/06 08:41:32 INFO Executor: Finished task 0.0 in stage 28.0 (TID 23). 2248 bytes result sent to driver\n",
            "25/08/06 08:41:32 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 23) in 208 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:41:32 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:41:32 INFO DAGScheduler: ResultStage 28 (parquet at Task3.java:72) finished in 0.400 s\n",
            "25/08/06 08:41:32 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:41:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished\n",
            "25/08/06 08:41:32 INFO DAGScheduler: Job 23 finished: parquet at Task3.java:72, took 0.405999 s\n",
            "25/08/06 08:41:32 INFO FileFormatWriter: Start to commit write Job 5688d39a-e334-4e76-bc55-f4854af67c6f.\n",
            "25/08/06 08:41:32 INFO FileFormatWriter: Write Job 5688d39a-e334-4e76-bc55-f4854af67c6f committed. Elapsed time: 30 ms.\n",
            "25/08/06 08:41:32 INFO FileFormatWriter: Finished processing stats for write job 5688d39a-e334-4e76-bc55-f4854af67c6f.\n",
            "25/08/06 08:41:32 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 08:41:32 INFO SparkUI: Stopped Spark web UI at http://fa3f124442cb:4040\n",
            "25/08/06 08:41:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 08:41:32 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 08:41:32 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 08:41:32 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 08:41:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 08:41:32 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 08:41:32 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 08:41:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-44146f5b-3f4e-42b0-b97e-c6f36188e52c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 4: Performance Optimization"
      ],
      "metadata": {
        "id": "A96j4LwhQpfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Task4.java\n",
        "import org.apache.spark.api.java.JavaPairRDD;\n",
        "import org.apache.spark.api.java.JavaRDD;\n",
        "import org.apache.spark.api.java.JavaSparkContext;\n",
        "import org.apache.spark.broadcast.Broadcast;\n",
        "import org.apache.spark.sql.*;\n",
        "import org.apache.spark.sql.types.*;\n",
        "import org.apache.spark.storage.StorageLevel;\n",
        "import scala.Tuple2;\n",
        "\n",
        "import java.util.Iterator;\n",
        "\n",
        "public class Task4 {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Task 4: Performance Optimization\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n",
        "\n",
        "        Dataset<Row> customers = spark.read().parquet(\"data/parquet/customers\");\n",
        "        Dataset<Row> payments = spark.read().parquet(\"data/parquet/payments\");\n",
        "        Dataset<Row> offices = spark.read().parquet(\"data/parquet/offices\");\n",
        "\n",
        "        // Broadcast smaller dataset\n",
        "        Broadcast<Dataset<Row>> broadcastOffices = jsc.broadcast(offices);\n",
        "\n",
        "        // Cache payments since we’ll use it multiple times\n",
        "        payments.persist(StorageLevel.MEMORY_AND_DISK());\n",
        "\n",
        "        // 1. Aggregate revenue per country using mapPartitions\n",
        "        JavaRDD<Row> revenueByCountryRDD = payments\n",
        "                .join(customers, \"customerNumber\")\n",
        "                .select(\"country\", \"amount\")\n",
        "                .javaRDD()\n",
        "                .mapPartitions(iterator -> {\n",
        "                    java.util.Map<String, Double> map = new java.util.HashMap<>();\n",
        "                    while (iterator.hasNext()) {\n",
        "                        Row row = iterator.next();\n",
        "                        String country = row.getString(0);\n",
        "                        double amount = row.getDouble(1);\n",
        "                        map.put(country, map.getOrDefault(country, 0.0) + amount);\n",
        "                    }\n",
        "                    java.util.List<Row> rows = new java.util.ArrayList<>();\n",
        "                    for (java.util.Map.Entry<String, Double> entry : map.entrySet()) {\n",
        "                        rows.add(RowFactory.create(entry.getKey(), entry.getValue()));\n",
        "                    }\n",
        "                    return rows.iterator();\n",
        "                });\n",
        "\n",
        "        // Define schema\n",
        "        StructType schema = new StructType()\n",
        "                .add(\"country\", DataTypes.StringType)\n",
        "                .add(\"totalRevenue\", DataTypes.DoubleType);\n",
        "\n",
        "        Dataset<Row> revenueByCountry = spark.createDataFrame(revenueByCountryRDD, schema);\n",
        "        revenueByCountry.show();\n",
        "\n",
        "        // 2. Aggregate using aggregateByKey\n",
        "        JavaPairRDD<String, Double> countryRevenuePair = payments\n",
        "                .join(customers, \"customerNumber\")\n",
        "                .select(\"country\", \"amount\")\n",
        "                .javaRDD()\n",
        "                .mapToPair(row -> new Tuple2<>(row.getString(0), row.getDouble(1)));\n",
        "\n",
        "        JavaPairRDD<String, Double> aggregatedRevenue = countryRevenuePair.aggregateByKey(\n",
        "                0.0,\n",
        "                Double::sum,\n",
        "                Double::sum\n",
        "        );\n",
        "\n",
        "        Dataset<Row> aggregatedDF = spark.createDataFrame(\n",
        "                aggregatedRevenue.map(tuple -> RowFactory.create(tuple._1, tuple._2)),\n",
        "                schema\n",
        "        );\n",
        "        aggregatedDF.show();\n",
        "\n",
        "        // 3. Lazy evaluation example\n",
        "        Dataset<Row> lazyEval = payments.filter(\"amount > 1000\");\n",
        "        System.out.println(\"Lazy evaluation example defined. Not triggered yet.\");\n",
        "        lazyEval.show(); // Action triggers execution\n",
        "\n",
        "        // Save output\n",
        "        revenueByCountry.write().mode(SaveMode.Overwrite).parquet(\"data/output/task4/revenueByCountry\");\n",
        "        aggregatedDF.write().mode(SaveMode.Overwrite).parquet(\"data/output/task4/aggregatedRevenue\");\n",
        "\n",
        "        // Unpersist after use\n",
        "        payments.unpersist();\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hke8Tm9fIYvf",
        "outputId": "0ff4ef0d-37c2-469f-af7c-52403b69782d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Task4.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" Task4.java"
      ],
      "metadata": {
        "id": "F0Vbt810RDVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$SPARK_HOME/jars/*\" Task4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wuWvPxeRha5",
        "outputId": "ee669ead-feeb-4c01-de8f-4c77965bf93b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 08:42:00 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 08:42:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 08:42:01 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:42:01 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 08:42:01 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:42:01 INFO SparkContext: Submitted application: Task 4: Performance Optimization\n",
            "25/08/06 08:42:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 08:42:01 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 08:42:01 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 08:42:01 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 08:42:01 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 08:42:01 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 08:42:01 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 08:42:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 08:42:01 INFO Utils: Successfully started service 'sparkDriver' on port 40213.\n",
            "25/08/06 08:42:02 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 08:42:02 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 08:42:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 08:42:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 08:42:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 08:42:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2ee00058-f074-4360-a44c-4a26387335d4\n",
            "25/08/06 08:42:02 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 08:42:02 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 08:42:02 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 08:42:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 08:42:03 INFO Executor: Starting executor ID driver on host fa3f124442cb\n",
            "25/08/06 08:42:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 08:42:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42831.\n",
            "25/08/06 08:42:03 INFO NettyBlockTransferService: Server created on fa3f124442cb:42831\n",
            "25/08/06 08:42:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 08:42:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fa3f124442cb, 42831, None)\n",
            "25/08/06 08:42:03 INFO BlockManagerMasterEndpoint: Registering block manager fa3f124442cb:42831 with 1767.6 MiB RAM, BlockManagerId(driver, fa3f124442cb, 42831, None)\n",
            "25/08/06 08:42:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fa3f124442cb, 42831, None)\n",
            "25/08/06 08:42:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fa3f124442cb, 42831, None)\n",
            "25/08/06 08:42:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 08:42:04 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 08:42:05 INFO InMemoryFileIndex: It took 92 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:42:06 INFO SparkContext: Starting job: parquet at Task4.java:21\n",
            "25/08/06 08:42:06 INFO DAGScheduler: Got job 0 (parquet at Task4.java:21) with 1 output partitions\n",
            "25/08/06 08:42:06 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at Task4.java:21)\n",
            "25/08/06 08:42:06 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:42:06 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:42:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at Task4.java:21), which has no missing parents\n",
            "25/08/06 08:42:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:42:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:42:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fa3f124442cb:42831 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:42:07 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:42:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at Task4.java:21) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:42:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:42:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7562 bytes) \n",
            "25/08/06 08:42:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 08:42:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2391 bytes result sent to driver\n",
            "25/08/06 08:42:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 889 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:42:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:42:08 INFO DAGScheduler: ResultStage 0 (parquet at Task4.java:21) finished in 1.231 s\n",
            "25/08/06 08:42:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:42:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 08:42:08 INFO DAGScheduler: Job 0 finished: parquet at Task4.java:21, took 1.356322 s\n",
            "25/08/06 08:42:08 INFO BlockManagerInfo: Removed broadcast_0_piece0 on fa3f124442cb:42831 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:42:12 INFO InMemoryFileIndex: It took 13 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:42:12 INFO SparkContext: Starting job: parquet at Task4.java:22\n",
            "25/08/06 08:42:12 INFO DAGScheduler: Got job 1 (parquet at Task4.java:22) with 1 output partitions\n",
            "25/08/06 08:42:12 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at Task4.java:22)\n",
            "25/08/06 08:42:12 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:42:12 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:42:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at Task4.java:22), which has no missing parents\n",
            "25/08/06 08:42:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:42:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:42:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fa3f124442cb:42831 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:42:12 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:42:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at Task4.java:22) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:42:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:42:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7561 bytes) \n",
            "25/08/06 08:42:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 08:42:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2054 bytes result sent to driver\n",
            "25/08/06 08:42:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 165 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:42:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:42:12 INFO DAGScheduler: ResultStage 1 (parquet at Task4.java:22) finished in 0.262 s\n",
            "25/08/06 08:42:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:42:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 08:42:12 INFO DAGScheduler: Job 1 finished: parquet at Task4.java:22, took 0.282418 s\n",
            "25/08/06 08:42:12 INFO InMemoryFileIndex: It took 12 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:42:12 INFO SparkContext: Starting job: parquet at Task4.java:23\n",
            "25/08/06 08:42:12 INFO DAGScheduler: Got job 2 (parquet at Task4.java:23) with 1 output partitions\n",
            "25/08/06 08:42:12 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at Task4.java:23)\n",
            "25/08/06 08:42:12 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:42:12 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:42:12 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at Task4.java:23), which has no missing parents\n",
            "25/08/06 08:42:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 102.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 08:42:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.3 MiB)\n",
            "25/08/06 08:42:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fa3f124442cb:42831 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:42:12 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:42:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at Task4.java:23) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:42:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:42:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7560 bytes) \n",
            "25/08/06 08:42:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 08:42:13 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1851 bytes result sent to driver\n",
            "25/08/06 08:42:13 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 118 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:42:13 INFO DAGScheduler: ResultStage 2 (parquet at Task4.java:23) finished in 0.198 s\n",
            "25/08/06 08:42:13 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:42:13 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:42:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 08:42:13 INFO DAGScheduler: Job 2 finished: parquet at Task4.java:23, took 0.217178 s\n",
            "25/08/06 08:42:13 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 3.8 MiB, free 1763.5 MiB)\n",
            "25/08/06 08:42:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 1763.5 MiB)\n",
            "25/08/06 08:42:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fa3f124442cb:42831 (size: 7.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:42:14 INFO SparkContext: Created broadcast 3 from broadcast at Task4.java:26\n",
            "25/08/06 08:42:15 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 08:42:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 08:42:15 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 08:42:15 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#0)\n",
            "25/08/06 08:42:16 INFO CodeGenerator: Code generated in 414.362058 ms\n",
            "25/08/06 08:42:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.2 KiB, free 1763.3 MiB)\n",
            "25/08/06 08:42:16 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1763.3 MiB)\n",
            "25/08/06 08:42:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on fa3f124442cb:42831 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:42:16 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:42:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:42:16 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:42:16 INFO DAGScheduler: Got job 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:42:16 INFO DAGScheduler: Final stage: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:42:16 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:42:16 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:42:16 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[9] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:42:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 13.6 KiB, free 1763.3 MiB)\n",
            "25/08/06 08:42:16 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1763.2 MiB)\n",
            "25/08/06 08:42:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on fa3f124442cb:42831 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:42:16 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:42:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:42:16 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:42:16 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 08:42:16 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 08:42:16 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-23671a68-7d7e-43a5-987e-fb6eb20921c3-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 08:42:16 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 08:42:17 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 08:42:17 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3363 bytes result sent to driver\n",
            "25/08/06 08:42:17 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 929 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:42:17 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:42:17 INFO DAGScheduler: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.950 s\n",
            "25/08/06 08:42:17 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:42:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 08:42:17 INFO DAGScheduler: Job 3 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.959438 s\n",
            "25/08/06 08:42:17 INFO CodeGenerator: Code generated in 18.403509 ms\n",
            "25/08/06 08:42:17 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 1027.1 KiB, free 1762.2 MiB)\n",
            "25/08/06 08:42:17 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 1762.2 MiB)\n",
            "25/08/06 08:42:17 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on fa3f124442cb:42831 (size: 2.5 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:42:17 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:42:17 INFO CodeGenerator: Code generated in 54.278484 ms\n",
            "25/08/06 08:42:17 INFO CodeGenerator: Code generated in 33.355201 ms\n",
            "25/08/06 08:42:17 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 200.4 KiB, free 1762.0 MiB)\n",
            "25/08/06 08:42:17 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1762.0 MiB)\n",
            "25/08/06 08:42:17 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on fa3f124442cb:42831 (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:42:17 INFO SparkContext: Created broadcast 7 from javaRDD at Task4.java:35\n",
            "25/08/06 08:42:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:42:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(customerNumber#26) generates partition filter: ((customerNumber.count#144 - customerNumber.nullCount#143) > 0)\n",
            "25/08/06 08:42:18 INFO CodeGenerator: Code generated in 43.397867 ms\n",
            "25/08/06 08:42:18 INFO SparkContext: Starting job: show at Task4.java:57\n",
            "25/08/06 08:42:18 INFO DAGScheduler: Got job 4 (show at Task4.java:57) with 1 output partitions\n",
            "25/08/06 08:42:18 INFO DAGScheduler: Final stage: ResultStage 4 (show at Task4.java:57)\n",
            "25/08/06 08:42:18 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:42:18 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:42:18 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at show at Task4.java:57), which has no missing parents\n",
            "25/08/06 08:42:18 INFO BlockManagerInfo: Removed broadcast_5_piece0 on fa3f124442cb:42831 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:42:18 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 49.7 KiB, free 1762.0 MiB)\n",
            "25/08/06 08:42:18 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 21.4 KiB, free 1762.0 MiB)\n",
            "25/08/06 08:42:18 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on fa3f124442cb:42831 (size: 21.4 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:42:18 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:42:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at show at Task4.java:57) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:42:18 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:42:18 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 08:42:18 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 08:42:18 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments/part-00000-d589cdd0-28a9-4494-a3a5-0147b264c1d8-c000.snappy.parquet, range: 0-7229, partition values: [empty row]\n",
            "25/08/06 08:42:18 INFO MemoryStore: Block rdd_14_0 stored as values in memory (estimated size 7.3 KiB, free 1762.0 MiB)\n",
            "25/08/06 08:42:18 INFO BlockManagerInfo: Added rdd_14_0 in memory on fa3f124442cb:42831 (size: 7.3 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:42:18 INFO CodeGenerator: Code generated in 13.634928 ms\n",
            "25/08/06 08:42:19 INFO CodeGenerator: Code generated in 55.795794 ms\n",
            "25/08/06 08:42:19 INFO CodeGenerator: Code generated in 24.038733 ms\n",
            "25/08/06 08:42:19 INFO CodeGenerator: Code generated in 110.381948 ms\n",
            "25/08/06 08:42:19 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 3793 bytes result sent to driver\n",
            "25/08/06 08:42:19 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 610 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:42:19 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:42:19 INFO DAGScheduler: ResultStage 4 (show at Task4.java:57) finished in 0.855 s\n",
            "25/08/06 08:42:19 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:42:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 08:42:19 INFO DAGScheduler: Job 4 finished: show at Task4.java:57, took 0.870111 s\n",
            "25/08/06 08:42:19 INFO CodeGenerator: Code generated in 12.973747 ms\n",
            "+-----------+------------------+\n",
            "|    country|      totalRevenue|\n",
            "+-----------+------------------+\n",
            "|        USA|3040029.5199999996|\n",
            "|  Singapore|261671.59999999998|\n",
            "|  Hong Kong|          45480.79|\n",
            "|      Japan|         167909.95|\n",
            "|Philippines|           87468.3|\n",
            "|   Norway  |         166621.51|\n",
            "|Switzerland|         108777.92|\n",
            "|      Spain| 994438.5300000003|\n",
            "|New Zealand|         392486.59|\n",
            "|     Canada|         205911.86|\n",
            "|     Sweden|         120457.09|\n",
            "|    Austria|         136119.99|\n",
            "|    Belgium|          91471.03|\n",
            "|     Norway|         104224.79|\n",
            "|         UK|391503.89999999997|\n",
            "|    Ireland|49898.270000000004|\n",
            "|    Finland|         295149.35|\n",
            "|    Denmark|          197356.3|\n",
            "|      Italy|325254.55000000005|\n",
            "|     France| 965750.5800000001|\n",
            "+-----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "25/08/06 08:42:19 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 08:42:19 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#0)\n",
            "25/08/06 08:42:19 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.2 KiB, free 1761.8 MiB)\n",
            "25/08/06 08:42:19 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1761.7 MiB)\n",
            "25/08/06 08:42:19 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on fa3f124442cb:42831 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:42:19 INFO SparkContext: Created broadcast 9 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:42:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:42:19 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:42:19 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:42:19 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:42:19 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:42:19 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:42:19 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:42:19 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 13.6 KiB, free 1761.7 MiB)\n",
            "25/08/06 08:42:19 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1761.7 MiB)\n",
            "25/08/06 08:42:19 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on fa3f124442cb:42831 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:42:19 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:42:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:42:19 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:42:19 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 08:42:19 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 08:42:19 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-23671a68-7d7e-43a5-987e-fb6eb20921c3-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 08:42:19 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 08:42:19 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3320 bytes result sent to driver\n",
            "25/08/06 08:42:19 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 58 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:42:19 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:42:19 INFO DAGScheduler: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.093 s\n",
            "25/08/06 08:42:19 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:42:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 08:42:19 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.106697 s\n",
            "25/08/06 08:42:19 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 1027.1 KiB, free 1760.7 MiB)\n",
            "25/08/06 08:42:19 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 1760.7 MiB)\n",
            "25/08/06 08:42:19 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on fa3f124442cb:42831 (size: 2.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:42:19 INFO SparkContext: Created broadcast 11 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:42:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(customerNumber#26) generates partition filter: ((customerNumber.count#258 - customerNumber.nullCount#257) > 0)\n",
            "25/08/06 08:42:19 INFO SparkContext: Starting job: show at Task4.java:76\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Registering RDD 37 (mapToPair at Task4.java:64) as input to shuffle 0\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Got job 6 (show at Task4.java:76) with 1 output partitions\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Final stage: ResultStage 7 (show at Task4.java:76)\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[37] at mapToPair at Task4.java:64), which has no missing parents\n",
            "25/08/06 08:42:20 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 44.8 KiB, free 1760.7 MiB)\n",
            "25/08/06 08:42:20 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 1760.6 MiB)\n",
            "25/08/06 08:42:20 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on fa3f124442cb:42831 (size: 19.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:42:20 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[37] at mapToPair at Task4.java:64) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:42:20 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:42:20 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7974 bytes) \n",
            "25/08/06 08:42:20 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "25/08/06 08:42:20 INFO BlockManager: Found block rdd_14_0 locally\n",
            "25/08/06 08:42:20 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 3077 bytes result sent to driver\n",
            "25/08/06 08:42:20 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 177 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:42:20 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:42:20 INFO DAGScheduler: ShuffleMapStage 6 (mapToPair at Task4.java:64) finished in 0.241 s\n",
            "25/08/06 08:42:20 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 08:42:20 INFO DAGScheduler: running: Set()\n",
            "25/08/06 08:42:20 INFO DAGScheduler: waiting: Set(ResultStage 7)\n",
            "25/08/06 08:42:20 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[42] at show at Task4.java:76), which has no missing parents\n",
            "25/08/06 08:42:20 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 23.9 KiB, free 1760.6 MiB)\n",
            "25/08/06 08:42:20 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1760.6 MiB)\n",
            "25/08/06 08:42:20 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on fa3f124442cb:42831 (size: 10.9 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:42:20 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[42] at show at Task4.java:76) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:42:20 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:42:20 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (fa3f124442cb, executor driver, partition 0, NODE_LOCAL, 7181 bytes) \n",
            "25/08/06 08:42:20 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "25/08/06 08:42:20 INFO ShuffleBlockFetcherIterator: Getting 1 (593.0 B) non-empty blocks including 1 (593.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:42:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms\n",
            "25/08/06 08:42:20 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 3037 bytes result sent to driver\n",
            "25/08/06 08:42:20 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 150 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:42:20 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:42:20 INFO DAGScheduler: ResultStage 7 (show at Task4.java:76) finished in 0.183 s\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:42:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Job 6 finished: show at Task4.java:76, took 0.540333 s\n",
            "+-----------+------------------+\n",
            "|    country|      totalRevenue|\n",
            "+-----------+------------------+\n",
            "|  Australia|509385.81999999995|\n",
            "|         UK|391503.89999999997|\n",
            "|    Belgium|          91471.03|\n",
            "|     Canada|         205911.86|\n",
            "|      Japan|         167909.95|\n",
            "|      Italy|325254.55000000005|\n",
            "|     France| 965750.5800000001|\n",
            "|   Norway  |         166621.51|\n",
            "|    Finland|         295149.35|\n",
            "|Switzerland|         108777.92|\n",
            "|      Spain| 994438.5300000003|\n",
            "|        USA|3040029.5199999996|\n",
            "|  Singapore|261671.59999999998|\n",
            "|    Germany|         196470.99|\n",
            "|  Hong Kong|          45480.79|\n",
            "|    Ireland|49898.270000000004|\n",
            "|New Zealand|         392486.59|\n",
            "|     Sweden|         120457.09|\n",
            "|Philippines|           87468.3|\n",
            "|     Norway|         104224.79|\n",
            "+-----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Lazy evaluation example defined. Not triggered yet.\n",
            "25/08/06 08:42:20 INFO CodeGenerator: Code generated in 25.210373 ms\n",
            "25/08/06 08:42:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(amount#29) generates partition filter: ((amount.count#385 - amount.nullCount#384) > 0)\n",
            "25/08/06 08:42:20 INFO DefaultCachedBatchSerializer: Predicate (amount#29 > 1000.0) generates partition filter: (1000.0 < amount.upperBound#382)\n",
            "25/08/06 08:42:20 INFO SparkContext: Starting job: show at Task4.java:81\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Got job 7 (show at Task4.java:81) with 1 output partitions\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Final stage: ResultStage 8 (show at Task4.java:81)\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[47] at show at Task4.java:81), which has no missing parents\n",
            "25/08/06 08:42:20 INFO BlockManagerInfo: Removed broadcast_10_piece0 on fa3f124442cb:42831 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:42:20 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 32.9 KiB, free 1760.6 MiB)\n",
            "25/08/06 08:42:20 INFO BlockManagerInfo: Removed broadcast_13_piece0 on fa3f124442cb:42831 in memory (size: 10.9 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:42:20 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 13.9 KiB, free 1760.6 MiB)\n",
            "25/08/06 08:42:20 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on fa3f124442cb:42831 (size: 13.9 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:42:20 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[47] at show at Task4.java:81) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:42:20 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:42:20 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 08:42:20 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
            "25/08/06 08:42:20 INFO BlockManagerInfo: Removed broadcast_12_piece0 on fa3f124442cb:42831 in memory (size: 19.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:42:20 INFO BlockManager: Found block rdd_14_0 locally\n",
            "25/08/06 08:42:20 INFO CodeGenerator: Code generated in 22.37442 ms\n",
            "25/08/06 08:42:20 INFO BlockManagerInfo: Removed broadcast_8_piece0 on fa3f124442cb:42831 in memory (size: 21.4 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:42:20 INFO CodeGenerator: Code generated in 27.697325 ms\n",
            "25/08/06 08:42:20 INFO Executor: 1 block locks were not released by task 0.0 in stage 8.0 (TID 8)\n",
            "[rdd_14_0]\n",
            "25/08/06 08:42:20 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2800 bytes result sent to driver\n",
            "25/08/06 08:42:20 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 114 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:42:20 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:42:20 INFO DAGScheduler: ResultStage 8 (show at Task4.java:81) finished in 0.144 s\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:42:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
            "25/08/06 08:42:20 INFO DAGScheduler: Job 7 finished: show at Task4.java:81, took 0.153725 s\n",
            "25/08/06 08:42:21 INFO CodeGenerator: Code generated in 31.820363 ms\n",
            "+--------------+-----------+-----------+---------+\n",
            "|customerNumber|checkNumber|paymentDate|   amount|\n",
            "+--------------+-----------+-----------+---------+\n",
            "|           103|   HQ336336| 2004-10-19|  6066.78|\n",
            "|           103|   JM555205| 2003-06-05| 14571.44|\n",
            "|           103|   OM314933| 2004-12-18|  1676.14|\n",
            "|           112|   BO864823| 2004-12-17| 14191.12|\n",
            "|           112|    HQ55022| 2003-06-06| 32641.98|\n",
            "|           112|   ND748579| 2004-08-20| 33347.88|\n",
            "|           114|    GG31455| 2003-05-20| 45864.03|\n",
            "|           114|   MA765515| 2004-12-15| 82261.22|\n",
            "|           114|   NP603840| 2003-05-31|  7565.08|\n",
            "|           114|    NR27552| 2004-03-10| 44894.74|\n",
            "|           119|   DB933704| 2004-11-14| 19501.82|\n",
            "|           119|   LN373447| 2004-08-08| 47924.19|\n",
            "|           119|    NG94694| 2005-02-22| 49523.67|\n",
            "|           121|   DB889831| 2003-02-16| 50218.95|\n",
            "|           121|   FD317790| 2003-10-28|  1491.38|\n",
            "|           121|   KI831359| 2004-11-04| 17876.32|\n",
            "|           121|   MA302151| 2004-11-28| 34638.14|\n",
            "|           124|   AE215433| 2005-03-05|101244.59|\n",
            "|           124|   BG255406| 2004-08-28| 85410.87|\n",
            "|           124|   CQ287967| 2003-04-11|  11044.3|\n",
            "+--------------+-----------+-----------+---------+\n",
            "only showing top 20 rows\n",
            "\n",
            "25/08/06 08:42:21 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:42:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:42:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:42:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:42:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:42:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:42:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:42:21 INFO CodeGenerator: Code generated in 23.630229 ms\n",
            "25/08/06 08:42:21 INFO SparkContext: Starting job: parquet at Task4.java:84\n",
            "25/08/06 08:42:21 INFO DAGScheduler: Got job 8 (parquet at Task4.java:84) with 1 output partitions\n",
            "25/08/06 08:42:21 INFO DAGScheduler: Final stage: ResultStage 9 (parquet at Task4.java:84)\n",
            "25/08/06 08:42:21 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:42:21 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:42:21 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[49] at parquet at Task4.java:84), which has no missing parents\n",
            "25/08/06 08:42:21 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 249.1 KiB, free 1760.5 MiB)\n",
            "25/08/06 08:42:21 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 91.4 KiB, free 1760.4 MiB)\n",
            "25/08/06 08:42:21 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on fa3f124442cb:42831 (size: 91.4 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:42:21 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:42:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[49] at parquet at Task4.java:84) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:42:21 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:42:21 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (fa3f124442cb, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 08:42:21 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
            "25/08/06 08:42:21 INFO BlockManager: Found block rdd_14_0 locally\n",
            "25/08/06 08:42:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:42:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:42:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:42:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:42:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:42:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:42:21 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:42:21 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:42:21 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:42:21 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalRevenue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary country (STRING);\n",
            "  optional double totalRevenue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:42:21 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 08:42:21 INFO FileOutputCommitter: Saved output of task 'attempt_202508060842218705580264999525540_0009_m_000000_9' to file:/content/data/output/task4/revenueByCountry/_temporary/0/task_202508060842218705580264999525540_0009_m_000000\n",
            "25/08/06 08:42:21 INFO SparkHadoopMapRedUtil: attempt_202508060842218705580264999525540_0009_m_000000_9: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 08:42:21 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 4205 bytes result sent to driver\n",
            "25/08/06 08:42:21 INFO BlockManagerInfo: Removed broadcast_14_piece0 on fa3f124442cb:42831 in memory (size: 13.9 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:42:21 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 418 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:42:21 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:42:21 INFO DAGScheduler: ResultStage 9 (parquet at Task4.java:84) finished in 0.503 s\n",
            "25/08/06 08:42:21 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:42:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "25/08/06 08:42:21 INFO DAGScheduler: Job 8 finished: parquet at Task4.java:84, took 0.523790 s\n",
            "25/08/06 08:42:21 INFO FileFormatWriter: Start to commit write Job d34ec8fe-6dfe-4cb1-852a-4eb2e45f1698.\n",
            "25/08/06 08:42:21 INFO FileFormatWriter: Write Job d34ec8fe-6dfe-4cb1-852a-4eb2e45f1698 committed. Elapsed time: 63 ms.\n",
            "25/08/06 08:42:21 INFO FileFormatWriter: Finished processing stats for write job d34ec8fe-6dfe-4cb1-852a-4eb2e45f1698.\n",
            "25/08/06 08:42:22 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:42:22 INFO BlockManagerInfo: Removed broadcast_2_piece0 on fa3f124442cb:42831 in memory (size: 36.9 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:42:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:42:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:42:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:42:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:42:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:42:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:42:22 INFO BlockManagerInfo: Removed broadcast_1_piece0 on fa3f124442cb:42831 in memory (size: 36.9 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:42:22 INFO SparkContext: Starting job: parquet at Task4.java:85\n",
            "25/08/06 08:42:22 INFO DAGScheduler: Got job 9 (parquet at Task4.java:85) with 1 output partitions\n",
            "25/08/06 08:42:22 INFO DAGScheduler: Final stage: ResultStage 11 (parquet at Task4.java:85)\n",
            "25/08/06 08:42:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
            "25/08/06 08:42:22 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:42:22 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[51] at parquet at Task4.java:85), which has no missing parents\n",
            "25/08/06 08:42:22 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 223.7 KiB, free 1760.5 MiB)\n",
            "25/08/06 08:42:22 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 81.1 KiB, free 1760.4 MiB)\n",
            "25/08/06 08:42:22 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on fa3f124442cb:42831 (size: 81.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:42:22 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:42:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[51] at parquet at Task4.java:85) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:42:22 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:42:22 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 10) (fa3f124442cb, executor driver, partition 0, NODE_LOCAL, 7181 bytes) \n",
            "25/08/06 08:42:22 INFO Executor: Running task 0.0 in stage 11.0 (TID 10)\n",
            "25/08/06 08:42:22 INFO ShuffleBlockFetcherIterator: Getting 1 (593.0 B) non-empty blocks including 1 (593.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:42:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
            "25/08/06 08:42:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:42:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:42:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:42:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:42:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:42:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:42:22 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:42:22 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:42:22 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:42:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalRevenue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary country (STRING);\n",
            "  optional double totalRevenue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:42:22 INFO FileOutputCommitter: Saved output of task 'attempt_202508060842226613100915419684857_0011_m_000000_10' to file:/content/data/output/task4/aggregatedRevenue/_temporary/0/task_202508060842226613100915419684857_0011_m_000000\n",
            "25/08/06 08:42:22 INFO SparkHadoopMapRedUtil: attempt_202508060842226613100915419684857_0011_m_000000_10: Committed. Elapsed time: 5 ms.\n",
            "25/08/06 08:42:22 INFO Executor: Finished task 0.0 in stage 11.0 (TID 10). 3301 bytes result sent to driver\n",
            "25/08/06 08:42:22 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 10) in 155 ms on fa3f124442cb (executor driver) (1/1)\n",
            "25/08/06 08:42:22 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:42:22 INFO DAGScheduler: ResultStage 11 (parquet at Task4.java:85) finished in 0.203 s\n",
            "25/08/06 08:42:22 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:42:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
            "25/08/06 08:42:22 INFO DAGScheduler: Job 9 finished: parquet at Task4.java:85, took 0.214780 s\n",
            "25/08/06 08:42:22 INFO FileFormatWriter: Start to commit write Job a7e545f3-442d-4bac-b09a-61b587f8a87c.\n",
            "25/08/06 08:42:22 INFO FileFormatWriter: Write Job a7e545f3-442d-4bac-b09a-61b587f8a87c committed. Elapsed time: 18 ms.\n",
            "25/08/06 08:42:22 INFO FileFormatWriter: Finished processing stats for write job a7e545f3-442d-4bac-b09a-61b587f8a87c.\n",
            "25/08/06 08:42:22 INFO MapPartitionsRDD: Removing RDD 14 from persistence list\n",
            "25/08/06 08:42:22 INFO BlockManager: Removing RDD 14\n",
            "25/08/06 08:42:22 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 08:42:22 INFO SparkUI: Stopped Spark web UI at http://fa3f124442cb:4040\n",
            "25/08/06 08:42:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 08:42:22 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 08:42:22 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 08:42:22 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 08:42:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 08:42:22 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 08:42:22 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 08:42:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-bb8a8080-87c4-4407-8629-faa5b4fe3cc7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 5: Code Structure & Submission"
      ],
      "metadata": {
        "id": "1eTb44AmjLKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r data_folder.zip /content/data/\n",
        "\n",
        "# Step 2: Download the zipped folder\n",
        "from google.colab import files\n",
        "files.download('data_folder.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P4zyY3oti00N",
        "outputId": "537bc66d-ce6c-4ef1-abb9-3be906f8058a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/data/ (stored 0%)\n",
            "  adding: content/data/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/data/output/ (stored 0%)\n",
            "  adding: content/data/output/task1/ (stored 0%)\n",
            "  adding: content/data/output/task1/orderdetails/ (stored 0%)\n",
            "  adding: content/data/output/task1/orderdetails/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/output/task1/orderdetails/_SUCCESS (stored 0%)\n",
            "  adding: content/data/output/task1/orderdetails/part-00000-e416c70d-cd9e-43af-986e-3ba02e998a1e-c000.snappy.parquet (deflated 14%)\n",
            "  adding: content/data/output/task1/orderdetails/.part-00000-e416c70d-cd9e-43af-986e-3ba02e998a1e-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/output/task1/customers/ (stored 0%)\n",
            "  adding: content/data/output/task1/customers/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/output/task1/customers/part-00000-23671a68-7d7e-43a5-987e-fb6eb20921c3-c000.snappy.parquet (deflated 29%)\n",
            "  adding: content/data/output/task1/customers/.part-00000-23671a68-7d7e-43a5-987e-fb6eb20921c3-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/output/task1/customers/_SUCCESS (stored 0%)\n",
            "  adding: content/data/output/task1/payments/ (stored 0%)\n",
            "  adding: content/data/output/task1/payments/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/output/task1/payments/part-00000-d589cdd0-28a9-4494-a3a5-0147b264c1d8-c000.snappy.parquet (deflated 25%)\n",
            "  adding: content/data/output/task1/payments/_SUCCESS (stored 0%)\n",
            "  adding: content/data/output/task1/payments/.part-00000-d589cdd0-28a9-4494-a3a5-0147b264c1d8-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/output/task1/employees/ (stored 0%)\n",
            "  adding: content/data/output/task1/employees/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/output/task1/employees/.part-00000-dcd7ad0b-72a1-44fd-a1a1-f1d3add83c01-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/output/task1/employees/part-00000-dcd7ad0b-72a1-44fd-a1a1-f1d3add83c01-c000.snappy.parquet (deflated 35%)\n",
            "  adding: content/data/output/task1/employees/_SUCCESS (stored 0%)\n",
            "  adding: content/data/output/task1/orders/ (stored 0%)\n",
            "  adding: content/data/output/task1/orders/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/output/task1/orders/.part-00000-ab60cd2c-3d7b-43fd-98bb-649efba8034c-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/output/task1/orders/_SUCCESS (stored 0%)\n",
            "  adding: content/data/output/task1/orders/part-00000-ab60cd2c-3d7b-43fd-98bb-649efba8034c-c000.snappy.parquet (deflated 37%)\n",
            "  adding: content/data/output/task1/offices/ (stored 0%)\n",
            "  adding: content/data/output/task1/offices/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/output/task1/offices/part-00000-04015d57-ffbc-4cd4-be9f-019fb971cf40-c000.snappy.parquet (deflated 39%)\n",
            "  adding: content/data/output/task1/offices/.part-00000-04015d57-ffbc-4cd4-be9f-019fb971cf40-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/output/task1/offices/_SUCCESS (stored 0%)\n",
            "  adding: content/data/output/task1/productlines/ (stored 0%)\n",
            "  adding: content/data/output/task1/productlines/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/output/task1/productlines/.part-00000-4888c880-09ed-4383-8b2f-2f0a8372f0b3-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/output/task1/productlines/_SUCCESS (stored 0%)\n",
            "  adding: content/data/output/task1/productlines/part-00000-4888c880-09ed-4383-8b2f-2f0a8372f0b3-c000.snappy.parquet (deflated 39%)\n",
            "  adding: content/data/output/task1/products/ (stored 0%)\n",
            "  adding: content/data/output/task1/products/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/output/task1/products/_SUCCESS (stored 0%)\n",
            "  adding: content/data/output/task1/products/part-00000-014e40be-2e5d-4608-8367-4ae8c0d15af6-c000.snappy.parquet (deflated 26%)\n",
            "  adding: content/data/output/task1/products/.part-00000-014e40be-2e5d-4608-8367-4ae8c0d15af6-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/output/task2/ (stored 0%)\n",
            "  adding: content/data/output/task2/product_revenue/ (stored 0%)\n",
            "  adding: content/data/output/task2/product_revenue/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/output/task2/product_revenue/.part-00000-440f2c71-9e88-49f5-bb06-a8f3fe307151-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/output/task2/product_revenue/_SUCCESS (stored 0%)\n",
            "  adding: content/data/output/task2/product_revenue/part-00000-440f2c71-9e88-49f5-bb06-a8f3fe307151-c000.snappy.parquet (deflated 21%)\n",
            "  adding: content/data/output/task2/customer_aov/ (stored 0%)\n",
            "  adding: content/data/output/task2/customer_aov/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/output/task2/customer_aov/.part-00000-bc505027-6bf5-4694-aa2e-37e29cb826c9-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/output/task2/customer_aov/part-00000-bc505027-6bf5-4694-aa2e-37e29cb826c9-c000.snappy.parquet (deflated 49%)\n",
            "  adding: content/data/output/task2/customer_aov/_SUCCESS (stored 0%)\n",
            "  adding: content/data/output/task2/top_products/ (stored 0%)\n",
            "  adding: content/data/output/task2/top_products/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/output/task2/top_products/part-00000-fcc78009-bef4-4e95-8401-a349f5ae108d-c000.snappy.parquet (deflated 36%)\n",
            "  adding: content/data/output/task2/top_products/_SUCCESS (stored 0%)\n",
            "  adding: content/data/output/task2/top_products/.part-00000-fcc78009-bef4-4e95-8401-a349f5ae108d-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/output/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/data/output/task3/ (stored 0%)\n",
            "  adding: content/data/output/task3/revenue_by_country/ (stored 0%)\n",
            "  adding: content/data/output/task3/revenue_by_country/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/output/task3/revenue_by_country/.part-00000-0250c585-fe3f-488b-9d5d-a4a3fbbc049a-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/output/task3/revenue_by_country/part-00000-0250c585-fe3f-488b-9d5d-a4a3fbbc049a-c000.snappy.parquet (deflated 27%)\n",
            "  adding: content/data/output/task3/revenue_by_country/_SUCCESS (stored 0%)\n",
            "  adding: content/data/output/task3/top_offices/ (stored 0%)\n",
            "  adding: content/data/output/task3/top_offices/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/output/task3/top_offices/.part-00000-1faddda4-a5e5-49ba-9700-8f98c8defd72-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/output/task3/top_offices/part-00000-1faddda4-a5e5-49ba-9700-8f98c8defd72-c000.snappy.parquet (deflated 48%)\n",
            "  adding: content/data/output/task3/top_offices/_SUCCESS (stored 0%)\n",
            "  adding: content/data/output/task3/sales_per_region/ (stored 0%)\n",
            "  adding: content/data/output/task3/sales_per_region/part-00000-3a034b75-8f05-4a23-b087-063171cd4caa-c000.snappy.parquet (deflated 47%)\n",
            "  adding: content/data/output/task3/sales_per_region/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/output/task3/sales_per_region/.part-00000-3a034b75-8f05-4a23-b087-063171cd4caa-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/output/task3/sales_per_region/_SUCCESS (stored 0%)\n",
            "  adding: content/data/output/task4/ (stored 0%)\n",
            "  adding: content/data/output/task4/aggregatedRevenue/ (stored 0%)\n",
            "  adding: content/data/output/task4/aggregatedRevenue/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/output/task4/aggregatedRevenue/.part-00000-2fcb51f5-e593-4192-aa11-7fdadf16fded-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/output/task4/aggregatedRevenue/part-00000-2fcb51f5-e593-4192-aa11-7fdadf16fded-c000.snappy.parquet (deflated 26%)\n",
            "  adding: content/data/output/task4/aggregatedRevenue/_SUCCESS (stored 0%)\n",
            "  adding: content/data/output/task4/revenueByCountry/ (stored 0%)\n",
            "  adding: content/data/output/task4/revenueByCountry/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/output/task4/revenueByCountry/part-00000-569792f3-b7e0-4d2a-9b1f-ce7e957f46ae-c000.snappy.parquet (deflated 25%)\n",
            "  adding: content/data/output/task4/revenueByCountry/_SUCCESS (stored 0%)\n",
            "  adding: content/data/output/task4/revenueByCountry/.part-00000-569792f3-b7e0-4d2a-9b1f-ce7e957f46ae-c000.snappy.parquet.crc (stored 0%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_06e70796-42db-4699-9e14-0d3d99613571\", \"data_folder.zip\", 78861)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6CjHLHq3jtTA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}